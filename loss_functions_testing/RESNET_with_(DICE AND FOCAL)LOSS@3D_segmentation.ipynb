{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RESNET_with_(DICE AND FOCAL)LOSS@3D_segmentation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","\n","residual network in each convolutional block\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","2. The problem of class imbalance still persists and dice coeff is more than 1. So i this version we have implemented a new dice coefficient function.\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2ZRgqHl1w9ot","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"510603dc-21ba-419c-eed7-2e8762ea4e93","executionInfo":{"status":"ok","timestamp":1557734527588,"user_tz":-330,"elapsed":1329,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}}},"source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","import math\n","import os\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"source":["'''DEFINING VARIABLES'''\n","\n","batch_size=1              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","'''Here we crop the 155x240x240 image to  160x160x192 by repeating the last layer 5 times to convert 155 to 160'''\n","#X = tf.placeholder(shape=[None,160,160,192,4], dtype=tf.float32, name='input_image')\n","#y = tf.placeholder(shape=[None,160,160,192,1], dtype=tf.int64, name='hot_encode_label')\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size*155,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size*155,1),dtype=np.int64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","    '''Defining a filter variable to perform convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))           #HE initialization\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,in_chan,out_chan],stddev=stddev),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","    '''Defining a filter variable to perform transpose convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,out_chan,in_chan],stddev=stddev),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","    '''Function to perform Convolution and apply activation filter'''\n","    '''Convolution'''\n","    conv = tf.nn.conv3d(input_im,filter_mask,strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","    '''Function to perform Transpose Convolution and apply activation filter'''\n","    '''Transpose Convolution'''\n","    inp_shape = np.shape(input_im) #tf.shape()\n","    out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2),int(inp_shape[3].value*2), int(inp_shape[4].value/2)]\n","    \n","    conv = tf.nn.conv3d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","    '''normalization of each input channels'''\n","    global out_img\n","    '''CHANNEL INFO'''\n","    # maximum value found using function called \"Finding_maximum_to_normalise \"\n","    #'''max value for dimension 4 is 5337.0'''\n","    #'''max value for dimension 3 is 11737.0'''\n","    #'''max value for dimension 2 is 9751.0'''\n","    #'''max value for dimension 1 is 6476.0'''\n","    out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","    out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","    out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","    out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","    \n","    \n","\n","def crop_image_fit_brain(in_image):\n","    '''cropping size was found using the code finding_brain.ipynb'''\n","    left = 19\n","    right= 210\n","    top  = 38\n","    bot  = 199\n","    out_image = in_image[top:(bot-1),left:(right+1),:,:]\n","    return (out_image)\n","\n","\n","def Pre_processing_3D(a):\n","    '''Function to roll axis to convert array into[depth,width,height,channels] and the divide it in batches'''\n","    #print(np.shape(a))\n","    b = np.rollaxis(a,2, 0)\n","    #print(np.shape(b))\n","    #image shape\n","    out_arr = np.empty(shape=[batch_size,160,np.shape(b)[1],np.shape(b)[2],np.shape(b)[3]])\n","    for i in range(batch_size):\n","        start = i*155\n","        end = start+155\n","        out_arr[i,0:155,:,:,:] = b[start:end,:,:,:]\n","    \n","    a = [out_arr[:,154:155,:,:,:]]*5\n","    out_arr[:,155:160,:,:,:] = a[0]\n","    \n","    '''clippig data from front of each batch'''\n","    '''to fit the model we remove first 3 slices of each batch'''\n","    '''shape of a is [batch_size,depth,width,height,channels]'''\n","    out_send = out_arr[:,:,:,:,:]\n","    #print(\"arr\",np.shape(out_arr),\"send\",np.shape(out_send))\n","    return (out_send)\n","\n","def random_rotate(in_image,in_label):\n","    \n","    check = np.random.random(1)[0]\n","    if check<0.25:\n","        out_image = in_image[:,:,::-1,:,:]\n","        out_lab = in_label[:,:,::-1,:,:]\n","        \n","    elif check<0.50:\n","        \n","        out_image = in_image[:,:,:,::-1,:]\n","        out_lab = in_label[:,:,:,::-1,:]\n","        \n","    elif check<0.75:\n","        \n","        out_image = in_image[:,::-1,:,:,:]\n","        out_lab = in_label[:,::-1,:,:,:]\n","        \n","    else:\n","        out_image =in_image\n","        out_lab = in_label\n","    \n","    return (out_image,out_lab)\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","    '''Function to take batches randomly'''\n","    global out_img\n","    global out_label\n","    \n","    '''training info'''\n","    train_info = 384  #100 patients with 155 images each\n","\n","    if current_batch_no == 0:\n","        no_of_batches = train_info//batch_size  \n","        permute_mat = np.random.permutation(no_of_batches)\n","    \n","    start = permute_mat[current_batch_no]*batch_size*155\n","    end = start + (batch_size*155)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    current_batch_no += 1\n","    #print(len(out_img))\n","    '''Input normalization'''\n","    normalizing_input()\n","    '''normalization oof labels'''\n","    #out_label = out_label\n","    #normalizing_label()\n","    '''converting multi class to dual class'''\n","    #out_label = convert_dual_class(out_label)\n","    '''cropping image and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label)\n","    '''Rolling axes'''\n","    #out_img_send = np.rollaxis(crop_out_image,2, 0)\n","    '''hot encoding'''\n","    #out_label_send = crop_out_label\n","    '''3D conversion'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    '''Data augmentation rotation'''\n","    #out_img_send,out_label_send = random_rotate(out_img_send,out_label_send)\n","     \n","    last=0\n","    if current_batch_no == len(permute_mat):\n","        last=1\n","    \n","    return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","def test_batch():\n","    '''Function to take next test batch''' \n","    global out_img\n","    global out_label\n","    \n","    '''training and testing info'''\n","    train_info = 380  #100 patients with 155 images each\n","    test_info = 484-train_info  #100 patients with 155 images each\n","    \n","    no_of_batches = test_info//batch_size  \n","    permute_mat = np.random.permutation(no_of_batches)\n","    start = (permute_mat[0]*batch_size*155) +(train_info*155)\n","    end = start + (155*batch_size)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","    '''normalization'''\n","    normalizing_input()\n","    #normalizing_label()\n","    '''croping images and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label) \n","    '''3D processing'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    \n","    return (out_img_send,out_label_send)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"source":["INITIALIZING THE MODEL FILTERS\n","\n","\n","Filters with Resnet modifications"]},{"cell_type":"code","metadata":{"id":"8gltgoRN2psk","colab_type":"code","colab":{}},"source":["\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","    '''function for hot encoding images'''\n","    a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","    b = tf.transpose(a,perm=[0,1,2,3,5,4])\n","    return b\n","\n","\n","#############################################################################################################################\n","#GENERALIZED DICE LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_dice_coeff(y_true, y_pred):\n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    w = 1/((w**2))\n","    print(np.shape(w))\n","    # Compute gen dice coef:\n","    numerator = y_true*y_pred\n","    denominator = y_true+y_pred\n","    a=b=0\n","    for i in range(np.shape(w)[0]):\n","        a += w[i]*numerator[:,:,:,:,i]\n","        b += w[i]*denominator[:,:,:,:,i]\n","            \n","    num = tf.reduce_sum(a)\n","    den = tf.reduce_sum(b)\n","\n","    gen_dice_coef = tf.subtract(1.0,tf.divide((2*num),den),name='dice_loss')\n","\n","    return (gen_dice_coef)\n","  \n","#############################################################################################################################\n","#GENERALIZED FOCAL LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n","    \n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    w = 1/((w))\n","    y_pred = y_pred + 0.000000001                                               #to ensure that logarithm in next step doenst give math error\n","    \n","    ce = tf.multiply(y_true, -tf.log(y_pred))                                   #cross entropy (multiclass)\n","    fl_var = tf.multiply(y_true, tf.pow(tf.subtract(1., y_pred), gamma))        #focal loss variables (gamma*(1-pt)*(graund_truth))\n","    #fl = tf.multiply(alpha, tf.multiply(fl_var, ce))\n","    fl = tf.multiply(fl_var, ce)\n","    #fl=ce\n","    normalized_focal = 0\n","    \n","    for i in range(np.shape(w)[0]):\n","        #print(i)\n","        a = w[i]#/tf.reduce_sum(w)\n","        b = fl[:,:,:,:,i]\n","        #c = (b)/(tf.reduce_max(b)+1)\n","        normalized_focal += tf.reduce_sum(a*b)\n","        #b += w[i]*denominator[:,:,:,i]\n","    weighted_focal = tf.divide(normalized_focal,4.0,name='focal_loss')\n","    return (weighted_focal)\n","\n","#############################################################################################################################\n","#MEAN SQUARE ERROR(MSE) FUNCTION\n","#############################################################################################################################\n","def diff_error(y_true, y_pred):\n","    error = tf.square(y_true-y_pred)\n","    #error = (error+0.00000000001-tf.reduce_min(error))/(tf.reduce_max(error)+0.0000001-tf.reduce_min(error))\n","    #a = np.shape(error)#/(1.0)\n","    #size = batch_size*a[1]*a[2]*a[3]*a[4]\n","    #size = 19660800.0\n","    #error = tf.divide(tf.reduce_mean(error),size, name='mse_loss')\n","    return tf.reduce_mean(error, name='mse_loss')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6_uNrovR4mU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHmPMH2NWrEu","colab_type":"code","colab":{}},"source":["def find_hybrid_loss(y_true,y_pred,gamma=0.6,alpha=0.5):\n","    dice = generalized_dice_coeff(y_true, y_pred)\n","    focal = generalized_focal_loss(y_true, y_pred)\n","    MeanAndSquare = diff_error(y_true, y_pred)\n","    hybrid = tf.identity((gamma*dice)+((1-gamma)*((alpha*focal)+((1-alpha)*MeanAndSquare))),name='hybrid_loss')\n","    return hybrid,dice,focal,MeanAndSquare"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"source":["DEFINING THE MODEL STRUCTURE\n","\n","Model with Resnet Modification"]},{"cell_type":"code","metadata":{"id":"mOK_Zubt7JIa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgMnuQeV7JCB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"source":["def predict_model1(X):\n","    '''FILTER DEFINITIONS'''\n","    '''MODEL1 Filter definition'''\n","    '''LEFT'''\n","    \n","    filter1 = left_filter_def(3,4,8,name='filter1')\n","    filter2 = left_filter_def(3,8,8,name='filter2')\n","    filter3 = left_filter_def(3,8,8,name='filter3')\n","    filter4 = left_filter_def(3,8,8,name='filter4')\n","       \n","    filter5 = left_filter_def(3,8,16,name='filter5')\n","    filter6 = left_filter_def(3,16,16,name='filter6')\n","    filter7 = left_filter_def(3,16,16,name='filter7')\n","    filter8 = left_filter_def(3,16,16,name='filter8')\n","        \n","    filter9 = left_filter_def(3,16,32,name='filter9')\n","    filter10= left_filter_def(3,32,32,name='filter10')\n","    filter11= left_filter_def(3,32,32,name='filter11')\n","    filter12= left_filter_def(3,32,32,name='filter12')\n","        \n","    filter13= left_filter_def(3,32,64,name='filter13')\n","    filter14= left_filter_def(3,64,64,name='filter14')\n","    filter15= left_filter_def(3,64,64,name='filter15')\n","    filter16= left_filter_def(3,64,64,name='filter16')\n","        \n","    filter17= left_filter_def(3,64,128,name='filter17')\n","    filter18= left_filter_def(3,128,128,name='filter18')\n","    filter19= left_filter_def(3,128,64,name='filter19')\n","        \n","    '''RIGHT'''\n","        \n","    filter20= right_filter_def(3,128,64,name='filter20')\n","    depth_filter1 = left_filter_def(1,(2*64),64,name='depth_filter1')\n","    filter21= left_filter_def(3,64,64,name='filter21')\n","    filter22= left_filter_def(3,64,64,name='filter22')\n","    filter23= left_filter_def(3,64,32,name='filter23')\n","    \n","    filter24= right_filter_def(3,64,32,name='filter24')\n","    depth_filter2 = left_filter_def(1,(2*32),32,name='depth_filter2')\n","    filter25= left_filter_def(3,32,32,name='filter25')\n","    filter26= left_filter_def(3,32,32,name='filter26')\n","    filter27= left_filter_def(3,32,16,name='filter27')\n","    \n","    filter28= right_filter_def(3,32,16,name='filter28')\n","    depth_filter3 = left_filter_def(1,(2*16),16,name='depth_filter3')\n","    filter29= left_filter_def(3,16,16,name='filter29')\n","    filter30= left_filter_def(3,16,16,name='filter30')\n","    filter31= left_filter_def(3,16,8,name='filter31')\n","    \n","    filter32= right_filter_def(3,16,8,name='filter32')\n","    depth_filter4 = left_filter_def(1,(2*8),8,name='depth_filter4')\n","    filter33= left_filter_def(3,8,8,name='filter33')\n","    filter34= left_filter_def(3,8,8,name='filter34')\n","    filter35= left_filter_def(3,8,n_class,name='filter35')\n","    \n","    '''Function to define the UNET model'''\n","    with tf.name_scope(\"BLOCK1\"):\n","        '''BLOCK1'''\n","        print(\"X\",np.shape(X))\n","        CNN1 = Conv_layer(X,filter1,stride=1,activation='relu',name='CNN1')\n","        #rint (\"CNN1\",np.shape(CNN1))\n","        CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='CNN2')\n","        #rint (\"CNN2\",np.shape(CNN2))\n","        CNN3 = Conv_layer(CNN2,filter3,stride=1,activation='relu',name='CNN3')\n","        CNNR1 = tf.add(Conv_layer(CNN3,filter4,stride=1,activation='relu',name='CNNR1'), (CNN1+CNN2), name='Resnet1')\n","        print (\"CNNR1\",np.shape(CNNR1))\n","        pool1 = tf.nn.max_pool3d(CNNR1,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","        '''BLOCK2'''\n","        CNN4 = Conv_layer(pool1,filter5,stride=1,activation='relu',name='CNN4')\n","        print (\"CNN4\",np.shape(CNN4))\n","        CNN5 = Conv_layer(CNN4,filter6,stride=1,activation='relu',name='CNN5')\n","        #rint (\"CNN4\",np.shape(CNN4))\n","        CNN6 = Conv_layer(CNN5,filter7,stride=1,activation='relu',name='CNN6')\n","        CNNR2 = tf.add(Conv_layer(CNN6,filter8,stride=1,activation='relu',name='CNNR2'), (CNN4+CNN5), name='Resnet2')\n","        #rint (\"CNNR2\",np.shape(CNNR2))\n","        pool2 = tf.nn.max_pool3d(CNNR2,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL2')\n","        print(\"pool2\",np.shape(pool2))\n","    with tf.name_scope(\"BLOCK3\"):\n","        '''BLOCK3'''\n","        CNN7 = Conv_layer(pool2,filter9,stride=1,activation='relu',name='CNN7')\n","        print (\"CNN7\",np.shape(CNN7))\n","        CNN8 = Conv_layer(CNN7,filter10,stride=1,activation='relu',name='CNN8')\n","        #rint (\"CNN6\",np.shape(CNN6))\n","        CNN9 = Conv_layer(CNN8,filter11,stride=1,activation='relu',name='CNN9')\n","        CNNR3 = tf.add(Conv_layer(CNN9,filter12,stride=1,activation='relu',name='CNNR3'), (CNN7+CNN8), name='Resnet3')\n","        #rint (\"CNNR3N\",np.shape(CNNR3))\n","        pool3 = tf.nn.max_pool3d(CNNR3,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL3')\n","        print(\"pool3\",np.shape(pool3))\n","    with tf.name_scope(\"BLOCK4\"):\n","        '''BLOCK4'''\n","        CNN10= Conv_layer(pool3,filter13,stride=1,activation='relu',name='CNN10')\n","        print (\"CNN10\",np.shape(CNN10))\n","        CNN11= Conv_layer(CNN10,filter14,stride=1,activation='relu',name='CNN11')\n","        #rint (\"CNN8\",np.shape(CNN8))\n","        CNN12 = Conv_layer(CNN11,filter15,stride=1,activation='relu',name='CNN12')\n","        CNNR4 = tf.add(Conv_layer(CNN12,filter16,stride=1,activation='relu',name='CNNR4'), (CNN10+CNN11), name='Resnet4')\n","        #rint (\"CNNR4\",np.shape(CNNR4))\n","        pool4 = tf.nn.max_pool3d(CNNR4,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL4')\n","        print(\"pool4\",np.shape(pool4))\n","    with tf.name_scope(\"BLOCK5\"):\n","        '''BLOCK5'''\n","        CNN13 = Conv_layer(pool4,filter17,stride=1,activation='relu',name='CNN13')\n","        #rint (\"CNN9\",np.shape(CNN9))\n","        CNN14 = Conv_layer(CNN13,filter18,stride=1,activation='relu',name='CNN14')\n","        #rint (\"CNN10\",np.shape(CNN10))\n","        CNN15 = Conv_layer(CNN14,filter19,stride=1,activation='relu',name='CNN15')\n","        #NNR5 = tf.add(Conv_layer(CNN10,filter15,stride=1,activation='relu',name='CNNR5'), CNN9, name='Resnet5')\n","        print (\"CNN15\",np.shape(CNN15))\n","    \n","    '''Moving UP'''\n","    \n","    with tf.name_scope(\"BLOCK6\"):\n","        '''BLOCK6'''\n","        concat1 = tf.concat([CNN15,pool4],axis=4,name='CONCAT1')\n","        print (\"concat1\",np.shape(concat1))\n","        DCNN1= Deconv_layer(concat1,filter20,stride=2,activation='relu',name='DE_CONV1')\n","        #rint (\"DCNN1\",np.shape(DCNN1))\n","        \n","        CNN11 = Conv_layer(DCNN1,filter21,stride=1,activation='relu',name='CNN11')\n","        depth_CNN1 = Conv_layer(tf.concat([CNN11,CNNR4],axis=4),depth_filter1,stride=1,activation='relu',name='DepthCNN1') \n","        #rint (\"CNN11\",np.shape(CNN11))\n","        CNNR6 = tf.add(Conv_layer(depth_CNN1,filter22,stride=1,activation='relu',name='CNNR6'), DCNN1, name='Resnet6')\n","        #rint (\"CNNR6\",np.shape(CNNR6))\n","        CNN12 = Conv_layer(CNNR6,filter23,stride=1,activation='relu',name='CNN12')\n","        #rint (\"CNN12\",np.shape(CNN12))\n","    \n","    with tf.name_scope(\"BLOCK7\"):\n","        '''BLOCK7'''\n","        concat2 = tf.concat([CNN12,pool3],axis=4,name='CONCAT2')\n","        #rint (\"concat2\",np.shape(concat2))\n","        DCNN2= Deconv_layer(concat2,filter24,stride=2,activation='relu',name='DE_CONV2')\n","        #rint (\"DCNN2\",np.shape(DCNN2))\n","        \n","        CNN13 = Conv_layer(DCNN2,filter25,stride=1,activation='relu',name='CNN13')\n","        depth_CNN2 = Conv_layer(tf.concat([CNN13,CNNR3],axis=4),depth_filter2,stride=1,activation='relu',name='DepthCNN12')\n","        #rint (\"CNN13\",np.shape(CNN13))\n","        CNNR7 = tf.add(Conv_layer(depth_CNN2,filter26,stride=1,activation='relu',name='CNNR7'), DCNN2, name='Resnet7')\n","        #rint (\"CNNR7\",np.shape(CNNR7))\n","        CNN14 = Conv_layer(CNNR7,filter27,stride=1,activation='relu',name='CNN14')\n","        #rint (\"CNN14\",np.shape(CNN14))\n","    \n","    with tf.name_scope(\"BLOCK8\"):\n","        '''BLOCK8'''\n","        concat3 = tf.concat([CNN14,pool2],axis=4,name='CONCAT3')\n","        #rint (\"concat3\",np.shape(concat3))\n","        DCNN3= Deconv_layer(concat3,filter28,stride=2,activation='relu',name='DE_CONV3')\n","        #rint (\"DCNN3\",np.shape(DCNN3))\n","        \n","        CNN15 = Conv_layer(DCNN3,filter29,stride=1,activation='relu',name='CNN15')\n","        depth_CNN3 = Conv_layer(tf.concat([CNN15,CNNR2],axis=4),depth_filter3,stride=1,activation='relu',name='DepthCNN12')\n","        #rint (\"CNN15\",np.shape(CNN15))\n","        CNNR8 = tf.add(Conv_layer(depth_CNN3,filter30,stride=1,activation='relu',name='CNNR8'), DCNN3, name='Resnet8')\n","        #rint (\"CNNR8\",np.shape(CNNR8))\n","        CNN16 = Conv_layer(CNNR8,filter31,stride=1,activation='relu',name='CNN16')\n","        #rint (\"CNN16\",np.shape(CNN16))\n","         \n","    with tf.name_scope(\"BLOCK9\"):\n","        '''BLOCK9'''\n","        concat4 = tf.concat([CNN16,pool1],axis=4,name='CONCAT4')\n","        #rint (\"concat4\",np.shape(concat4))\n","        DCNN4= Deconv_layer(concat4,filter32,stride=2,activation='relu',name='DE_CONV4')\n","        #rint (\"DCNN4\",np.shape(DCNN4))\n","        \n","        CNN17 = Conv_layer(DCNN4,filter33,stride=1,activation='relu',name='CNN17')\n","        depth_CNN4 = Conv_layer(tf.concat([CNN17,CNNR1],axis=4),depth_filter4,stride=1,activation='relu',name='DepthCNN12')\n","        #rint (\"CNN17\",np.shape(CNN17))\n","        CNNR9 = tf.add(Conv_layer(depth_CNN4,filter34,stride=1,activation='relu',name='CNNR8'), DCNN4, name='Resnet9')\n","        #rint (\"CNNR9\",np.shape(CNNR9))\n","        CNN18 = Conv_layer(CNNR9,filter35,stride=1,activation='softmax',name='CNN18')\n","        #rint (\"CNN18\",np.shape(CNN18))\n","    return (CNN18)\n","\n","#a = (predict_model1(X))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"42yzmFwI9adS","colab_type":"code","colab":{}},"source":["def chose_train_restore(learning_rate =0.0001,n_epochs = 100):\n","    output_dir = \"/content/gdrive/My Drive/Brain_Tumour_segmentation/loss_functions_testing/saved_model_3D\"\n","    model_checkpoint_file_base = os.path.join(output_dir, \"model.ckpt\")\n","\n","    \n","    if not os.path.exists(model_checkpoint_file_base + \".meta\"):\n","        '''FIRST TIME TRAINING'''\n","        print(\"Making new\")\n","        brand_new = True\n","        \n","        prediction = predict_model1(X)#logits\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"LOSS_FUNCTION\"):\n","            '''using multi dimensional dice'''\n","            hot_y = hot_encode(y)\n","            #dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","            #dice = generalized_dice_coeff(hot_y[:,:,:,:,0], prediction)\n","            #focal = generalized_focal_loss(hot_y[:,:,:,:,0], prediction)\n","            hybrid,dice,focal,sep = find_hybrid_loss(hot_y[:,:,:,:,:,0], prediction, gamma=0.9,alpha=0.8)\n","            #mean_error = diff_error(hot_y[:,:,:,:,0], prediction)\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"COST_FUNCTION\"):\n","            '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","            loss = tf.reduce_mean(hybrid, name=\"loss\")\n","\n","        saver = tf.train.Saver()\n","        \n","    else:\n","        '''RESTORED MODEL'''\n","        print(\"Reloading existing\")\n","        brand_new = False\n","        saver = tf.train.import_meta_graph(model_checkpoint_file_base + \".meta\")\n","        g = tf.get_default_graph()\n","        \n","        sep = g.get_tensor_by_name(\"LOSS_FUNCTION/mse_loss:0\")\n","        dice = g.get_tensor_by_name(\"LOSS_FUNCTION/dice_loss:0\")\n","        focal = g.get_tensor_by_name(\"LOSS_FUNCTION/focal_loss:0\")\n","        hybrid = g.get_tensor_by_name(\"LOSS_FUNCTION/hybrid_loss:0\")\n","        prediction = g.get_tensor_by_name(\"BLOCK9/Softmax:0\") \n","        loss = g.get_tensor_by_name(\"COST_FUNCTION/loss:0\")\n","        \n","        X = g.get_tensor_by_name(\"input_image:0\")\n","        y = g.get_tensor_by_name(\"hot_encode_label:0\")\n","\n","\n","    \n","    \n","    \n","    '''TRAINING'''\n","    '''starting session'''\n","    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n","    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_option)) as sess:\n","        '''Initializing optimizer'''\n","        if brand_new:\n","            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","            init = tf.global_variables_initializer()\n","            sess.run(init)\n","            tf.add_to_collection(\"optimizer\", optimizer)\n","        else:\n","            saver = tf.train.Saver()\n","            saver.restore(sess, model_checkpoint_file_base)\n","            optimizer = tf.get_collection(\"optimizer\")[0]\n","\n","        for epoch in range(9,n_epochs):\n","            current_batch_no = 0\n","            permute_mat = 0\n","            iteration = 0\n","            while(1):\n","                #with tf.device('/cpu:0'):\n","                epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","                sess_results = sess.run(optimizer, feed_dict={X: epoch_x, y: epoch_y})\n","                #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","                \n","                '''DICE Coefficient for iteration'''\n","                #with tf.device('/cpu:0'):\n","                if iteration%10==0:\n","                    acc_train = 1-(hybrid.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","                    test_images, test_labels = test_batch()\n","                    acc_test = 1-(hybrid.eval(feed_dict={X: test_images, y: test_labels}))\n","                    print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","                if last ==1:\n","                    break\n","                iteration +=1\n","            test_images, test_labels = test_batch()\n","            hybrid_test,dice_test,focal_test,diff_test = sess.run([hybrid,dice,focal,sep], feed_dict={X: test_images, y: test_labels})\n","            #diff_test = sess.run(mean_error, feed_dict={X: test_images, y: test_labels})\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            print(\"After Epoch\", epoch+1, \"Hybrid Test accuracy:\", 1-hybrid_test, \"Dice Test accuracy:\", 1-dice_test, \"Focal Test accuracy:\", 1-focal_test, \"MSE Test accuracy:\", 1-diff_test)\n","            #print(\"mse\",1-diff_test)\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            '''saving model after each epoch'''\n","            save_path = tf.train.Saver(max_to_keep=1).save(sess, model_checkpoint_file_base)\n","            \n","            if epoch % 1 == 0:\n","                test_example =   test_images\n","                test_example_gt = test_labels#np.rollaxis(test_labels,2,0)\n","                sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","                sess_results = sess_results[0,100,:,:,1] + (2*sess_results[0,100,:,:,2]) + (3*sess_results[0,100,:,:,3])\n","                test_example = test_example[0,100,:,:,3]\n","                test_example_gt = test_example_gt[0,100,:,:,:]\n","                \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Original Image')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"a_Original_Image.png\")\n","                 \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Ground Truth Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","                plt.figure()\n","                plt.imshow(np.squeeze(sess_results),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Generated Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","                plt.close('all')\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ix_lwNslKZOx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"cell_type":"code","metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"900fdaf7-48c2-400f-ecb0-baeafd4625f4","executionInfo":{"status":"error","timestamp":1557753396762,"user_tz":-330,"elapsed":9042777,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":3641}},"source":["chose_train_restore()\n","#chose_train_restore_model_2()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Reloading existing\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Brain_Tumour_segmentation/loss_functions_testing/saved_model_3D/model.ckpt\n","Minibatch at Epoch 10 batch 1 Train accuracy: 0.07594114542007446 Test accuracy: 0.063301682472229\n","Minibatch at Epoch 10 batch 11 Train accuracy: 0.0850566029548645 Test accuracy: 0.06201213598251343\n","Minibatch at Epoch 10 batch 21 Train accuracy: 0.07265520095825195 Test accuracy: 0.07443499565124512\n","Minibatch at Epoch 10 batch 31 Train accuracy: 0.0647505521774292 Test accuracy: 0.06477689743041992\n","Minibatch at Epoch 10 batch 41 Train accuracy: 0.07051956653594971 Test accuracy: 0.06247437000274658\n","Minibatch at Epoch 10 batch 51 Train accuracy: 0.06851798295974731 Test accuracy: 0.060847461223602295\n","Minibatch at Epoch 10 batch 61 Train accuracy: 0.06150805950164795 Test accuracy: 0.06417447328567505\n","Minibatch at Epoch 10 batch 71 Train accuracy: 0.07389247417449951 Test accuracy: 0.05442678928375244\n","Minibatch at Epoch 10 batch 81 Train accuracy: 0.09364986419677734 Test accuracy: 0.07371652126312256\n","Minibatch at Epoch 10 batch 91 Train accuracy: 0.014159142971038818 Test accuracy: 0.041461944580078125\n","Minibatch at Epoch 10 batch 101 Train accuracy: 0.07910257577896118 Test accuracy: 0.07036256790161133\n","Minibatch at Epoch 10 batch 111 Train accuracy: 0.0777023434638977 Test accuracy: 0.04567086696624756\n","Minibatch at Epoch 10 batch 121 Train accuracy: 0.07303500175476074 Test accuracy: 0.05806422233581543\n","Minibatch at Epoch 10 batch 131 Train accuracy: 0.14357686042785645 Test accuracy: 0.09943592548370361\n","Minibatch at Epoch 10 batch 141 Train accuracy: 0.08886456489562988 Test accuracy: 0.06540018320083618\n","Minibatch at Epoch 10 batch 151 Train accuracy: 0.07164239883422852 Test accuracy: 0.06650310754776001\n","Minibatch at Epoch 10 batch 161 Train accuracy: 0.10683852434158325 Test accuracy: 0.07443702220916748\n","Minibatch at Epoch 10 batch 171 Train accuracy: 0.04525601863861084 Test accuracy: 0.08013904094696045\n","Minibatch at Epoch 10 batch 181 Train accuracy: 0.06775057315826416 Test accuracy: 0.08332526683807373\n","Minibatch at Epoch 10 batch 191 Train accuracy: 0.07038635015487671 Test accuracy: 0.13519781827926636\n","Minibatch at Epoch 10 batch 201 Train accuracy: 0.17762595415115356 Test accuracy: 0.06966054439544678\n","Minibatch at Epoch 10 batch 211 Train accuracy: 0.06122714281082153 Test accuracy: -0.02776658535003662\n","Minibatch at Epoch 10 batch 221 Train accuracy: 0.06990182399749756 Test accuracy: 0.06769979000091553\n","Minibatch at Epoch 10 batch 231 Train accuracy: 0.09238666296005249 Test accuracy: -0.017312169075012207\n","Minibatch at Epoch 10 batch 241 Train accuracy: 0.06832277774810791 Test accuracy: 0.10030198097229004\n","Minibatch at Epoch 10 batch 251 Train accuracy: 0.1049838662147522 Test accuracy: 0.1444886326789856\n","Minibatch at Epoch 10 batch 261 Train accuracy: 0.0593944787979126 Test accuracy: 0.06669884920120239\n","Minibatch at Epoch 10 batch 271 Train accuracy: 0.0629395842552185 Test accuracy: 0.05891317129135132\n","Minibatch at Epoch 10 batch 281 Train accuracy: 0.03905832767486572 Test accuracy: 0.06952130794525146\n","Minibatch at Epoch 10 batch 291 Train accuracy: 0.07450109720230103 Test accuracy: 0.06389939785003662\n","Minibatch at Epoch 10 batch 301 Train accuracy: 0.0517733097076416 Test accuracy: 0.04964172840118408\n","Minibatch at Epoch 10 batch 311 Train accuracy: 0.04278552532196045 Test accuracy: 0.05643761157989502\n","Minibatch at Epoch 10 batch 321 Train accuracy: 0.0785558819770813 Test accuracy: 0.05499851703643799\n","Minibatch at Epoch 10 batch 331 Train accuracy: 0.10268104076385498 Test accuracy: 0.07428956031799316\n","Minibatch at Epoch 10 batch 341 Train accuracy: 0.06072133779525757 Test accuracy: 0.04401212930679321\n","Minibatch at Epoch 10 batch 351 Train accuracy: 0.08592808246612549 Test accuracy: 0.06078898906707764\n","Minibatch at Epoch 10 batch 361 Train accuracy: 0.0790245532989502 Test accuracy: 0.0654606819152832\n","Minibatch at Epoch 10 batch 371 Train accuracy: 0.0900036096572876 Test accuracy: 0.09282374382019043\n","Minibatch at Epoch 10 batch 381 Train accuracy: 0.07388651371002197 Test accuracy: 0.07558643817901611\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 10 Hybrid Test accuracy: 0.05969679355621338 Dice Test accuracy: 0.0008652806282043457 Focal Test accuracy: 0.510478138923645 MSE Test accuracy: 0.9039893224835396\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 11 batch 1 Train accuracy: 0.0739825963973999 Test accuracy: 0.057375311851501465\n","Minibatch at Epoch 11 batch 11 Train accuracy: 0.07537120580673218 Test accuracy: 0.07074534893035889\n","Minibatch at Epoch 11 batch 21 Train accuracy: 0.0930706262588501 Test accuracy: 0.1338489055633545\n","Minibatch at Epoch 11 batch 31 Train accuracy: 0.059613168239593506 Test accuracy: -0.00510406494140625\n","Minibatch at Epoch 11 batch 41 Train accuracy: 0.14348161220550537 Test accuracy: 0.1056898832321167\n","Minibatch at Epoch 11 batch 51 Train accuracy: 0.05911606550216675 Test accuracy: 0.016588926315307617\n","Minibatch at Epoch 11 batch 61 Train accuracy: 0.07141691446304321 Test accuracy: 0.03813374042510986\n","Minibatch at Epoch 11 batch 71 Train accuracy: 0.05454772710800171 Test accuracy: 0.0466575026512146\n","Minibatch at Epoch 11 batch 81 Train accuracy: 0.054434359073638916 Test accuracy: 0.05451464653015137\n","Minibatch at Epoch 11 batch 91 Train accuracy: 0.05168581008911133 Test accuracy: 0.04547226428985596\n","Minibatch at Epoch 11 batch 101 Train accuracy: 0.05043691396713257 Test accuracy: 0.06735491752624512\n","Minibatch at Epoch 11 batch 111 Train accuracy: 0.18923938274383545 Test accuracy: 0.09644103050231934\n","Minibatch at Epoch 11 batch 121 Train accuracy: 0.11109340190887451 Test accuracy: 0.09118711948394775\n","Minibatch at Epoch 11 batch 131 Train accuracy: 0.09450191259384155 Test accuracy: 0.061111629009246826\n","Minibatch at Epoch 11 batch 141 Train accuracy: 0.08799088001251221 Test accuracy: 0.05867511034011841\n","Minibatch at Epoch 11 batch 151 Train accuracy: 0.0895184874534607 Test accuracy: 0.0972321629524231\n","Minibatch at Epoch 11 batch 161 Train accuracy: -0.0033551454544067383 Test accuracy: 0.07321470975875854\n","Minibatch at Epoch 11 batch 171 Train accuracy: 0.07910698652267456 Test accuracy: 0.05346357822418213\n","Minibatch at Epoch 11 batch 181 Train accuracy: 0.0654364824295044 Test accuracy: 0.05702120065689087\n","Minibatch at Epoch 11 batch 191 Train accuracy: 0.04790318012237549 Test accuracy: -0.12881267070770264\n","Minibatch at Epoch 11 batch 201 Train accuracy: 0.03669220209121704 Test accuracy: 0.06335651874542236\n","Minibatch at Epoch 11 batch 211 Train accuracy: 0.06371867656707764 Test accuracy: 0.06985718011856079\n","Minibatch at Epoch 11 batch 221 Train accuracy: 0.07898366451263428 Test accuracy: 0.07659369707107544\n","Minibatch at Epoch 11 batch 231 Train accuracy: 0.053378403186798096 Test accuracy: 0.04001510143280029\n","Minibatch at Epoch 11 batch 241 Train accuracy: 0.07140731811523438 Test accuracy: 0.03448528051376343\n","Minibatch at Epoch 11 batch 251 Train accuracy: 0.08049142360687256 Test accuracy: 0.07215005159378052\n","Minibatch at Epoch 11 batch 261 Train accuracy: 0.0798943042755127 Test accuracy: 0.06481564044952393\n","Minibatch at Epoch 11 batch 271 Train accuracy: 0.07253050804138184 Test accuracy: 0.07845914363861084\n","Minibatch at Epoch 11 batch 281 Train accuracy: 0.1402440071105957 Test accuracy: 0.06542271375656128\n","Minibatch at Epoch 11 batch 291 Train accuracy: 0.06592041254043579 Test accuracy: 0.09146714210510254\n","Minibatch at Epoch 11 batch 301 Train accuracy: 0.050255417823791504 Test accuracy: -0.12072062492370605\n","Minibatch at Epoch 11 batch 311 Train accuracy: 0.032186269760131836 Test accuracy: -0.09317290782928467\n","Minibatch at Epoch 11 batch 321 Train accuracy: 0.05013275146484375 Test accuracy: 0.04994243383407593\n","Minibatch at Epoch 11 batch 331 Train accuracy: 0.05673956871032715 Test accuracy: 0.05050468444824219\n","Minibatch at Epoch 11 batch 341 Train accuracy: 0.05822259187698364 Test accuracy: 0.0690927505493164\n","Minibatch at Epoch 11 batch 351 Train accuracy: 0.05915921926498413 Test accuracy: 0.06895750761032104\n","Minibatch at Epoch 11 batch 361 Train accuracy: 0.07938545942306519 Test accuracy: 0.08383560180664062\n","Minibatch at Epoch 11 batch 371 Train accuracy: 0.11713248491287231 Test accuracy: 0.09958118200302124\n","Minibatch at Epoch 11 batch 381 Train accuracy: 0.08148574829101562 Test accuracy: 0.09912979602813721\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 11 Hybrid Test accuracy: 0.07598316669464111 Dice Test accuracy: 0.027038276195526123 Focal Test accuracy: 0.41203010082244873 MSE Test accuracy: 0.9343152344226837\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 12 batch 1 Train accuracy: 0.05121767520904541 Test accuracy: 0.07678604125976562\n","Minibatch at Epoch 12 batch 11 Train accuracy: 0.0885779857635498 Test accuracy: 0.05546987056732178\n","Minibatch at Epoch 12 batch 21 Train accuracy: 0.04981732368469238 Test accuracy: 0.03221863508224487\n","Minibatch at Epoch 12 batch 31 Train accuracy: 0.06290358304977417 Test accuracy: 0.08068382740020752\n","Minibatch at Epoch 12 batch 41 Train accuracy: 0.047983407974243164 Test accuracy: 0.10162609815597534\n","Minibatch at Epoch 12 batch 51 Train accuracy: 0.07209384441375732 Test accuracy: -0.07024931907653809\n","Minibatch at Epoch 12 batch 61 Train accuracy: 0.11222630739212036 Test accuracy: 0.09187543392181396\n","Minibatch at Epoch 12 batch 71 Train accuracy: 0.074851393699646 Test accuracy: 0.06309032440185547\n","Minibatch at Epoch 12 batch 81 Train accuracy: 0.07384783029556274 Test accuracy: -0.015799522399902344\n","Minibatch at Epoch 12 batch 91 Train accuracy: 0.06727033853530884 Test accuracy: 0.06889522075653076\n","Minibatch at Epoch 12 batch 101 Train accuracy: 0.0908358097076416 Test accuracy: 0.07336169481277466\n","Minibatch at Epoch 12 batch 111 Train accuracy: 0.15243220329284668 Test accuracy: 0.08904784917831421\n","Minibatch at Epoch 12 batch 121 Train accuracy: 0.0627942681312561 Test accuracy: 0.07233673334121704\n","Minibatch at Epoch 12 batch 131 Train accuracy: 0.0552370548248291 Test accuracy: 0.07629275321960449\n","Minibatch at Epoch 12 batch 141 Train accuracy: 0.08609360456466675 Test accuracy: 0.13964569568634033\n","Minibatch at Epoch 12 batch 151 Train accuracy: 0.11080372333526611 Test accuracy: 0.07540512084960938\n","Minibatch at Epoch 12 batch 161 Train accuracy: 0.11358600854873657 Test accuracy: 0.06879192590713501\n","Minibatch at Epoch 12 batch 171 Train accuracy: 0.1060531735420227 Test accuracy: 0.10172158479690552\n","Minibatch at Epoch 12 batch 181 Train accuracy: 0.0837244987487793 Test accuracy: 0.07514303922653198\n","Minibatch at Epoch 12 batch 191 Train accuracy: 0.081728994846344 Test accuracy: 0.11609715223312378\n","Minibatch at Epoch 12 batch 201 Train accuracy: 0.12775874137878418 Test accuracy: 0.13408738374710083\n","Minibatch at Epoch 12 batch 211 Train accuracy: 0.09412884712219238 Test accuracy: 0.08820176124572754\n","Minibatch at Epoch 12 batch 221 Train accuracy: 0.0766218900680542 Test accuracy: 0.07425254583358765\n","Minibatch at Epoch 12 batch 231 Train accuracy: 0.13094723224639893 Test accuracy: -0.2563878297805786\n","Minibatch at Epoch 12 batch 241 Train accuracy: 0.06723666191101074 Test accuracy: 0.062253475189208984\n","Minibatch at Epoch 12 batch 251 Train accuracy: 0.08746093511581421 Test accuracy: 0.09318840503692627\n","Minibatch at Epoch 12 batch 261 Train accuracy: 0.09159553050994873 Test accuracy: 0.06287157535552979\n","Minibatch at Epoch 12 batch 271 Train accuracy: 0.09885931015014648 Test accuracy: 0.08068627119064331\n","Minibatch at Epoch 12 batch 281 Train accuracy: 0.020266950130462646 Test accuracy: 0.11036843061447144\n","Minibatch at Epoch 12 batch 291 Train accuracy: 0.0727338194847107 Test accuracy: -0.03110647201538086\n","Minibatch at Epoch 12 batch 301 Train accuracy: -0.0018749237060546875 Test accuracy: -0.20203721523284912\n","Minibatch at Epoch 12 batch 311 Train accuracy: 0.028358042240142822 Test accuracy: 0.06780481338500977\n","Minibatch at Epoch 12 batch 321 Train accuracy: 0.1325455904006958 Test accuracy: 0.06634306907653809\n","Minibatch at Epoch 12 batch 331 Train accuracy: 0.12058037519454956 Test accuracy: 0.025765657424926758\n","Minibatch at Epoch 12 batch 341 Train accuracy: 0.039703547954559326 Test accuracy: 0.036405086517333984\n","Minibatch at Epoch 12 batch 351 Train accuracy: 0.08836913108825684 Test accuracy: 0.03991413116455078\n","Minibatch at Epoch 12 batch 361 Train accuracy: 0.09196245670318604 Test accuracy: 0.07733732461929321\n","Minibatch at Epoch 12 batch 371 Train accuracy: 0.06625235080718994 Test accuracy: 0.022556781768798828\n","Minibatch at Epoch 12 batch 381 Train accuracy: 0.1505635380744934 Test accuracy: 0.10381317138671875\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 12 Hybrid Test accuracy: 0.08115130662918091 Dice Test accuracy: 0.03702688217163086 Focal Test accuracy: 0.3650844097137451 MSE Test accuracy: 0.9310183376073837\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 13 batch 1 Train accuracy: 0.08937329053878784 Test accuracy: 0.13209587335586548\n","Minibatch at Epoch 13 batch 11 Train accuracy: 0.07692903280258179 Test accuracy: 0.06797373294830322\n","Minibatch at Epoch 13 batch 21 Train accuracy: 0.07764941453933716 Test accuracy: 0.17157131433486938\n","Minibatch at Epoch 13 batch 31 Train accuracy: 0.0563046932220459 Test accuracy: 0.11891543865203857\n","Minibatch at Epoch 13 batch 41 Train accuracy: 0.10176926851272583 Test accuracy: 0.07569634914398193\n","Minibatch at Epoch 13 batch 51 Train accuracy: 0.137384295463562 Test accuracy: 0.09892714023590088\n","Minibatch at Epoch 13 batch 61 Train accuracy: 0.19429123401641846 Test accuracy: 0.06089097261428833\n","Minibatch at Epoch 13 batch 71 Train accuracy: 0.11998730897903442 Test accuracy: -0.0555117130279541\n","Minibatch at Epoch 13 batch 81 Train accuracy: 0.081318199634552 Test accuracy: 0.0817989706993103\n","Minibatch at Epoch 13 batch 91 Train accuracy: 0.09581166505813599 Test accuracy: 0.08076655864715576\n","Minibatch at Epoch 13 batch 101 Train accuracy: 0.06002652645111084 Test accuracy: 0.16269028186798096\n","Minibatch at Epoch 13 batch 111 Train accuracy: 0.17979001998901367 Test accuracy: 0.0008815526962280273\n","Minibatch at Epoch 13 batch 121 Train accuracy: 0.15828263759613037 Test accuracy: 0.05472540855407715\n","Minibatch at Epoch 13 batch 131 Train accuracy: 0.06750333309173584 Test accuracy: 0.13644760847091675\n","Minibatch at Epoch 13 batch 141 Train accuracy: 0.07028079032897949 Test accuracy: 0.07061207294464111\n","Minibatch at Epoch 13 batch 151 Train accuracy: 0.09387844800949097 Test accuracy: 0.09844112396240234\n","Minibatch at Epoch 13 batch 161 Train accuracy: 0.06859540939331055 Test accuracy: -0.10277819633483887\n","Minibatch at Epoch 13 batch 171 Train accuracy: 0.10111159086227417 Test accuracy: 0.0684969425201416\n","Minibatch at Epoch 13 batch 181 Train accuracy: 0.04817080497741699 Test accuracy: 0.06525921821594238\n","Minibatch at Epoch 13 batch 191 Train accuracy: 0.08525043725967407 Test accuracy: 0.08584070205688477\n","Minibatch at Epoch 13 batch 201 Train accuracy: 0.11360424757003784 Test accuracy: -0.6818044185638428\n","Minibatch at Epoch 13 batch 211 Train accuracy: 0.08773112297058105 Test accuracy: 0.14997345209121704\n","Minibatch at Epoch 13 batch 221 Train accuracy: 0.11134076118469238 Test accuracy: 0.17922741174697876\n","Minibatch at Epoch 13 batch 231 Train accuracy: 0.09433752298355103 Test accuracy: 0.1066707968711853\n","Minibatch at Epoch 13 batch 241 Train accuracy: 0.057525694370269775 Test accuracy: -0.4154818058013916\n","Minibatch at Epoch 13 batch 251 Train accuracy: 0.0680704116821289 Test accuracy: 0.07988524436950684\n","Minibatch at Epoch 13 batch 261 Train accuracy: 0.15105891227722168 Test accuracy: 0.07193183898925781\n","Minibatch at Epoch 13 batch 271 Train accuracy: 0.02899038791656494 Test accuracy: -0.23313283920288086\n","Minibatch at Epoch 13 batch 281 Train accuracy: 0.3140178322792053 Test accuracy: 0.20527547597885132\n","Minibatch at Epoch 13 batch 291 Train accuracy: 0.07558536529541016 Test accuracy: 0.03401148319244385\n","Minibatch at Epoch 13 batch 301 Train accuracy: 0.1255507469177246 Test accuracy: 0.07043784856796265\n","Minibatch at Epoch 13 batch 311 Train accuracy: 0.03273165225982666 Test accuracy: 0.08796048164367676\n","Minibatch at Epoch 13 batch 321 Train accuracy: 0.08775615692138672 Test accuracy: 0.07430416345596313\n","Minibatch at Epoch 13 batch 331 Train accuracy: 0.07502990961074829 Test accuracy: 0.08219397068023682\n","Minibatch at Epoch 13 batch 341 Train accuracy: 0.10204869508743286 Test accuracy: 0.054878830909729004\n","Minibatch at Epoch 13 batch 351 Train accuracy: 0.11763519048690796 Test accuracy: 0.07515591382980347\n","Minibatch at Epoch 13 batch 361 Train accuracy: 0.3123908042907715 Test accuracy: 0.08252418041229248\n","Minibatch at Epoch 13 batch 371 Train accuracy: 0.25492626428604126 Test accuracy: 0.022425293922424316\n","Minibatch at Epoch 13 batch 381 Train accuracy: 0.0491446852684021 Test accuracy: 0.07724374532699585\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 13 Hybrid Test accuracy: 0.1436716914176941 Dice Test accuracy: 0.07128280401229858 Focal Test accuracy: 0.7563989758491516 MSE Test accuracy: 0.950263399630785\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 14 batch 1 Train accuracy: 0.10056215524673462 Test accuracy: 0.10408240556716919\n","Minibatch at Epoch 14 batch 11 Train accuracy: 0.3644906282424927 Test accuracy: 0.13690048456192017\n","Minibatch at Epoch 14 batch 21 Train accuracy: 0.06334537267684937 Test accuracy: 0.10360097885131836\n","Minibatch at Epoch 14 batch 31 Train accuracy: 0.2625592350959778 Test accuracy: 0.05665940046310425\n","Minibatch at Epoch 14 batch 41 Train accuracy: 0.19623351097106934 Test accuracy: 0.31959015130996704\n","Minibatch at Epoch 14 batch 51 Train accuracy: 0.26858818531036377 Test accuracy: 0.16334980726242065\n","Minibatch at Epoch 14 batch 61 Train accuracy: 0.2997649312019348 Test accuracy: 0.09236025810241699\n","Minibatch at Epoch 14 batch 71 Train accuracy: 0.0466998815536499 Test accuracy: 0.06386888027191162\n","Minibatch at Epoch 14 batch 81 Train accuracy: 0.24683910608291626 Test accuracy: 0.048920273780822754\n","Minibatch at Epoch 14 batch 91 Train accuracy: 0.03927779197692871 Test accuracy: 0.06947451829910278\n","Minibatch at Epoch 14 batch 101 Train accuracy: 0.10283476114273071 Test accuracy: 0.054271161556243896\n","Minibatch at Epoch 14 batch 111 Train accuracy: 0.27512073516845703 Test accuracy: 0.0876462459564209\n","Minibatch at Epoch 14 batch 121 Train accuracy: 0.23818957805633545 Test accuracy: 0.27000725269317627\n","Minibatch at Epoch 14 batch 131 Train accuracy: 0.27985286712646484 Test accuracy: 0.051038146018981934\n","Minibatch at Epoch 14 batch 141 Train accuracy: 0.08260840177536011 Test accuracy: 0.06748783588409424\n","Minibatch at Epoch 14 batch 151 Train accuracy: 0.07038605213165283 Test accuracy: 0.05885046720504761\n","Minibatch at Epoch 14 batch 161 Train accuracy: 0.10335862636566162 Test accuracy: 0.04162406921386719\n","Minibatch at Epoch 14 batch 171 Train accuracy: 0.06064265966415405 Test accuracy: 0.08188599348068237\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-0de08fd3b92f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchose_train_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#chose_train_restore_model_2()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-143dddd4c4da>\u001b[0m in \u001b[0;36mchose_train_restore\u001b[0;34m(learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m#with tf.device('/cpu:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mepoch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_batch_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermute_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_h5py_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermute_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0msess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m#print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-eba05f5dac84>\u001b[0m in \u001b[0;36mrandom_h5py_batch\u001b[0;34m(current_batch_no, permute_mat)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermute_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_batch_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mcurrent_batch_no\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mread_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trying_inception@2D_segmentation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"cell_type":"markdown","source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","\n","residual network in each convolutional block\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","2. The problem of class imbalance still persists and dice coeff is more than 1. So i this version we have implemented a new dice coefficient function.\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"metadata":{"id":"eSGVMeVbibw0","colab_type":"code","outputId":"fa55b93e-f8e4-4bf4-98a1-910594102bc3","executionInfo":{"status":"ok","timestamp":1555850728279,"user_tz":-330,"elapsed":50932,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"cell_type":"code","source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"cell_type":"code","source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"cell_type":"code","source":["'''DEFINING VARIABLES'''\n","\n","batch_size=50              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","X = tf.placeholder(shape=[None,240,240,4], dtype=tf.float32, name='input_image')\n","y = tf.placeholder(shape=[240,240,None,1], dtype=tf.int64, name='hot_encode_label')\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size,1),dtype=np.int64)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"cell_type":"markdown","source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"cell_type":"code","source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","    '''Defining a filter variable to perform convolution'''\n","    return (tf.Variable(tf.random_normal([ker_size,ker_size,in_chan,out_chan],stddev=0.05),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","    '''Defining a filter variable to perform transpose convolution'''\n","    return (tf.Variable(tf.random_normal([ker_size,ker_size,out_chan,in_chan],stddev=0.05),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","    '''Function to perform Convolution and apply activation filter'''\n","    '''Convolution'''\n","    conv = tf.nn.conv2d(input_im,filter_mask,strides = [1,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","  '''Function to perform Transpose Convolution and apply activation filter'''\n","  '''Transpose Convolution'''\n","  inp_shape = np.shape(input_im) #tf.shape()\n","  out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2), int(inp_shape[3].value/2)]\n","    \n","  conv = tf.nn.conv2d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,1], padding = \"SAME\",name=name)\n","  #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","  '''Activation'''\n","  if activation == 'relu':\n","      return(tf.nn.relu(conv))\n","  elif activation == 'softmax':\n","      return(tf.nn.softmax(conv))\n","  elif activation == 'elu':\n","      return(tf.nn.elu(conv))\n","  else:\n","      #activation == 'None'\n","      return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"cell_type":"code","source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","  '''normalization of each input channels'''\n","  global out_img\n","  '''CHANNEL INFO'''\n","  # maximum value found using function called \"Finding_maximum_to_normalise \"\n","  #'''max value for dimension 4 is 5337.0'''\n","  #'''max value for dimension 3 is 11737.0'''\n","  #'''max value for dimension 2 is 9751.0'''\n","  #'''max value for dimension 1 is 6476.0'''\n","  out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","  out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","  out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","  out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","  '''Function to take batches randomly'''\n","  global out_img\n","  global out_label\n","  \n","  '''training info'''\n","  train_info = 50*155  #100 patients with 155 images each\n","\n","  if current_batch_no == 0:\n","      no_of_batches = train_info//batch_size  \n","      permute_mat = np.random.permutation(no_of_batches)\n","  \n","  start = permute_mat[current_batch_no]*batch_size\n","  end = start + batch_size\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","  current_batch_no += 1\n","  #print(len(out_img))\n","  '''Input normalization'''\n","  normalizing_input()\n","  '''normalization oof labels'''\n","  #out_label = out_label\n","  '''converting multi class to dual class'''\n","  #out_label = convert_dual_class(out_label)\n","  \n","  '''Rolling axes'''\n","  out_img_send = np.rollaxis(out_img,2, 0)\n","  '''hot encoding'''\n","  out_label_send = out_label\n","    \n","  last=0\n","  if current_batch_no == len(permute_mat):\n","      last=1\n","    \n","  return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","\n","def test_batch():\n","  '''Function to take next test batch'''\n","  \n","  global out_img\n","  global out_label\n","    \n","  '''training and testing info'''\n","  train_info = 50*155  #100 patients with 155 images each\n","  test_info = 184*155  #100 patients with 155 images each\n","    \n","  no_of_batches = test_info//batch_size  \n","  permute_mat = np.random.permutation(no_of_batches)\n","  start = (permute_mat[0]*batch_size) +train_info\n","  end = start + batch_size\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","  '''normalization'''\n","  normalizing_input()\n","  '''Converting multiclass to dual class'''\n","  #out_label = convert_dual_class(out_label) \n","  '''rolling axis'''\n","  out_img_send = np.rollaxis(out_img,2, 0)\n","  '''hot encoding'''\n","  out_label_send = (out_label)\n","    \n","  return (out_img_send,out_label_send)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0-NTV_zXbaY7","colab_type":"code","colab":{}},"cell_type":"code","source":["'''loss function definition'''\n","\n","\n","###############################################################################################################################\n","#LOSS FUNCTION IN VERSION 1 OF TRIAL 2\n","###############################################################################################################################\n","def dice_coeff(y_true, y_pred):\n","  '''Finding dice coefficient for one class'''\n","  flat_layer = tf.layers.Flatten()\n","  y_true_f = flat_layer(y_true)\n","  y_pred_f = flat_layer(y_pred)\n","  intersection = tf.math.reduce_sum(y_true_f*y_pred_f)\n","  return (2*intersection )/(tf.math.reduce_sum(y_true_f)+tf.math.reduce_sum(y_pred_f)+1 )\n","\n","  \n","def dice_coef_multilabel(y_true, y_pred, numLabels = n_class):\n","  '''Finding dice loss for each class'''\n","  dice = 0\n","  \n","    \n","  for index in range(numLabels):\n","    #weight of each class(not in previous versions)\n","    frequency = (tf.reduce_sum(y_true[:,:,:,index,0]))\n","    #if frequency == 0 or frequency == 240*240*batch_size:\n","      #weight = 0\n","    #else:\n","      #weight = batch_size/frequency\n","    if index == 0:\n","      weight = 0\n","    else:\n","      weight = 1\n","    '''Here, as of now we are neglecting the background class'''\n","    dice -= (weight*dice_coeff(y_true[:,:,:,index,0],y_pred[:,:,:,index]))\n","    #print(weight)\n","  return (dice)\n","#############################################################################################################################\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","  '''function for hot encoding images'''\n","  a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","  b = tf.transpose(a,perm=[2,0,1,4,3])\n","  return b\n","\n","\n","#############################################################################################################################\n","#NEW DICE LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_dice_coeff(y_true, y_pred):\n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2])\n","    w = 1/((w**2)+0.000001)\n","    print(np.shape(w))\n","    # Compute gen dice coef:\n","    numerator = y_true*y_pred\n","    denominator = y_true+y_pred\n","    a=b=0\n","    for i in range(np.shape(w)[0]):\n","      a += w[i]*numerator[:,:,:,i]\n","      b += w[i]*denominator[:,:,:,i]\n","      \n","    \n","    num = tf.reduce_sum(a)\n","    den = tf.reduce_sum(b)\n","\n","    gen_dice_coef = num/den\n","\n","    return (-gen_dice_coef)\n","  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"cell_type":"markdown","source":["INITIALIZING THE MODEL FILTERS\n","\n","\n","Filters with Resnet modifications"]},{"metadata":{"id":"oHMGiKsBL-QK","colab_type":"code","outputId":"8f0308e4-a918-4ea4-cbe6-4aa278cc793e","executionInfo":{"status":"ok","timestamp":1555422895341,"user_tz":-330,"elapsed":1867,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"cell_type":"code","source":["def resnet_filter_def():\n","  '''MODEL1 Filter definition'''\n","  '''LEFT'''\n","\n","  filter1 = left_filter_def(3,4,8,name='filter1')\n","  filter2 = left_filter_def(3,8,8,name='filter2')\n","  filter3 = left_filter_def(3,8,8,name='filter3')\n","\n","  filter4 = left_filter_def(3,8,16,name='filter4')\n","  filter5 = left_filter_def(3,16,16,name='filter5')\n","  filter6 = left_filter_def(3,16,16,name='filter6')\n","\n","  filter7 = left_filter_def(3,16,32,name='filter7')\n","  filter8 = left_filter_def(3,32,32,name='filter8')\n","  filter9 = left_filter_def(3,32,32,name='filter9')\n","\n","  filter10 = left_filter_def(3,32,64,name='filter10')\n","  filter11 = left_filter_def(3,64,64,name='filter11')\n","  filter12 = left_filter_def(3,64,64,name='filter12')\n","\n","  filter13 = left_filter_def(3,64,128,name='filter13')\n","  filter14= left_filter_def(3,128,64,name='filter14')\n","  filter15= left_filter_def(3,128,64,name='filter15')\n","\n","  '''RIGHT'''\n","\n","  filter16 = right_filter_def(3,128,64,name='filter16')\n","  filter17 = left_filter_def(3,64,64,name='filter17')\n","  filter18 = left_filter_def(3,64,64,name='filter18')\n","  filter19 = left_filter_def(3,64,32,name='filter19')\n","\n","  filter20 = right_filter_def(3,64,32,name='filter20')\n","  filter21 = left_filter_def(3,32,32,name='filter21')\n","  filter22 = left_filter_def(3,32,32,name='filter22')\n","  filter23 = left_filter_def(3,32,16,name='filter23')\n","\n","  filter24 = right_filter_def(3,32,16,name='filter24')\n","  filter25 = left_filter_def(3,16,16,name='filter25')\n","  filter26 = left_filter_def(3,16,16,name='filter26')\n","  filter27 = left_filter_def(3,16,8,name='filter27')\n","\n","  filter28 = right_filter_def(3,16,8,name='filter28')\n","  filter29 = left_filter_def(3,8,8,name='filter29')\n","  filter30 = left_filter_def(3,8,8,name='filter30')\n","  filter31 = left_filter_def(3,8,n_class,name='filter31')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"g43zk2ajZ-Yt","colab_type":"code","colab":{}},"cell_type":"code","source":["#def inception_filter_def():\n","'''Scaling before inception'''\n","Scaling_filter0 = left_filter_def(3,4,8,name='Sfilter0')\n","'''1st inception block'''\n","filter1 = left_filter_def(1,8,(2*8),name='layer1_line1_filter1')\n","filter2 = left_filter_def(3,(2*8),(4*8),name='layer1_line1_filter2')\n","filter3 = left_filter_def(1,8,(2*8),name='layer1_line2_filter1')\n","filter4 = left_filter_def(3,(2*8),(4*8),name='layer1_line2_filter2')\n","filter5 = left_filter_def(3,(4*8),(4*8),name='layer1_line2_filter3')\n","filter6 = left_filter_def(1,8,(2*8),name='layer1_line3_filter1')\n","filter7 = left_filter_def(3,(8*10),8,name='layer1_final_filter1')  #in channels = (out cahnnens of all the three paths)\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter1 = left_filter_def(3,8,16,name='Sfilter1')\n","'''2nd inception block'''\n","filter8 = left_filter_def(1,16,(2*16),name='layer2_line1_filter1')\n","filter9 = left_filter_def(3,(2*16),(4*16),name='layer2_line1_filter2')\n","filter10= left_filter_def(1,16,(2*16),name='layer2_line2_filter1')\n","filter11= left_filter_def(3,(2*16),(4*16),name='layer2_line2_filter2')\n","filter12= left_filter_def(3,(4*16),(4*16),name='layer2_line2_filter3')\n","filter13= left_filter_def(1,16,(2*16),name='layer2_line3_filter1')\n","filter14= left_filter_def(3,(16*10),16,name='layer2_final_filter1')\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter2 = left_filter_def(3,16,32,name='Sfilter2')\n","'''3rd inception block'''\n","filter15= left_filter_def(1,32,(2*32),name='layer3_line1_filter1')\n","filter16= left_filter_def(3,(2*32),(4*32),name='layer3_line1_filter2')\n","filter17= left_filter_def(1,(32),(2*32),name='layer3_line2_filter1')\n","filter18= left_filter_def(3,(2*32),(4*32),name='layer3_line2_filter2')\n","filter19= left_filter_def(3,(4*32),(4*32),name='layer3_line2_filter3')\n","filter20= left_filter_def(1,32,(2*32),name='layer3_line3_filter1')\n","filter21= left_filter_def(3,(10*32),32,name='layer3_final_filter1')\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter3 = left_filter_def(3,32,64,name='Sfilter3')\n","'''4th inception block'''\n","filter22= left_filter_def(1,64,(2*64),name='layer4_line1_filter1')\n","filter23= left_filter_def(3,(2*64),(4*64),name='layer4_line1_filter2')\n","filter24= left_filter_def(1,64,(2*64),name='layer4_line2_filter1')\n","filter25= left_filter_def(3,(2*64),(4*64),name='layer4_line2_filter2')\n","filter26= left_filter_def(3,(4*64),(4*64),name='layer4_line2_filter3')\n","filter27= left_filter_def(1,(64),(2*64),name='layer4_line3_filter1')\n","filter28= left_filter_def(3,(10*64),64,name='layer4_final_filter1')\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter4 = left_filter_def(3,64,128,name='Sfilter4')\n","'''5th inception block'''\n","filter29= left_filter_def(1,128,(2*128),name='layer5_line1_filter1')\n","filter30= left_filter_def(3,(2*128),(4*128),name='layer5_line1_filter2')\n","filter31= left_filter_def(1,128,(2*128),name='layer5_line2_filter1')\n","filter32= left_filter_def(3,(2*128),(4*128),name='layer5_line2_filter2')\n","filter33= left_filter_def(3,(4*128),(4*128),name='layer5_line2_filter3')\n","filter34= left_filter_def(1,128,(2*128),name='layer5_line3_filter1')\n","filter35= left_filter_def(3,(10*128),128,name='layer5_final_filter1')\n","'''Scaling FIlter'''\n","Scaling_filter5 = left_filter_def(3,128,64,name='Sfilter5')\n"," \n","'''Right side'''\n","filter36 = right_filter_def(3,128,64,name='filter16')\n","filter37 = left_filter_def(3,64,64,name='filter17')\n","filter38 = left_filter_def(3,64,64,name='filter18')\n","filter39 = left_filter_def(3,64,32,name='filter19')\n","\n","filter40 = right_filter_def(3,64,32,name='filter20')\n","filter41 = left_filter_def(3,32,32,name='filter21')\n","filter42 = left_filter_def(3,32,32,name='filter22')\n","filter43 = left_filter_def(3,32,16,name='filter23')\n","\n","filter44 = right_filter_def(3,32,16,name='filter24')\n","filter45 = left_filter_def(3,16,16,name='filter25')\n","filter46 = left_filter_def(3,16,16,name='filter26')\n","filter47 = left_filter_def(3,16,8,name='filter27')\n","\n","filter48 = right_filter_def(3,16,8,name='filter28')\n","filter49 = left_filter_def(3,8,8,name='filter29')\n","filter50 = left_filter_def(3,8,8,name='filter30')\n","filter51 = left_filter_def(3,8,n_class,name='filter31')\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"cell_type":"markdown","source":["DEFINING THE MODEL STRUCTURE\n","\n","Model with Resnet Modification"]},{"metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_model1(X):\n","    \n","    '''Function to define the UNET model'''\n","    S_CNN0 = Conv_layer(X,Scaling_filter0,stride=1,activation='relu',name='Scaling_CNN0')\n","    #print (\"S_CNN0\",np.shape(S_CNN0))\n","    with tf.name_scope(\"BLOCK1\"):\n","      '''BLOCK1'''\n","      CNN1 = Conv_layer(S_CNN0,filter1,stride=1,activation='relu',name='layer1_line1_CNN1')\n","      #print (\"CNN1\",np.shape(CNN1))\n","      CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='layer1_line1_CNN2')\n","      #print (\"CNN2\",np.shape(CNN2))\n","      CNN3 = Conv_layer(S_CNN0,filter3,stride=1,activation='relu',name='layer1_line2_CNN1')\n","      #print (\"CNN3\",np.shape(CNN3))\n","      CNN4 = Conv_layer(CNN3,filter4,stride=1,activation='relu',name='layer1_line2_CNN2')\n","      #print (\"CNN4\",np.shape(CNN4))\n","      CNN5 = Conv_layer(CNN4,filter5,stride=1,activation='relu',name='layer1_line2_CNN3')\n","      #print (\"CNN5\",np.shape(CNN5))\n","      CNN6 = Conv_layer(S_CNN0,filter6,stride=1,activation='relu',name='layer1_line3_CNN1')\n","      #print (\"CNN6\",np.shape(CNN6))\n","      combo1 = tf.concat([CNN2,CNN5,CNN6],axis=3,name='COMBO1')\n","      #print (\"COMBO1\",np.shape(combo1))\n","      CNN7 = Conv_layer(combo1,filter7,stride=1,activation='relu',name='layer1_final_CNN1')\n","      #print (\"CNN7\",np.shape(CNN7))\n","      block1_out = tf.add(CNN7,S_CNN0,name='inception1')\n","      #print (\"BLOCK1_OUTPUT\",np.shape(block1_out))\n","    pool1 = tf.nn.max_pool(block1_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL1')\n","    S_CNN1 = Conv_layer(pool1,Scaling_filter1,stride=1,activation='relu',name='Scaling_CNN1')\n","    #print (\"S_NN1\",np.shape(S_CNN1))\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","      '''BLOCK2'''\n","      CNN8 = Conv_layer(S_CNN1,filter8,stride=1,activation='relu',name='layer2_line1_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN9 = Conv_layer(CNN8,filter9,stride=1,activation='relu',name='layer2_line1_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN10= Conv_layer(S_CNN1,filter10,stride=1,activation='relu',name='layer2_line2_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN11= Conv_layer(CNN10,filter11,stride=1,activation='relu',name='layer2_line2_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN12= Conv_layer(CNN11,filter12,stride=1,activation='relu',name='layer2_line2_CNN3')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN13= Conv_layer(S_CNN1,filter13,stride=1,activation='relu',name='layer2_line3_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      combo2 = tf.concat([CNN9,CNN12,CNN13],axis=3,name='COMBO2')\n","      #rint (\"Combo\",np.shape(combo2))\n","      CNN14= Conv_layer(combo2,filter14,stride=1,activation='relu',name='layer2_final_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      block2_out = tf.add(CNN14,S_CNN1,name='inception2')\n","      \n","    pool2 = tf.nn.max_pool(block2_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL1')\n","    S_CNN2 = Conv_layer(pool2,Scaling_filter2,stride=1,activation='relu',name='Scaling_CNN2')\n","    #rint (\"CNN1\",np.shape(CNN1))\n","    \n","    with tf.name_scope(\"BLOCK3\"):\n","      '''BLOCK3'''\n","      CNN15= Conv_layer(S_CNN2,filter15,stride=1,activation='relu',name='layer3_line1_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN16= Conv_layer(CNN15,filter16,stride=1,activation='relu',name='layer3_line1_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN17= Conv_layer(S_CNN2,filter17,stride=1,activation='relu',name='layer3_line2_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN18= Conv_layer(CNN17,filter18,stride=1,activation='relu',name='layer3_line2_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN19= Conv_layer(CNN18,filter19,stride=1,activation='relu',name='layer3_line2_CNN3')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN20= Conv_layer(S_CNN2,filter20,stride=1,activation='relu',name='layer3_line3_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      combo3 = tf.concat([CNN16,CNN19,CNN20],axis=3,name='COMBO3')\n","      #rint (\"Combo\",np.shape(combo2))\n","      CNN21= Conv_layer(combo3,filter21,stride=1,activation='relu',name='layer3_final_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      block3_out = tf.add(CNN21,S_CNN2,name='inception3')\n","      \n","    pool3 = tf.nn.max_pool(block3_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL3')\n","    S_CNN3 = Conv_layer(pool3,Scaling_filter3,stride=1,activation='relu',name='Scaling_CNN3')\n","    #rint (\"CNN1\",np.shape(CNN1))\n","    \n","    with tf.name_scope(\"BLOCK4\"):\n","      '''BLOCK4'''\n","      CNN22= Conv_layer(S_CNN3,filter22,stride=1,activation='relu',name='layer4_line1_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN23= Conv_layer(CNN22,filter23,stride=1,activation='relu',name='layer4_line1_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN24= Conv_layer(S_CNN3,filter24,stride=1,activation='relu',name='layer4_line2_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN25= Conv_layer(CNN24,filter25,stride=1,activation='relu',name='layer4_line2_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN26= Conv_layer(CNN25,filter26,stride=1,activation='relu',name='layer4_line2_CNN3')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN27= Conv_layer(S_CNN3,filter27,stride=1,activation='relu',name='layer4_line3_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      combo4 = tf.concat([CNN23,CNN26,CNN27],axis=3,name='COMBO4')\n","      #rint (\"Combo\",np.shape(combo2))\n","      CNN28= Conv_layer(combo4,filter28,stride=1,activation='relu',name='layer4_final_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      block4_out = tf.add(CNN28,S_CNN3,name='inception4')\n","      \n","    pool4 = tf.nn.max_pool(block4_out,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL4')\n","    S_CNN4 = Conv_layer(pool4,Scaling_filter4,stride=1,activation='relu',name='Scaling_CNN4')\n","    #rint (\"CNN1\",np.shape(CNN1))\n","    \n","    with tf.name_scope(\"BLOCK5\"):\n","      '''BLOCK5'''\n","      CNN29= Conv_layer(S_CNN4,filter29,stride=1,activation='relu',name='layer5_line1_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN30= Conv_layer(CNN29,filter30,stride=1,activation='relu',name='layer5_line1_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN31= Conv_layer(S_CNN4,filter31,stride=1,activation='relu',name='layer5_line2_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN32= Conv_layer(CNN31,filter32,stride=1,activation='relu',name='layer5_line2_CNN2')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN33= Conv_layer(CNN32,filter33,stride=1,activation='relu',name='layer5_line2_CNN3')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      CNN34= Conv_layer(S_CNN4,filter34,stride=1,activation='relu',name='layer5_line3_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      combo5 = tf.concat([CNN34,CNN33,CNN30],axis=3,name='COMBO4')\n","      #rint (\"Combo\",np.shape(combo2))\n","      CNN35= Conv_layer(combo5,filter35,stride=1,activation='relu',name='layer5_final_CNN1')\n","      #rint (\"CNN1\",np.shape(CNN1))\n","      block5_out = tf.add(CNN35,S_CNN4,name='inception5')\n","      \n","    S_CNN5 = Conv_layer(block5_out,Scaling_filter5,stride=1,activation='relu',name='Scaling_CNN5')\n","    \n","    '''Moving UP'''\n","    \n","    with tf.name_scope(\"BLOCK6\"):\n","      '''BLOCK6'''\n","      concat1 = tf.concat([S_CNN5,pool4],axis=3,name='CONCAT1')\n","      #rint (\"concat1\",np.shape(concat1))\n","      DCNN1= Deconv_layer(concat1,filter36,stride=2,activation='relu',name='DE_CONV1')\n","      #rint (\"DCNN1\",np.shape(DCNN1))\n","      CNN36 = Conv_layer(DCNN1,filter37,stride=1,activation='relu',name='CNN36')\n","      #rint (\"CNN11\",np.shape(CNN11))\n","      CNNR1 = tf.add(Conv_layer(CNN36,filter38,stride=1,activation='relu',name='CNNR1'), DCNN1, name='Resnet6')\n","      #rint (\"CNNR6\",np.shape(CNNR6))\n","      CNN37 = Conv_layer(CNNR1,filter39,stride=1,activation='relu',name='CNN37')\n","      #rint (\"CNN12\",np.shape(CNN12))\n","    \n","    with tf.name_scope(\"BLOCK7\"):\n","      '''BLOCK7'''\n","      concat2 = tf.concat([CNN37,pool3],axis=3,name='CONCAT2')\n","      #rint (\"concat2\",np.shape(concat2))\n","      DCNN2= Deconv_layer(concat2,filter40,stride=2,activation='relu',name='DE_CONV2')\n","      #rint (\"DCNN2\",np.shape(DCNN2))\n","      CNN38 = Conv_layer(DCNN2,filter41,stride=1,activation='relu',name='CNN38')\n","      #rint (\"CNN13\",np.shape(CNN13))\n","      CNNR2 = tf.add(Conv_layer(CNN38,filter42,stride=1,activation='relu',name='CNNR2'), DCNN2, name='Resnet7')\n","      #rint (\"CNNR7\",np.shape(CNNR7))\n","      CNN39 = Conv_layer(CNNR2,filter43,stride=1,activation='relu',name='CNN39')\n","      #rint (\"CNN14\",np.shape(CNN14))\n","    \n","    with tf.name_scope(\"BLOCK8\"):\n","      '''BLOCK8'''\n","      concat3 = tf.concat([CNN39,pool2],axis=3,name='CONCAT3')\n","      #rint (\"concat3\",np.shape(concat3))\n","      DCNN3= Deconv_layer(concat3,filter44,stride=2,activation='relu',name='DE_CONV3')\n","      #rint (\"DCNN3\",np.shape(DCNN3))\n","      CNN40 = Conv_layer(DCNN3,filter45,stride=1,activation='relu',name='CNN40')\n","      #rint (\"CNN15\",np.shape(CNN15))\n","      CNNR3 = tf.add(Conv_layer(CNN40,filter46,stride=1,activation='relu',name='CNNR3'), DCNN3, name='Resnet8')\n","      #rint (\"CNNR8\",np.shape(CNNR8))\n","      CNN41 = Conv_layer(CNNR3,filter47,stride=1,activation='relu',name='CNN41')\n","      #rint (\"CNN16\",np.shape(CNN16))\n","      \n","    with tf.name_scope(\"BLOCK9\"):\n","      '''BLOCK9'''\n","      concat4 = tf.concat([CNN41,pool1],axis=3,name='CONCAT4')\n","      #rint (\"concat4\",np.shape(concat4))\n","      DCNN4= Deconv_layer(concat4,filter48,stride=2,activation='relu',name='DE_CONV4')\n","      #rint (\"DCNN4\",np.shape(DCNN4))\n","      CNN42 = Conv_layer(DCNN4,filter49,stride=1,activation='relu',name='CNN42')\n","      #rint (\"CNN17\",np.shape(CNN17))\n","      CNNR4 = tf.add(Conv_layer(CNN42,filter50,stride=1,activation='relu',name='CNNR4'), DCNN4, name='Resnet9')\n","      #rint (\"CNNR9\",np.shape(CNNR9))\n","      CNN43 = Conv_layer(CNNR4,filter51,stride=1,activation='relu',name='CNN43')\n","      #print (\"CNN43\",np.shape(CNN43))\n","      return (CNN43)\n","\n","#a = (predict_model1(X))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c_ROD14VTzmB","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_unet(learning_rate =0.0001,n_epochs = 100):\n","  '''Function to Train U-Net'''\n","  prediction = predict_model1(X)#logits\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"LOSS_FUNCTION\"):\n","    '''using multi dimensional dice'''\n","    hot_y = hot_encode(y)\n","    #dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","    dice = 1 + generalized_dice_coeff(hot_y[:,:,:,:,0], prediction)\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"COST_FUNCTION\"):\n","    '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","    loss = tf.reduce_mean(dice, name=\"loss\")\n","\n","  with tf.name_scope(\"OPTIMIZER\"):\n","    '''Optimizer'''\n","    optimize = tf.train.AdamOptimizer(learning_rate = learning_rate)\n","    training_output = optimize.minimize(loss)\n","\n","\n","  '''initializing'''\n","  init = tf.global_variables_initializer()\n","  saver = tf.train.Saver()\n","\n","  '''Timing'''\n","  start = time.time()\n","  '''Session'''\n","  with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","      current_batch_no = 0\n","      permute_mat = 0\n","      iteration = 0\n","      while(1):\n","        epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","        dice_val,sess_results = sess.run([dice,training_output], feed_dict={X: epoch_x, y: epoch_y})\n","        #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","            \n","        '''DICE Coefficient for iteration'''\n","        if iteration%10==0:\n","          acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","          test_images, test_labels = test_batch()\n","          acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","          print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","        if last ==1:\n","          break\n","        iteration +=1\n","      test_images, test_labels = test_batch()\n","      acc_test = 1-dice.eval(feed_dict={X: test_images, y: test_labels})\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","      print(\"After Epoch\", epoch+1, \"Test accuracy:\", acc_test)\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","        \n","        \n","      if epoch % 1 == 0:\n","        test_example =   test_images\n","        test_example_gt = np.rollaxis(test_labels,2,0)\n","        sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","        sess_results = sess_results[0,:,:,1] + (2*sess_results[0,:,:,2]) + (3*sess_results[0,:,:,3])\n","        test_example = test_example[0,:,:,3]\n","        test_example_gt = test_example_gt[0,:,:,:]\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Original Image')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/completely_different_model_stratergy/trial_inception/'+str(epoch)+\"a_Original_Image.png\")\n","              \n","              \n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Ground Truth Mask')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/completely_different_model_stratergy/trial_inception/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(sess_results),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Generated Mask')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/completely_different_model_stratergy/trial_inception/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","        plt.close('all')\n","\n","    '''Saving the graph'''\n","    save_path = saver.save(sess, \"/content/gdrive/My Drive/Brain_Tumour_segmentation/completely_different_model_stratergy/final_madel_graph\")\n","  end = time.time()\n","  total_time = end-start\n","  return (total_time)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"cell_type":"markdown","source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"7c39ccec-40bc-43f4-8d8e-ac40b47ef8ad","executionInfo":{"status":"error","timestamp":1554878659087,"user_tz":-330,"elapsed":526824,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":1890}},"cell_type":"code","source":["train_unet()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(4,)\n","Minibatch at Epoch 1 batch 1 Train accuracy: 0.0 Test accuracy: 0.0005258321762084961\n","Minibatch at Epoch 1 batch 11 Train accuracy: 0.0008528232574462891 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 21 Train accuracy: 0.001278996467590332 Test accuracy: 0.00019317865371704102\n","Minibatch at Epoch 1 batch 31 Train accuracy: 0.0 Test accuracy: 0.0004798769950866699\n","Minibatch at Epoch 1 batch 41 Train accuracy: 0.0016903877258300781 Test accuracy: 0.0017605423927307129\n","Minibatch at Epoch 1 batch 51 Train accuracy: 0.0013685226440429688 Test accuracy: 0.0025460124015808105\n","Minibatch at Epoch 1 batch 61 Train accuracy: 0.0 Test accuracy: 0.00237429141998291\n","Minibatch at Epoch 1 batch 71 Train accuracy: 0.0005920529365539551 Test accuracy: 0.004571259021759033\n","Minibatch at Epoch 1 batch 81 Train accuracy: 0.0 Test accuracy: 0.004060149192810059\n","Minibatch at Epoch 1 batch 91 Train accuracy: 0.0 Test accuracy: 0.003999888896942139\n","Minibatch at Epoch 1 batch 101 Train accuracy: 0.004979908466339111 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 111 Train accuracy: 0.00437086820602417 Test accuracy: 0.0004100799560546875\n","Minibatch at Epoch 1 batch 121 Train accuracy: 0.0 Test accuracy: 0.00023037195205688477\n","Minibatch at Epoch 1 batch 131 Train accuracy: 0.0 Test accuracy: 0.00572359561920166\n","Minibatch at Epoch 1 batch 141 Train accuracy: 0.0017718076705932617 Test accuracy: 2.8014183044433594e-05\n","Minibatch at Epoch 1 batch 151 Train accuracy: 0.13502413034439087 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 1 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 2 batch 1 Train accuracy: 0.0 Test accuracy: 0.014817237854003906\n","Minibatch at Epoch 2 batch 11 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 21 Train accuracy: 0.012572169303894043 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 31 Train accuracy: 0.0 Test accuracy: 0.05002719163894653\n","Minibatch at Epoch 2 batch 41 Train accuracy: 0.08061790466308594 Test accuracy: 0.000943303108215332\n","Minibatch at Epoch 2 batch 51 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 61 Train accuracy: 0.0095747709274292 Test accuracy: 0.03192400932312012\n","Minibatch at Epoch 2 batch 71 Train accuracy: 0.021677613258361816 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 81 Train accuracy: 0.0 Test accuracy: 0.004124879837036133\n","Minibatch at Epoch 2 batch 91 Train accuracy: 0.0048552751541137695 Test accuracy: 0.004092037677764893\n","Minibatch at Epoch 2 batch 101 Train accuracy: 0.0 Test accuracy: 0.07698941230773926\n","Minibatch at Epoch 2 batch 111 Train accuracy: 0.07374858856201172 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 121 Train accuracy: 0.24117624759674072 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 131 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 2 batch 141 Train accuracy: 0.3161810636520386 Test accuracy: 0.025119423866271973\n","Minibatch at Epoch 2 batch 151 Train accuracy: 0.0 Test accuracy: 0.23496747016906738\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 2 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 3 batch 1 Train accuracy: 0.0 Test accuracy: 0.10315966606140137\n","Minibatch at Epoch 3 batch 11 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 21 Train accuracy: 0.08691048622131348 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 31 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 41 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 51 Train accuracy: 0.0 Test accuracy: 0.11861956119537354\n","Minibatch at Epoch 3 batch 61 Train accuracy: 0.0 Test accuracy: 1.329183578491211e-05\n","Minibatch at Epoch 3 batch 71 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 81 Train accuracy: 0.010218203067779541 Test accuracy: 0.3110283613204956\n","Minibatch at Epoch 3 batch 91 Train accuracy: 2.980232238769531e-07 Test accuracy: 0.006934106349945068\n","Minibatch at Epoch 3 batch 101 Train accuracy: 0.05307459831237793 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 111 Train accuracy: 0.0 Test accuracy: 5.781650543212891e-06\n","Minibatch at Epoch 3 batch 121 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 3 batch 131 Train accuracy: 0.0 Test accuracy: 0.0006711483001708984\n","Minibatch at Epoch 3 batch 141 Train accuracy: 0.024091780185699463 Test accuracy: 0.06853187084197998\n","Minibatch at Epoch 3 batch 151 Train accuracy: 0.27094602584838867 Test accuracy: 1.7881393432617188e-07\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 3 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 4 batch 1 Train accuracy: 0.001852869987487793 Test accuracy: 0.06340843439102173\n","Minibatch at Epoch 4 batch 11 Train accuracy: 0.0 Test accuracy: 0.16048604249954224\n","Minibatch at Epoch 4 batch 21 Train accuracy: 0.0 Test accuracy: 0.0226820707321167\n","Minibatch at Epoch 4 batch 31 Train accuracy: 0.04193747043609619 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 41 Train accuracy: 0.2333396077156067 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 51 Train accuracy: 0.030159950256347656 Test accuracy: 0.2877618074417114\n","Minibatch at Epoch 4 batch 61 Train accuracy: 0.21320313215255737 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 71 Train accuracy: 0.3030375838279724 Test accuracy: 0.004782676696777344\n","Minibatch at Epoch 4 batch 81 Train accuracy: 0.000307619571685791 Test accuracy: 0.019975721836090088\n","Minibatch at Epoch 4 batch 91 Train accuracy: 1.7881393432617188e-07 Test accuracy: 0.018487095832824707\n","Minibatch at Epoch 4 batch 101 Train accuracy: 0.1570291519165039 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 111 Train accuracy: 0.0766867995262146 Test accuracy: 0.30210351943969727\n","Minibatch at Epoch 4 batch 121 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 131 Train accuracy: 0.4119699001312256 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 141 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 4 batch 151 Train accuracy: 0.0 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 4 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 5 batch 1 Train accuracy: 0.0 Test accuracy: 3.421306610107422e-05\n","Minibatch at Epoch 5 batch 11 Train accuracy: 0.16782259941101074 Test accuracy: 0.00045794248580932617\n","Minibatch at Epoch 5 batch 21 Train accuracy: 0.004608333110809326 Test accuracy: 0.02465212345123291\n","Minibatch at Epoch 5 batch 31 Train accuracy: 0.2836313247680664 Test accuracy: 0.0\n","Minibatch at Epoch 5 batch 41 Train accuracy: 0.0 Test accuracy: 0.3746193051338196\n","Minibatch at Epoch 5 batch 51 Train accuracy: 0.0 Test accuracy: 5.364418029785156e-07\n","Minibatch at Epoch 5 batch 61 Train accuracy: 0.10874086618423462 Test accuracy: 0.0\n","Minibatch at Epoch 5 batch 71 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 5 batch 81 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 5 batch 91 Train accuracy: 0.014892041683197021 Test accuracy: 0.1415693163871765\n","Minibatch at Epoch 5 batch 101 Train accuracy: 0.0 Test accuracy: 0.1161797046661377\n","Minibatch at Epoch 5 batch 111 Train accuracy: 0.4412810802459717 Test accuracy: 0.0\n","Minibatch at Epoch 5 batch 121 Train accuracy: 0.0 Test accuracy: 0.0005744695663452148\n","Minibatch at Epoch 5 batch 131 Train accuracy: 0.0 Test accuracy: 0.008991777896881104\n","Minibatch at Epoch 5 batch 141 Train accuracy: 0.0 Test accuracy: 0.00036203861236572266\n","Minibatch at Epoch 5 batch 151 Train accuracy: 0.08918559551239014 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 5 Test accuracy: 0.3383234739303589\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 6 batch 1 Train accuracy: 0.0 Test accuracy: 0.01870638132095337\n","Minibatch at Epoch 6 batch 11 Train accuracy: 0.0 Test accuracy: 0.027253270149230957\n","Minibatch at Epoch 6 batch 21 Train accuracy: 0.042693495750427246 Test accuracy: 0.3880847096443176\n","Minibatch at Epoch 6 batch 31 Train accuracy: 0.0 Test accuracy: 0.14830446243286133\n","Minibatch at Epoch 6 batch 41 Train accuracy: 0.013739347457885742 Test accuracy: 0.4944026470184326\n","Minibatch at Epoch 6 batch 51 Train accuracy: 0.4112222194671631 Test accuracy: 0.0\n"],"name":"stdout"}]},{"metadata":{"id":"Icc3WNqr_ZYR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
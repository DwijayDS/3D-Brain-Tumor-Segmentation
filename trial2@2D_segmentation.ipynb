{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trial2@2D_segmentation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"cell_type":"markdown","source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"metadata":{"id":"eSGVMeVbibw0","colab_type":"code","outputId":"70ee15bf-786d-4377-f1da-0da01bda0814","executionInfo":{"status":"ok","timestamp":1554887960798,"user_tz":-330,"elapsed":35828,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"cell_type":"code","source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"cell_type":"code","source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"cell_type":"code","source":["'''DEFINING VARIABLES'''\n","\n","batch_size=10              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","X = tf.placeholder(shape=[None,240,240,4], dtype=tf.float32, name='input_image')\n","y = tf.placeholder(shape=[240,240,None,1], dtype=tf.int64, name='hot_encode_label')\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size,1),dtype=np.int64)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"cell_type":"markdown","source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"cell_type":"code","source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","  '''Defining a filter variable to perform convolution'''\n","  return (tf.Variable(tf.random_normal([ker_size,ker_size,in_chan,out_chan],stddev=0.05),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","  '''Defining a filter variable to perform transpose convolution'''\n","  return (tf.Variable(tf.random_normal([ker_size,ker_size,out_chan,in_chan],stddev=0.05),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","  '''Function to perform Convolution and apply activation filter'''\n","  '''Convolution'''\n","  conv = tf.nn.conv2d(input_im,filter_mask,strides = [1,stride,stride,1], padding = \"SAME\",name=name)\n","  #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","  '''Activation'''\n","  if activation == 'relu':\n","      return(tf.nn.relu(conv))\n","  elif activation == 'softmax':\n","      return(tf.nn.softmax(conv))\n","  elif activation == 'elu':\n","      return(tf.nn.elu(conv))\n","  else:\n","      #activation == 'None'\n","      return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","  '''Function to perform Transpose Convolution and apply activation filter'''\n","  '''Transpose Convolution'''\n","  inp_shape = np.shape(input_im) #tf.shape()\n","  out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2), int(inp_shape[3].value/2)]\n","    \n","  conv = tf.nn.conv2d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,1], padding = \"SAME\",name=name)\n","  #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","  '''Activation'''\n","  if activation == 'relu':\n","      return(tf.nn.relu(conv))\n","  elif activation == 'softmax':\n","      return(tf.nn.softmax(conv))\n","  elif activation == 'elu':\n","      return(tf.nn.elu(conv))\n","  else:\n","      #activation == 'None'\n","      return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"cell_type":"code","source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","  '''normalization of each input channels'''\n","  global out_img\n","  '''CHANNEL INFO'''\n","  # maximum value found using function called \"Finding_maximum_to_normalise \"\n","  #'''max value for dimension 4 is 5337.0'''\n","  #'''max value for dimension 3 is 11737.0'''\n","  #'''max value for dimension 2 is 9751.0'''\n","  #'''max value for dimension 1 is 6476.0'''\n","  out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","  out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","  out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","  out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","  '''Function to take batches randomly'''\n","  global out_img\n","  global out_label\n","  \n","  '''training info'''\n","  train_info = 50*155  #100 patients with 155 images each\n","\n","  if current_batch_no == 0:\n","      no_of_batches = train_info//batch_size  \n","      permute_mat = np.random.permutation(no_of_batches)\n","  \n","  start = permute_mat[current_batch_no]*batch_size\n","  end = start + batch_size\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","  current_batch_no += 1\n","  #print(len(out_img))\n","  '''Input normalization'''\n","  normalizing_input()\n","  '''normalization oof labels'''\n","  #out_label = out_label\n","  '''converting multi class to dual class'''\n","  #out_label = convert_dual_class(out_label)\n","  \n","  '''Rolling axes'''\n","  out_img_send = np.rollaxis(out_img,2, 0)\n","  '''hot encoding'''\n","  out_label_send = out_label\n","    \n","  last=0\n","  if current_batch_no == len(permute_mat):\n","      last=1\n","    \n","  return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","\n","def test_batch():\n","  '''Function to take next test batch'''\n","  \n","  global out_img\n","  global out_label\n","    \n","  '''training and testing info'''\n","  train_info = 50*155  #100 patients with 155 images each\n","  test_info = 184*155  #100 patients with 155 images each\n","    \n","  no_of_batches = test_info//batch_size  \n","  permute_mat = np.random.permutation(no_of_batches)\n","  start = (permute_mat[0]*batch_size) +train_info\n","  end = start + batch_size\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","  '''normalization'''\n","  normalizing_input()\n","  '''Converting multiclass to dual class'''\n","  #out_label = convert_dual_class(out_label) \n","  '''rolling axis'''\n","  out_img_send = np.rollaxis(out_img,2, 0)\n","  '''hot encoding'''\n","  out_label_send = (out_label)\n","    \n","  return (out_img_send,out_label_send)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0-NTV_zXbaY7","colab_type":"code","colab":{}},"cell_type":"code","source":["'''loss function definition'''\n","\n","\n","def dice_coeff(y_true, y_pred):\n","  '''Finding dice coefficient for one class'''\n","  flat_layer = tf.layers.Flatten()\n","  y_true_f = flat_layer(y_true)\n","  y_pred_f = flat_layer(y_pred)\n","  intersection = tf.math.reduce_sum(y_true_f*y_pred_f)\n","  return (2*intersection )/(tf.math.reduce_sum(y_true_f)+tf.math.reduce_sum(y_pred_f)+1 )\n","\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","  '''function for hot encoding images'''\n","  a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","  b = tf.transpose(a,perm=[2,0,1,4,3])\n","  return b\n","\n","\n","  \n","def dice_coef_multilabel(y_true, y_pred, numLabels = n_class):\n","  '''Finding dice loss for each class'''\n","  dice = 0\n","  \n","    \n","  for index in range(numLabels):\n","    #weight of each class(not in previous versions)\n","    frequency = (tf.reduce_sum(y_true[:,:,:,index,0]))\n","    #if frequency == 0 or frequency == 240*240*batch_size:\n","      #weight = 0\n","    #else:\n","      #weight = batch_size/frequency\n","    if index == 0:\n","      weight = 0\n","    else:\n","      weight = 1\n","    '''Here, as of now we are neglecting the background class'''\n","    dice -= (weight*dice_coeff(y_true[:,:,:,index,0],y_pred[:,:,:,index]))\n","    #print(weight)\n","  return (dice)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"cell_type":"markdown","source":["INITIALIZING THE MODEL FILTERS"]},{"metadata":{"id":"oHMGiKsBL-QK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"7e9378be-53c2-47c3-8ca3-e191353648d4","executionInfo":{"status":"ok","timestamp":1554887995868,"user_tz":-330,"elapsed":2071,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}}},"cell_type":"code","source":["'''MODEL1 Filter definition'''\n","'''LEFT'''\n","\n","filter1 = left_filter_def(3,4,8,name='filter1')\n","filter2 = left_filter_def(3,8,8,name='filter2')\n","\n","filter3 = left_filter_def(3,8,16,name='filter3')\n","filter4 = left_filter_def(3,16,16,name='filter4')\n","\n","filter5 = left_filter_def(3,16,32,name='filter5')\n","filter6 = left_filter_def(3,32,32,name='filter6')\n","\n","filter7 = left_filter_def(3,32,64,name='filter7')\n","filter8 = left_filter_def(3,64,64,name='filter8')\n","\n","filter9 = left_filter_def(3,64,128,name='filter9')\n","filter10= left_filter_def(3,128,64,name='filter10')\n","\n","'''RIGHT'''\n","\n","filter11 = right_filter_def(3,128,64,name='filter11')\n","filter12 = left_filter_def(3,64,64,name='filter12')\n","filter13 = left_filter_def(3,64,32,name='filter13')\n","\n","filter14 = right_filter_def(3,64,32,name='filter14')\n","filter15 = left_filter_def(3,32,32,name='filter15')\n","filter16 = left_filter_def(3,32,16,name='filter16')\n","\n","filter17 = right_filter_def(3,32,16,name='filter17')\n","filter18 = left_filter_def(3,16,16,name='filter18')\n","filter19 = left_filter_def(3,16,8,name='filter19')\n","\n","filter20 = right_filter_def(3,16,8,name='filter20')\n","filter21 = left_filter_def(3,8,8,name='filter21')\n","filter22 = left_filter_def(3,8,n_class,name='filter22')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"cell_type":"markdown","source":["DEFINING THE MODEL STRUCTURE"]},{"metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_model1(X):\n","    '''Function to define the UNET model'''\n","    with tf.name_scope(\"BLOCK1\"):\n","      '''BLOCK1'''\n","      CNN1 = Conv_layer(X,filter1,stride=1,activation='relu',name='CNN1')\n","      #print (\"CNN1\",np.shape(CNN1))\n","      CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='CNN2')\n","      #print (\"CNN2\",np.shape(CNN2))\n","      pool1 = tf.nn.max_pool(CNN2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL1')\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","      '''BLOCK2'''\n","      CNN3 = Conv_layer(pool1,filter3,stride=1,activation='relu',name='CNN3')\n","      #print (\"CNN3\",np.shape(CNN3))\n","      CNN4 = Conv_layer(CNN3,filter4,stride=1,activation='relu',name='CNN4')\n","      #print (\"CNN4\",np.shape(CNN4))\n","      pool2 = tf.nn.max_pool(CNN4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL2')\n","    \n","    with tf.name_scope(\"BLOCK3\"):\n","      '''BLOCK3'''\n","      CNN5 = Conv_layer(pool2,filter5,stride=1,activation='relu',name='CNN5')\n","      #print (\"CNN5\",np.shape(CNN5))\n","      CNN6 = Conv_layer(CNN5,filter6,stride=1,activation='relu',name='CNN6')\n","      #print (\"CNN6\",np.shape(CNN6))\n","      pool3 = tf.nn.max_pool(CNN6,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL3')\n","    \n","    with tf.name_scope(\"BLOCK4\"):\n","      '''BLOCK4'''\n","      CNN7 = Conv_layer(pool3,filter7,stride=1,activation='relu',name='CNN7')\n","      #print (\"CNN7\",np.shape(CNN7))\n","      CNN8 = Conv_layer(CNN7,filter8,stride=1,activation='relu',name='CNN8')\n","      #print (\"CNN8\",np.shape(CNN8))\n","      pool4 = tf.nn.max_pool(CNN8,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID',name='POOL4')\n","    \n","    with tf.name_scope(\"BLOCK5\"):\n","      '''BLOCK5'''\n","      CNN9 = Conv_layer(pool4,filter9,stride=1,activation='relu',name='CNN9')\n","      #print (\"CNN9\",np.shape(CNN9))\n","      CNN10 = Conv_layer(CNN9,filter10,stride=1,activation='relu',name='CNN10')\n","      #print (\"CNN10\",np.shape(CNN10))\n","    \n","    '''Moving UP'''\n","    \n","    with tf.name_scope(\"BLOCK6\"):\n","      '''BLOCK6'''\n","      concat1 = tf.concat([CNN10,pool4],axis=3,name='CONCAT1')\n","      #print (\"concat1\",np.shape(concat1))\n","      DCNN1= Deconv_layer(concat1,filter11,stride=2,activation='relu',name='DE_CONV1')\n","      #print (\"DCNN1\",np.shape(DCNN1))\n","      CNN11 = Conv_layer(DCNN1,filter12,stride=1,activation='relu',name='CNN11')\n","      #print (\"CNN11\",np.shape(CNN11))\n","      CNN12 = Conv_layer(CNN11,filter13,stride=1,activation='relu',name='CNN12')\n","      #print (\"CNN12\",np.shape(CNN12))\n","    \n","    with tf.name_scope(\"BLOCK7\"):\n","      '''BLOCK7'''\n","      concat2 = tf.concat([CNN12,pool3],axis=3,name='CONCAT2')\n","      #print (\"concat2\",np.shape(concat2))\n","      DCNN2= Deconv_layer(concat2,filter14,stride=2,activation='relu',name='DE_CONV2')\n","      #print (\"DCNN2\",np.shape(DCNN2))\n","      CNN13 = Conv_layer(DCNN2,filter15,stride=1,activation='relu',name='CNN13')\n","      #print (\"CNN13\",np.shape(CNN13))\n","      CNN14 = Conv_layer(CNN13,filter16,stride=1,activation='relu',name='CNN14')\n","      #print (\"CNN14\",np.shape(CNN14))\n","    \n","    with tf.name_scope(\"BLOCK8\"):\n","      '''BLOCK8'''\n","      concat3 = tf.concat([CNN14,pool2],axis=3,name='CONCAT3')\n","      #print (\"concat3\",np.shape(concat3))\n","      DCNN3= Deconv_layer(concat3,filter17,stride=2,activation='relu',name='DE_CONV3')\n","      #print (\"DCNN3\",np.shape(DCNN3))\n","      CNN15 = Conv_layer(DCNN3,filter18,stride=1,activation='relu',name='CNN14')\n","      #print (\"CNN15\",np.shape(CNN15))\n","      CNN16 = Conv_layer(CNN15,filter19,stride=1,activation='relu',name='CNN15')\n","      #print (\"CNN16\",np.shape(CNN16))\n","      \n","    with tf.name_scope(\"BLOCK9\"):\n","      '''BLOCK9'''\n","      concat4 = tf.concat([CNN16,pool1],axis=3,name='CONCAT4')\n","      #print (\"concat4\",np.shape(concat4))\n","      DCNN4= Deconv_layer(concat4,filter20,stride=2,activation='relu',name='DE_CONV4')\n","      #print (\"DCNN4\",np.shape(DCNN4))\n","      CNN17 = Conv_layer(DCNN4,filter21,stride=1,activation='relu',name='CNN17')\n","      #print (\"CNN17\",np.shape(CNN17))\n","      CNN18 = Conv_layer(CNN17,filter22,stride=1,activation='relu',name='CNN18')\n","      #print (\"CNN18\",np.shape(CNN18))\n","      return (CNN18)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c_ROD14VTzmB","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_unet(learning_rate =0.0001,n_epochs = 100):\n","  '''Function to Train U-Net'''\n","  prediction = predict_model1(X)#logits\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"LOSS_FUNCTION\"):\n","    '''using multi dimensional dice'''\n","    hot_y = hot_encode(y)\n","    dice = 1+ dice_coef_multilabel(hot_y,prediction)\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"COST_FUNCTION\"):\n","    '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","    loss = tf.reduce_mean(dice, name=\"loss\")\n","\n","  with tf.name_scope(\"OPTIMIZER\"):\n","    '''Optimizer'''\n","    optimize = tf.train.AdamOptimizer(learning_rate = learning_rate)\n","    training_output = optimize.minimize(loss)\n","\n","\n","  '''initializing'''\n","  init = tf.global_variables_initializer()\n","  saver = tf.train.Saver()\n","\n","  '''Timing'''\n","  start = time.time()\n","  '''Session'''\n","  with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","      current_batch_no = 0\n","      permute_mat = 0\n","      iteration = 0\n","      while(1):\n","        epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","        dice_val,sess_results = sess.run([dice,training_output], feed_dict={X: epoch_x, y: epoch_y})\n","        #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","            \n","        '''DICE Coefficient for iteration'''\n","        if iteration%10==0:\n","          acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","          test_images, test_labels = test_batch()\n","          acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","          print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","        if last ==1:\n","          break\n","        iteration +=1\n","      test_images, test_labels = test_batch()\n","      acc_test = dice.eval(feed_dict={X: test_images, y: test_labels})\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","      print(\"After Epoch\", epoch+1, \"Test accuracy:\", acc_test)\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","        \n","        \n","      if epoch % 2 == 0:\n","        test_example =   test_images\n","        test_example_gt = test_labels\n","        sess_results = sess.run([prediction],feed_dict={X:test_example})\n","\n","        sess_results = sess_results[0][0,:,:,1,:] + (2*sess_results[0][0,:,:,2,:]) + (3*sess_results[0][0,:,:,3,:])\n","        test_example = test_example[0,:,:,3]\n","        test_example_gt = test_example_gt[0,:,:,1,:] + (2*test_example_gt[0,:,:,2,:]) + (3*test_example_gt[0,:,:,0,:])\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Original Image')\n","        plt.savefig('train_change_15_1_3/'+str(epoch)+\"a_Original_Image.png\")\n","              \n","              \n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Ground Truth Mask')\n","        plt.savefig('train_change_15_1_3/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(sess_results),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Generated Mask')\n","        plt.savefig('train_change_15_1_3/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","        plt.close('all')\n","\n","    '''Saving the graph'''\n","    save_path = saver.save(sess, \"./final_madel_graph\")\n","  end = time.time()\n","  total_time = end-start\n","  return (total_time)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"cell_type":"markdown","source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"efc9e106-5c73-4240-f25b-9688579479f2","executionInfo":{"status":"error","timestamp":1554896648815,"user_tz":-330,"elapsed":7022468,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":1788}},"cell_type":"code","source":["train_unet()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Minibatch at Epoch 1 batch 1 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 11 Train accuracy: 0.0 Test accuracy: 0.0003127455711364746\n","Minibatch at Epoch 1 batch 21 Train accuracy: 0.00043767690658569336 Test accuracy: 0.00047785043716430664\n","Minibatch at Epoch 1 batch 31 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 41 Train accuracy: 0.0 Test accuracy: 0.000545203685760498\n","Minibatch at Epoch 1 batch 51 Train accuracy: 0.0015339851379394531 Test accuracy: 0.0005252361297607422\n","Minibatch at Epoch 1 batch 61 Train accuracy: 0.0024448633193969727 Test accuracy: 0.0010772347450256348\n","Minibatch at Epoch 1 batch 71 Train accuracy: 0.0007189512252807617 Test accuracy: 0.0018087029457092285\n","Minibatch at Epoch 1 batch 81 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 91 Train accuracy: 0.0 Test accuracy: 0.0012024641036987305\n","Minibatch at Epoch 1 batch 101 Train accuracy: 0.0 Test accuracy: 0.005080997943878174\n","Minibatch at Epoch 1 batch 111 Train accuracy: 0.0 Test accuracy: 0.00731271505355835\n","Minibatch at Epoch 1 batch 121 Train accuracy: 0.0 Test accuracy: 0.01175987720489502\n","Minibatch at Epoch 1 batch 131 Train accuracy: 0.0 Test accuracy: 0.04842132329940796\n","Minibatch at Epoch 1 batch 141 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 151 Train accuracy: 0.0 Test accuracy: 0.10975837707519531\n","Minibatch at Epoch 1 batch 161 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 171 Train accuracy: 0.0 Test accuracy: 0.3070704936981201\n","Minibatch at Epoch 1 batch 181 Train accuracy: 0.0 Test accuracy: 0.3970540761947632\n","Minibatch at Epoch 1 batch 191 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 201 Train accuracy: 0.30505967140197754 Test accuracy: 0.14989197254180908\n","Minibatch at Epoch 1 batch 211 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 221 Train accuracy: 0.0020439624786376953 Test accuracy: 0.2879241704940796\n","Minibatch at Epoch 1 batch 231 Train accuracy: 0.4964876174926758 Test accuracy: 0.7287285327911377\n","Minibatch at Epoch 1 batch 241 Train accuracy: 0.05447649955749512 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 251 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 261 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 271 Train accuracy: 0.0 Test accuracy: 0.013658344745635986\n","Minibatch at Epoch 1 batch 281 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 291 Train accuracy: 0.0 Test accuracy: 0.08803141117095947\n","Minibatch at Epoch 1 batch 301 Train accuracy: 1.663846492767334 Test accuracy: 0.8002076148986816\n","Minibatch at Epoch 1 batch 311 Train accuracy: 0.0 Test accuracy: 0.00012123584747314453\n","Minibatch at Epoch 1 batch 321 Train accuracy: 1.303913950920105 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 331 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 341 Train accuracy: 0.0 Test accuracy: 0.5422148108482361\n","Minibatch at Epoch 1 batch 351 Train accuracy: 0.4938938617706299 Test accuracy: 1.1072556972503662\n","Minibatch at Epoch 1 batch 361 Train accuracy: 1.619245171546936 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 371 Train accuracy: 0.0 Test accuracy: 0.02706754207611084\n","Minibatch at Epoch 1 batch 381 Train accuracy: 0.009708285331726074 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 391 Train accuracy: 1.6250057220458984 Test accuracy: 1.6926411390304565\n","Minibatch at Epoch 1 batch 401 Train accuracy: 1.7265539169311523 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 411 Train accuracy: 1.9248244762420654 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 421 Train accuracy: 0.0 Test accuracy: 1.1642835140228271\n","Minibatch at Epoch 1 batch 431 Train accuracy: 1.6211462020874023 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 441 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 451 Train accuracy: 0.0 Test accuracy: 1.7541253566741943\n","Minibatch at Epoch 1 batch 461 Train accuracy: 1.1742405891418457 Test accuracy: 1.273476004600525\n","Minibatch at Epoch 1 batch 471 Train accuracy: 0.0 Test accuracy: 1.713721513748169\n","Minibatch at Epoch 1 batch 481 Train accuracy: 0.004769384860992432 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 491 Train accuracy: 1.9345128536224365 Test accuracy: 0.0008425116539001465\n","Minibatch at Epoch 1 batch 501 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 511 Train accuracy: 0.0 Test accuracy: 0.038485705852508545\n","Minibatch at Epoch 1 batch 521 Train accuracy: 0.06972038745880127 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 531 Train accuracy: 0.0 Test accuracy: 0.3072482943534851\n","Minibatch at Epoch 1 batch 541 Train accuracy: 1.8699860572814941 Test accuracy: 1.8114347457885742\n","Minibatch at Epoch 1 batch 551 Train accuracy: 0.0014824867248535156 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 561 Train accuracy: 1.2563161849975586 Test accuracy: 0.056377291679382324\n","Minibatch at Epoch 1 batch 571 Train accuracy: 0.0 Test accuracy: 1.9289724826812744\n","Minibatch at Epoch 1 batch 581 Train accuracy: 0.0 Test accuracy: 1.9500514268875122\n","Minibatch at Epoch 1 batch 591 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 601 Train accuracy: 0.0 Test accuracy: 1.7574106454849243\n","Minibatch at Epoch 1 batch 611 Train accuracy: 0.0 Test accuracy: 0.00017595291137695312\n","Minibatch at Epoch 1 batch 621 Train accuracy: 1.9044665098190308 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 631 Train accuracy: 1.78633451461792 Test accuracy: 1.991382122039795\n","Minibatch at Epoch 1 batch 641 Train accuracy: 0.0 Test accuracy: 1.895295262336731\n","Minibatch at Epoch 1 batch 651 Train accuracy: 0.0 Test accuracy: 1.245737075805664e-05\n","Minibatch at Epoch 1 batch 661 Train accuracy: 0.09026259183883667 Test accuracy: 1.955334186553955\n","Minibatch at Epoch 1 batch 671 Train accuracy: 0.0 Test accuracy: 1.5682698488235474\n","Minibatch at Epoch 1 batch 681 Train accuracy: 1.8394641876220703 Test accuracy: 1.3791708946228027\n","Minibatch at Epoch 1 batch 691 Train accuracy: 1.9696921110153198 Test accuracy: 1.875622272491455\n","Minibatch at Epoch 1 batch 701 Train accuracy: 1.9511096477508545 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 711 Train accuracy: 1.9880485534667969 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 721 Train accuracy: 1.8956180810928345 Test accuracy: 0.0\n","Minibatch at Epoch 1 batch 731 Train accuracy: 0.0 Test accuracy: 1.9817583560943604\n","Minibatch at Epoch 1 batch 741 Train accuracy: 0.0 Test accuracy: 0.08670812845230103\n","Minibatch at Epoch 1 batch 751 Train accuracy: 2.0503997802734375e-05 Test accuracy: 0.0005791783332824707\n","Minibatch at Epoch 1 batch 761 Train accuracy: 0.0 Test accuracy: 1.6850415468215942\n","Minibatch at Epoch 1 batch 771 Train accuracy: 0.5117945671081543 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 1 Test accuracy: 1.0\n","-------------------------------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-efe8ffd75c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-fcc6ed340812>\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m(learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_example\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msess_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msess_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mtest_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtest_example_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_example_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_example_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_example_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for array"]}]},{"metadata":{"id":"Icc3WNqr_ZYR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Y-NET@2D.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"TFJUj68p8KDT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"07124621-d1a2-425c-c19f-95a3020160fb","executionInfo":{"status":"ok","timestamp":1556574374524,"user_tz":-330,"elapsed":924,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}}},"cell_type":"code","source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"niHv4ZHU8mKU","colab_type":"code","colab":{}},"cell_type":"code","source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xMG7Db-g8rW0","colab_type":"code","colab":{}},"cell_type":"code","source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","import math\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q0g-5AVG8uQi","colab_type":"code","colab":{}},"cell_type":"code","source":["'''DEFINING VARIABLES'''\n","\n","batch_size=1              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","'''Here we crop the 155x240x240 image to  160x160x192 by repeating the last layer 5 times to convert 155 to 160'''\n","X = tf.placeholder(shape=[None,160,160,192,4], dtype=tf.float32, name='input_image')\n","y = tf.placeholder(shape=[None,160,160,192,1], dtype=tf.int64, name='hot_encode_label')\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size*155,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size*155,1),dtype=np.int64)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l9YsFzcsDkQA","colab_type":"code","colab":{}},"cell_type":"code","source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","    '''normalization of each input channels'''\n","    global out_img\n","    '''CHANNEL INFO'''\n","    # maximum value found using function called \"Finding_maximum_to_normalise \"\n","    #'''max value for dimension 4 is 5337.0'''\n","    #'''max value for dimension 3 is 11737.0'''\n","    #'''max value for dimension 2 is 9751.0'''\n","    #'''max value for dimension 1 is 6476.0'''\n","    out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","    out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","    out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","    out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","\n","def crop_image_fit_brain(in_image):\n","    '''cropping size was found using the code finding_brain.ipynb'''\n","    left = 19\n","    right= 210\n","    top  = 38\n","    bot  = 199\n","    out_image = in_image[top:(bot-1),left:(right+1),:,:]\n","    return (out_image)\n","\n","\n","def Pre_processing_3D(a):\n","    '''Function to roll axis to convert array into[depth,width,height,channels] and the divide it in batches'''\n","    #print(np.shape(a))\n","    b = np.rollaxis(a,2, 0)\n","    #print(np.shape(b))\n","    #image shape\n","    out_arr = np.empty(shape=[batch_size,160,np.shape(b)[1],np.shape(b)[2],np.shape(b)[3]])\n","    for i in range(batch_size):\n","        start = i*155\n","        end = start+155\n","        out_arr[i,0:155,:,:,:] = b[start:end,:,:,:]\n","    \n","    a = [out_arr[:,154:155,:,:,:]]*5\n","    out_arr[:,155:160,:,:,:] = a[0]\n","    \n","    '''clippig data from front of each batch'''\n","    '''to fit the model we remove first 3 slices of each batch'''\n","    '''shape of a is [batch_size,depth,width,height,channels]'''\n","    out_send = out_arr[:,:,:,:,:]\n","    #print(\"arr\",np.shape(out_arr),\"send\",np.shape(out_send))\n","    return (out_send)\n","\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","    '''Function to take batches randomly'''\n","    global out_img\n","    global out_label\n","    \n","    '''training info'''\n","    train_info = 300  #100 patients with 155 images each\n","\n","    if current_batch_no == 0:\n","        no_of_batches = train_info//batch_size  \n","        permute_mat = np.random.permutation(no_of_batches)\n","    \n","    start = permute_mat[current_batch_no]*batch_size*155\n","    end = start + (batch_size*155)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    current_batch_no += 1\n","    #print(len(out_img))\n","    '''Input normalization'''\n","    normalizing_input()\n","    '''normalization oof labels'''\n","    #out_label = out_label\n","    '''converting multi class to dual class'''\n","    #out_label = convert_dual_class(out_label)\n","    '''cropping image and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label)\n","    '''Rolling axes'''\n","    #out_img_send = np.rollaxis(crop_out_image,2, 0)\n","    '''hot encoding'''\n","    #out_label_send = crop_out_label\n","    '''3D conversion'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","     \n","    last=0\n","    if current_batch_no == len(permute_mat):\n","        last=1\n","    \n","    return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","def test_batch():\n","    '''Function to take next test batch''' \n","    global out_img\n","    global out_label\n","    \n","    '''training and testing info'''\n","    train_info = 380  #100 patients with 155 images each\n","    test_info = 484-train_info  #100 patients with 155 images each\n","    \n","    no_of_batches = test_info//batch_size  \n","    permute_mat = np.random.permutation(no_of_batches)\n","    start = (permute_mat[0]*batch_size*155) +(train_info*155)\n","    end = start + (155*batch_size)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","    '''normalization'''\n","    normalizing_input()\n","    '''croping images and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label) \n","    '''3D processing'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    \n","    return (out_img_send,out_label_send)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b4lif58Rc7zv","colab_type":"code","colab":{}},"cell_type":"code","source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","    '''Defining a filter variable to perform convolution'''\n","    return (tf.Variable(tf.random_normal([ker_size,ker_size,ker_size,in_chan,out_chan],stddev=0.05),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","    '''Defining a filter variable to perform transpose convolution'''\n","    return (tf.Variable(tf.random_normal([ker_size,ker_size,ker_size,out_chan,in_chan],stddev=0.05),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","    '''Function to perform Convolution and apply activation filter'''\n","    '''Convolution'''\n","    conv = tf.nn.conv3d(input_im,filter_mask,strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","    '''Function to perform Transpose Convolution and apply activation filter'''\n","    '''Transpose Convolution'''\n","    inp_shape = np.shape(input_im) #tf.shape()\n","    out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2),int(inp_shape[3].value*2), int(inp_shape[4].value/2)]\n","    \n","    conv = tf.nn.conv3d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZrDAATGnbXHb","colab_type":"code","colab":{}},"cell_type":"code","source":["'''loss function definition'''\n","\n","\n","###############################################################################################################################\n","#WEIGHTED MULTICLASS DICE LOSS\n","###############################################################################################################################\n","def dice_coeff(y_true, y_pred):\n","    '''Finding dice coefficient for one class'''\n","    intersection = tf.math.reduce_sum(y_true*y_pred)\n","    union = ((tf.math.reduce_sum(y_true*y_true))+(tf.math.reduce_sum(y_pred*y_pred)))\n","    return(intersection,union)\n","\n","  \n","def dice_coef_multilabel(y_true, y_pred, numLabels = n_class):\n","    '''Finding dice loss for each class'''\n","    dice = denominator = numerator = 0\n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    #print(np.shape(y_true))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    #w = 1/((w**2))\n","    #w = np.sum(y_true, axis=(0,1,2))\n","    weight = np.zeros(shape=(Ncl,))\n","    weight = 1/w\n","      \n","    for index in range(numLabels):\n","        a = weight[index]/(tf.reduce_sum(weight))     #for removing the weight added in all\n","        '''Here, as of now we are neglecting the background class'''\n","        #dice -= (weight*dice_coeff(y_true[:,:,:,index,0],y_pred[:,:,:,index]))\n","        num,den = (dice_coeff(y_true[:,:,:,:,index,0],y_pred[:,:,:,:,index]))\n","        denominator += den\n","        numerator += a*(num)\n","    dice = -((2*numerator)/denominator)\n","    return (dice)\n","#############################################################################################################################\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lbUqouTgU_Gg","colab_type":"code","colab":{}},"cell_type":"code","source":["def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","    '''function for hot encoding images'''\n","    a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","    #print(np.shape(a))\n","    b = tf.transpose(a,perm=[0,1,2,3,5,4])\n","    return b\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lCRdCf_wXVlo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"0502238f-2ac0-43ad-a021-00ff76c37e17","executionInfo":{"status":"ok","timestamp":1556574407117,"user_tz":-330,"elapsed":2007,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}}},"cell_type":"code","source":["'''MODEL1 Filter definition'''\n","\n","'''LEFT'''\n","\n","'''Input scaling'''\n","Scaling_filter0 = left_filter_def(3,4,8,name='preparing_input')\n","'''RESNET PART OF YNET'''\n","\n","#res_filter1 = left_filter_def(3,4,8,name='res_filter1')\n","res_filter1 = left_filter_def(3,8,8,name='res_filter2')\n","res_filter2 = left_filter_def(3,8,8,name='res_filter3')\n","res_filter3 = left_filter_def(3,8,8,name='res_filter4')\n","\n","res_filter4 = left_filter_def(3,8,16,name='res_filter5')\n","res_filter5 = left_filter_def(3,16,16,name='res_filter6')\n","res_filter6 = left_filter_def(3,16,16,name='res_filter7')\n","res_filter7 = left_filter_def(3,16,16,name='res_filter8')\n","\n","res_filter8 = left_filter_def(3,16,32,name='res_filter9')\n","res_filter9 = left_filter_def(3,32,32,name='res_filter10')\n","res_filter10= left_filter_def(3,32,32,name='res_filter11')\n","res_filter11= left_filter_def(3,32,32,name='res_filter12')\n","\n","res_filter12= left_filter_def(3,32,64,name='res_filter13')\n","res_filter13= left_filter_def(3,64,64,name='res_filter14')\n","res_filter14= left_filter_def(3,64,64,name='res_filter15')\n","res_filter15= left_filter_def(3,64,64,name='res_filter16')\n","'''preparing resnet to combine with inception network'''\n","res_prep_filter = left_filter_def(3,64,128,name='res_prep_filter')\n","\n","\n","'''INCEPTION PART OF YNET'''\n","\n","\n","'''1st inception block'''\n","inception_filter1 = left_filter_def(1,8,(2*8),name='inception_layer1_line1_filter1')\n","inception_filter2 = left_filter_def(3,(2*8),(4*8),name='inception_layer1_line1_filter2')\n","inception_filter3 = left_filter_def(1,8,(2*8),name='inception_layer1_line2_filter1')\n","inception_filter4 = left_filter_def(3,(2*8),(4*8),name='inception_layer1_line2_filter2')\n","inception_filter5 = left_filter_def(3,(4*8),(4*8),name='inception_layer1_line2_filter3')\n","inception_filter6 = left_filter_def(1,8,(2*8),name='inception_layer1_line3_filter1')\n","inception_filter7 = left_filter_def(3,(8*10),8,name='inception_layer1_final_filter1')  #in channels = (out cahnnens of all the three paths)\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter1 = left_filter_def(3,8,16,name='inception_Sfilter1')\n","\n","'''2nd inception block'''\n","inception_filter8 = left_filter_def(1,16,(2*16),name='inception_layer2_line1_filter1')\n","inception_filter9 = left_filter_def(3,(2*16),(4*16),name='inception_layer2_line1_filter2')\n","inception_filter10= left_filter_def(1,16,(2*16),name='inception_layer2_line2_filter1')\n","inception_filter11= left_filter_def(3,(2*16),(4*16),name='inception_layer2_line2_filter2')\n","inception_filter12= left_filter_def(3,(4*16),(4*16),name='inception_layer2_line2_filter3')\n","inception_filter13= left_filter_def(1,16,(2*16),name='inception_layer2_line3_filter1')\n","inception_filter14= left_filter_def(3,(16*10),16,name='inception_layer2_final_filter1')\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter2 = left_filter_def(3,16,32,name='inception_Sfilter2')\n","\n","'''3rd inception block'''\n","inception_filter15= left_filter_def(1,32,(2*32),name='inception_layer3_line1_filter1')\n","inception_filter16= left_filter_def(3,(2*32),(4*32),name='inception_layer3_line1_filter2')\n","inception_filter17= left_filter_def(1,(32),(2*32),name='inception_layer3_line2_filter1')\n","inception_filter18= left_filter_def(3,(2*32),(4*32),name='inception_layer3_line2_filter2')\n","inception_filter19= left_filter_def(3,(4*32),(4*32),name='inception_layer3_line2_filter3')\n","inception_filter20= left_filter_def(1,32,(2*32),name='inception_layer3_line3_filter1')\n","inception_filter21= left_filter_def(3,(10*32),32,name='inception_layer3_final_filter1')\n","'''Scaling FIlter(used after pooling)'''\n","Scaling_filter3 = left_filter_def(3,32,64,name='Sfilter3')\n","\n","'''4th inception block'''\n","inception_filter22= left_filter_def(1,64,(2*64),name='inception_layer4_line1_filter1')\n","inception_filter23= left_filter_def(3,(2*64),(4*64),name='inception_layer4_line1_filter2')\n","inception_filter24= left_filter_def(1,64,(2*64),name='inception_layer4_line2_filter1')\n","inception_filter25= left_filter_def(3,(2*64),(4*64),name='inception_layer4_line2_filter2')\n","inception_filter26= left_filter_def(3,(4*64),(4*64),name='inception_layer4_line2_filter3')\n","inception_filter27= left_filter_def(1,(64),(2*64),name='inception_layer4_line3_filter1')\n","inception_filter28= left_filter_def(3,(10*64),64,name='inception_layer4_final_filter1')\n","'''preparing inception net to combine with resnet'''\n","incident_prep_filter = left_filter_def(3,64,128,name='incident_prep_filter')\n","\n","\n","'''COMBINING '''\n","combo_filter1 = left_filter_def(1,(2*128),128,name='combo_filter1')             #concating ineption and resnet\n","combo_filter2 = left_filter_def(1,(3*64),64,name='combo_filter2')               #concating deconv, resnet and inception\n","combo_filter3 = left_filter_def(1,(3*32),32,name='combo_filter3')               #concating deconv, resnet and inception\n","combo_filter4 = left_filter_def(1,(3*16),16,name='combo_filter4')               #concating deconv, resnet and inception\n","combo_filter5 = left_filter_def(1,(3*8),8,name='combo_filter5')                 #concating deconv, resnet and inception\n","'''Right'''\n","\n","UP_filter1 = right_filter_def(3,128,64,name='UP_filter11')\n","UP_filter2 = left_filter_def(1,(3*64),64,name='combo_filter2')\n","UP_filter3 = left_filter_def(3,64,64,name='UP_filter13')\n","\n","UP_filter4 = right_filter_def(3,64,32,name='UP_filter14')\n","UP_filter5 = left_filter_def(1,(3*32),32,name='combo_filter3')\n","UP_filter6 = left_filter_def(3,32,32,name='UP_filter16')\n","\n","UP_filter7 = right_filter_def(3,32,16,name='UP_filter17')\n","UP_filter8 = left_filter_def(1,(3*16),16,name='combo_filter4')\n","UP_filter9 = left_filter_def(3,16,16,name='UP_filter19')\n","\n","UP_filter10 = right_filter_def(3,16,8,name='UP_filter20')\n","UP_filter11 = left_filter_def(1,(3*8),8,name='combo_filter5')\n","UP_filter12 = left_filter_def(3,8,n_class,name='UP_filter22')\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"metadata":{"id":"QVgknxWFl12M","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_model1(X):\n","    \n","    '''Function to define the UNET model'''\n","    Prep_Conv = Conv_layer(X,Scaling_filter0,stride=1,activation='relu',name='CNN_preparing_input')\n","    #print (\"Prep_Conv\",np.shape(Prep_Conv))\n","    with tf.name_scope(\"INCEPTION_NET\"):\n","        \n","        with tf.name_scope(\"INCEPTION_BLOCK1\"):\n","            '''BLOCK1'''\n","            CNN1 = Conv_layer(Prep_Conv,inception_filter1,stride=1,activation='relu',name='layer1_line1_CNN1')\n","            #print (\"CNN1\",np.shape(CNN1))\n","            CNN2 = Conv_layer(CNN1,inception_filter2,stride=1,activation='relu',name='layer1_line1_CNN2')\n","            #print (\"CNN2\",np.shape(CNN2))\n","            CNN3 = Conv_layer(Prep_Conv,inception_filter3,stride=1,activation='relu',name='layer1_line2_CNN1')\n","            #print (\"CNN3\",np.shape(CNN3))\n","            CNN4 = Conv_layer(CNN3,inception_filter4,stride=1,activation='relu',name='layer1_line2_CNN2')\n","            #print (\"CNN4\",np.shape(CNN4))\n","            CNN5 = Conv_layer(CNN4,inception_filter5,stride=1,activation='relu',name='layer1_line2_CNN3')\n","            #print (\"CNN5\",np.shape(CNN5))\n","            CNN6 = Conv_layer(Prep_Conv,inception_filter6,stride=1,activation='relu',name='layer1_line3_CNN1')\n","            #print (\"CNN6\",np.shape(CNN6))\n","            combo1 = tf.concat([CNN2,CNN5,CNN6],axis=4,name='COMBO1')\n","            #print (\"COMBO1\",np.shape(combo1))\n","            CNN7 = Conv_layer(combo1,inception_filter7,stride=1,activation='relu',name='layer1_final_CNN1')\n","            #print (\"CNN7\",np.shape(CNN7))\n","            Inception_block1_out = tf.add(CNN7,Prep_Conv,name='inception1')\n","            #print (\"Inception_block1_outPUT\",np.shape(Inception_block1_out))\n","            #print(\"Inception_block1_out\",np.shape(Inception_block1_out))\n","        Inception_pool1 = tf.nn.max_pool3d(Inception_block1_out,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='Inception_pool1')\n","        S_CNN1 = Conv_layer(Inception_pool1,Scaling_filter1,stride=1,activation='relu',name='Scaling_CNN1')\n","        #print (\"S_NN1\",np.shape(S_CNN1))\n","    \n","        with tf.name_scope(\"INCEPTION_BLOCK2\"):\n","            '''BLOCK2'''\n","            CNN8 = Conv_layer(S_CNN1,inception_filter8,stride=1,activation='relu',name='layer2_line1_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN9 = Conv_layer(CNN8,inception_filter9,stride=1,activation='relu',name='layer2_line1_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN10= Conv_layer(S_CNN1,inception_filter10,stride=1,activation='relu',name='layer2_line2_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN11= Conv_layer(CNN10,inception_filter11,stride=1,activation='relu',name='layer2_line2_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN12= Conv_layer(CNN11,inception_filter12,stride=1,activation='relu',name='layer2_line2_CNN3')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN13= Conv_layer(S_CNN1,inception_filter13,stride=1,activation='relu',name='layer2_line3_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            combo2 = tf.concat([CNN9,CNN12,CNN13],axis=4,name='COMBO2')\n","            #rint (\"Combo\",np.shape(combo2))\n","            CNN14= Conv_layer(combo2,inception_filter14,stride=1,activation='relu',name='layer2_final_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            Inception_block2_out = tf.add(CNN14,S_CNN1,name='inception2')\n","            #print(\"Inception_block2_out\",np.shape(Inception_block2_out))\n","        \n","        Inception_pool2 = tf.nn.max_pool3d(Inception_block2_out,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","        S_CNN2 = Conv_layer(Inception_pool2,Scaling_filter2,stride=1,activation='relu',name='Scaling_CNN2')\n","        #print (\"SCNN2\",np.shape(S_CNN2))\n","        \n","        with tf.name_scope(\"INCEPTION_BLOCK3\"):\n","            '''BLOCK3'''\n","            CNN15= Conv_layer(S_CNN2,inception_filter15,stride=1,activation='relu',name='layer3_line1_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN16= Conv_layer(CNN15,inception_filter16,stride=1,activation='relu',name='layer3_line1_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN17= Conv_layer(S_CNN2,inception_filter17,stride=1,activation='relu',name='layer3_line2_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN18= Conv_layer(CNN17,inception_filter18,stride=1,activation='relu',name='layer3_line2_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN19= Conv_layer(CNN18,inception_filter19,stride=1,activation='relu',name='layer3_line2_CNN3')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN20= Conv_layer(S_CNN2,inception_filter20,stride=1,activation='relu',name='layer3_line3_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            combo3 = tf.concat([CNN16,CNN19,CNN20],axis=4,name='COMBO3')\n","            #rint (\"Combo\",np.shape(combo2))\n","            CNN21= Conv_layer(combo3,inception_filter21,stride=1,activation='relu',name='layer3_final_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            Inception_block3_out = tf.add(CNN21,S_CNN2,name='inception3')\n","            #print(\"Inception_block3_out\",np.shape(Inception_block3_out))\n","        \n","        Inception_pool3 = tf.nn.max_pool3d(Inception_block3_out,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='Inception_pool3')\n","        S_CNN3 = Conv_layer(Inception_pool3,Scaling_filter3,stride=1,activation='relu',name='Scaling_CNN3')\n","        #print (\"sCNN3\",np.shape(S_CNN3))\n","    \n","        with tf.name_scope(\"INCEPTION_BLOCK4\"):\n","            '''BLOCK4'''\n","            CNN22= Conv_layer(S_CNN3,inception_filter22,stride=1,activation='relu',name='layer4_line1_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN23= Conv_layer(CNN22,inception_filter23,stride=1,activation='relu',name='layer4_line1_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN24= Conv_layer(S_CNN3,inception_filter24,stride=1,activation='relu',name='layer4_line2_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN25= Conv_layer(CNN24,inception_filter25,stride=1,activation='relu',name='layer4_line2_CNN2')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN26= Conv_layer(CNN25,inception_filter26,stride=1,activation='relu',name='layer4_line2_CNN3')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            CNN27= Conv_layer(S_CNN3,inception_filter27,stride=1,activation='relu',name='layer4_line3_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            combo4 = tf.concat([CNN23,CNN26,CNN27],axis=4,name='COMBO4')\n","            #rint (\"Combo\",np.shape(combo2))\n","            CNN28= Conv_layer(combo4,inception_filter28,stride=1,activation='relu',name='layer4_final_CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            Inception_block4_out = tf.add(CNN28,S_CNN3,name='inception4')\n","            #print(\"Inception_block4_out\",np.shape(Inception_block4_out))\n","             \n","        Inception_pool4 = tf.nn.max_pool3d(Inception_block4_out,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='Inception_pool4')\n","        '''preparing inception for resnet'''\n","        ineption_prep = Conv_layer(Inception_pool4,incident_prep_filter,stride=1,activation='relu',name='incident_prep')\n","        #print (\"ineption_prep\",np.shape(ineption_prep))\n","    \n","    with tf.name_scope(\"RESNET\"):\n","        \n","        with tf.name_scope(\"RESNET_BLOCK1\"):\n","            '''BLOCK1'''\n","            Res_CNN1 = Conv_layer(Prep_Conv,res_filter1,stride=1,activation='relu',name='CNN1')\n","            #rint (\"CNN1\",np.shape(CNN1))\n","            Res_CNN2 = Conv_layer(Res_CNN1,res_filter2,stride=1,activation='relu',name='CNN2')\n","            #rint (\"CNN2\",np.shape(CNN2))\n","            Res_CNNR1 = tf.add(Conv_layer(Res_CNN2,res_filter3,stride=1,activation='relu',name='CNNR1'), Prep_Conv, name='Resnet1')\n","            #print(\"res_CNN1\",np.shape(Res_CNNR1))\n","            #rint (\"CNNR1\",np.shape(CNNR1))\n","            pool1 = tf.nn.max_pool3d(Res_CNNR1,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","            #print(\"res_pool1\",np.shape(pool1))\n","        with tf.name_scope(\"RESNET_BLOCK2\"):\n","            '''BLOCK2'''\n","            Res_CNN3 = Conv_layer(pool1,res_filter4,stride=1,activation='relu',name='CNN3')\n","            Res_CNN4 = Conv_layer(Res_CNN3,res_filter5,stride=1,activation='relu',name='CNN3')\n","            #rint (\"CNN3\",np.shape(CNN3))\n","            Res_CNN5 = Conv_layer(Res_CNN4,res_filter6,stride=1,activation='relu',name='CNN4')\n","            #rint (\"CNN4\",np.shape(CNN4))\n","            Res_CNNR2 = tf.add(Conv_layer(Res_CNN5,res_filter7,stride=1,activation='relu',name='CNNR2'), Res_CNN3, name='Resnet2')\n","            #print(\"res_CNN2\",np.shape(Res_CNNR2))\n","            #rint (\"CNNR2\",np.shape(CNNR2))\n","            pool2 = tf.nn.max_pool3d(Res_CNNR2,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL2')\n","            #print(\"res_pool2\",np.shape(pool2))\n","        with tf.name_scope(\"RESNET_BLOCK3\"):\n","            '''BLOCK3'''\n","            Res_CNN6 = Conv_layer(pool2,res_filter8,stride=1,activation='relu',name='CNN3')\n","            Res_CNN7 = Conv_layer(Res_CNN6,res_filter9,stride=1,activation='relu',name='CNN5')\n","            #rint (\"CNN5\",np.shape(CNN5))\n","            Res_CNN8 = Conv_layer(Res_CNN7,res_filter10,stride=1,activation='relu',name='CNN6')\n","            #rint (\"CNN6\",np.shape(CNN6))\n","            Res_CNNR3 = tf.add(Conv_layer(Res_CNN8,res_filter11,stride=1,activation='relu',name='CNNR3'), Res_CNN6, name='Resnet3')\n","            #print(\"res_CNN3\",np.shape(Res_CNNR3))\n","            #rint (\"CNNR3N\",np.shape(CNNR3))\n","            pool3 = tf.nn.max_pool3d(Res_CNNR3,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL3')\n","            #print(\"res_pool3\",np.shape(pool3))\n","        with tf.name_scope(\"RESNET_BLOCK4\"):\n","            '''BLOCK4'''\n","            Res_CNN9 = Conv_layer(pool3,res_filter12,stride=1,activation='relu',name='CNN3')\n","            Res_CNN10= Conv_layer(Res_CNN9,res_filter13,stride=1,activation='relu',name='CNN7')\n","            #rint (\"CNN7\",np.shape(CNN7))\n","            Res_CNN11= Conv_layer(Res_CNN10,res_filter14,stride=1,activation='relu',name='CNN8')\n","            #rint (\"CNN8\",np.shape(CNN8))\n","            Res_CNNR4 = tf.add(Conv_layer(Res_CNN11,res_filter15,stride=1,activation='relu',name='CNNR4'), Res_CNN9, name='Resnet4')\n","            #print(\"res_CNN4\",np.shape(Res_CNNR4))\n","            #rint (\"CNNR4\",np.shape(CNNR4))\n","            pool4 = tf.nn.max_pool3d(Res_CNNR4,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL4')\n","        '''preparing resnet for inception net'''\n","        resnet_prep = Conv_layer(pool4,res_prep_filter,stride=1,activation='relu',name='res_prep')\n","        #print(\"res_prep\",np.shape(resnet_prep))\n","        \n","\n","    with tf.name_scope(\"Flat_Layer\"):\n","        concat_res_incep = tf.concat([resnet_prep,ineption_prep],axis=4,name='CONCAT1')\n","        capturing_depth1 = Conv_layer(concat_res_incep,combo_filter1,stride=1,activation='relu',name='capturing_depth1')\n","        #print(\"capturing_depth1\",np.shape(capturing_depth1))\n","    \n","    with tf.name_scope(\"UP_LAYER\"):\n","        with tf.name_scope(\"UP_block1\"):\n","            DCNN1= Deconv_layer(capturing_depth1,UP_filter1,stride=2,activation='relu',name='DE_CONV1')\n","            #print (\"DCNN1\",np.shape(DCNN1))\n","            concat_res_incep_decov1 = tf.concat([Res_CNNR4,Inception_block4_out,DCNN1],axis=4,name='CONCAT2')\n","            UP_CNN1 = Conv_layer(concat_res_incep_decov1,UP_filter2,stride=1,activation='relu',name='upCNN1')\n","            #print (\"CNN11\",np.shape(CNN11))\n","            UP_CNN2 = Conv_layer(UP_CNN1,UP_filter3,stride=1,activation='relu',name='upCNN2')\n","            #print (\"UP_block1\",np.shape(UP_CNN2))\n","        with tf.name_scope(\"UP_block2\"):\n","            DCNN2= Deconv_layer(UP_CNN2,UP_filter4,stride=2,activation='relu',name='DE_CONV2')\n","            #print (\"DCNN1\",np.shape(DCNN1))\n","            concat_res_incep_decov2 = tf.concat([Res_CNNR3,Inception_block3_out,DCNN2],axis=4,name='CONCAT3')\n","            UP_CNN3 = Conv_layer(concat_res_incep_decov2,UP_filter5,stride=1,activation='relu',name='upCNN3')\n","            #print (\"CNN11\",np.shape(CNN11))\n","            UP_CNN4 = Conv_layer(UP_CNN3,UP_filter6,stride=1,activation='relu',name='upCNN4')\n","            #print (\"UP_block2\",np.shape(UP_CNN4))\n","        with tf.name_scope(\"UP_block3\"):\n","            DCNN3= Deconv_layer(UP_CNN4,UP_filter7,stride=2,activation='relu',name='DE_CONV3')\n","            #print (\"DCNN1\",np.shape(DCNN1))\n","            concat_res_incep_decov3 = tf.concat([Res_CNNR2,Inception_block2_out,DCNN3],axis=4,name='CONCAT4')\n","            UP_CNN5 = Conv_layer(concat_res_incep_decov3,UP_filter8,stride=1,activation='relu',name='upCNN5')\n","            #print (\"CNN11\",np.shape(CNN11))\n","            UP_CNN6 = Conv_layer(UP_CNN5,UP_filter9,stride=1,activation='relu',name='upCNN6')\n","            #print (\"UP_block3\",np.shape(UP_CNN6))\n","        with tf.name_scope(\"UP_block4\"):\n","            DCNN4= Deconv_layer(UP_CNN6,UP_filter10,stride=2,activation='relu',name='DE_CONV4')\n","            #print (\"DCNN1\",np.shape(DCNN1))\n","            concat_res_incep_decov4 = tf.concat([Res_CNNR1,Inception_block1_out,DCNN4],axis=4,name='CONCAT5')\n","            UP_CNN7 = Conv_layer(concat_res_incep_decov4,UP_filter11,stride=1,activation='relu',name='upCNN7')\n","            #print (\"CNN11\",np.shape(CNN11))\n","            UP_CNN8 = Conv_layer(UP_CNN7,UP_filter12,stride=1,activation='softmax',name='upCNN8')\n","            #print (\"UP_block4\",np.shape(UP_CNN8))\n","    return(UP_CNN8)\n","        \n","        \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xn3PqivQ6bee","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_unet(learning_rate =0.0001,n_epochs = 100):\n","    '''Function to Train U-Net'''\n","    prediction = predict_model1(X)#logits\n","    '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","    with tf.name_scope(\"LOSS_FUNCTION\"):\n","        '''using multi dimensional dice'''\n","        hot_y = hot_encode(y)\n","        dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","        #dice = 1 + generalized_dice_coeff(hot_y[:,:,:,:,:,0], prediction)\n","    '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","    with tf.name_scope(\"COST_FUNCTION\"):\n","        '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","        loss = tf.reduce_mean(dice, name=\"loss\")\n","\n","    with tf.name_scope(\"OPTIMIZER\"):\n","        '''Optimizer'''\n","        optimize = tf.train.AdamOptimizer(learning_rate = learning_rate)\n","        training_output = optimize.minimize(loss)\n","\n","\n","    '''initializing'''\n","    init = tf.global_variables_initializer()\n","    saver = tf.train.Saver()\n","\n","    '''Timing'''\n","    start = time.time()\n","    '''Session'''\n","    with tf.Session() as sess:\n","        init.run()\n","        for epoch in range(n_epochs):\n","            current_batch_no = 0\n","            permute_mat = 0\n","            iteration = 0\n","            while(1):\n","                epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","                dice_val,sess_results = sess.run([dice,training_output], feed_dict={X: epoch_x, y: epoch_y})\n","                #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","            \n","                '''DICE Coefficient for iteration'''\n","                if iteration%10==0:\n","                    acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","                    test_images, test_labels = test_batch()\n","                    acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","                    print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","                if last ==1:\n","                    break\n","                iteration +=1\n","            test_images, test_labels = test_batch()\n","            acc_test = 1-dice.eval(feed_dict={X: test_images, y: test_labels})\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            print(\"After Epoch\", epoch+1, \"Test accuracy:\", acc_test)\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","        \n","        \n","            if epoch % 1 == 0:\n","                test_example =   test_images\n","                test_example_gt = test_labels#np.rollaxis(test_labels,2,0)\n","                sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","                sess_results = sess_results[0,100,:,:,1] + (2*sess_results[0,100,:,:,2]) + (3*sess_results[0,100,:,:,3])\n","                test_example = test_example[0,100,:,:,3]\n","                test_example_gt = test_example_gt[0,100,:,:,:]\n","                \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Original Image')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"a_Original_Image.png\")\n","                 \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Ground Truth Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","                plt.figure()\n","                plt.imshow(np.squeeze(sess_results),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Generated Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/YNET_without_FOCAL_LOSS/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","                plt.close('all')\n","\n","        '''Saving the graph'''\n","        save_path = saver.save(sess, \"/content/gdrive/My Drive/Brain_Tumour_segmentation/Ynet/final_madel_graph_model1\")\n","    end = time.time()\n","    total_time = end-start\n","    return (total_time)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sDkrVf3lCybi","colab_type":"code","colab":{}},"cell_type":"code","source":["train_unet()"],"execution_count":0,"outputs":[]}]}
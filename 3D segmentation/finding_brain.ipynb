{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finding_brain.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","2. The problem of class imbalance still persists and dice coeff is more than 1. So i this version we have implemented a new dice coefficient function.\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eSGVMeVbibw0","colab_type":"code","outputId":"c4241ba9-db34-4b34-9b08-7a352f191939","executionInfo":{"status":"ok","timestamp":1557740145041,"user_tz":-330,"elapsed":1246,"user":{"displayName":"JAHANVI PATEL","photoUrl":"","userId":"14445844665407264126"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYJxbT5x6FQv","colab_type":"code","colab":{}},"source":["'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,155,4),dtype=np.float32)\n","out_label = np.empty((240,240,155,1),dtype=np.int64)\n","def calc_empty():\n","    global out_img\n","    global out_label\n","    a = np.empty(shape=(484,1))\n","    for i in range(484):\n","        start = i*155\n","        end = start+155\n","        train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","        train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","        if np.sum(out_label) == 0:\n","            a[i,0] = 0\n","            print(i,0,np.sum(out_label))\n","        else:\n","            a[i,0] = 1\n","            print(i,1,np.sum(out_label))\n","    return a\n","  \n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","    '''Function to take batches randomly'''\n","    global out_img\n","    global out_label\n","    batch_size=155\n","    '''training info'''\n","    train_info = 484*155  #50 patients with 155 images each\n","\n","    #if current_batch_no == 0:\n","    #    no_of_batches = train_info//(batch_size) \n","    #    permute_mat = np.random.permutation(no_of_batches)\n","    #  \n","    #start = permute_mat[current_batch_no]*batch_size\n","    #end = start + (batch_size)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    current_batch_no += 1\n","    #print(len(out_img))\n","    '''Input normalization'''\n","    #normalizing_input()\n","    '''normalization oof labels'''\n","    #out_label = out_label\n","    '''converting multi class to dual class'''\n","    #out_label = convert_dual_class(out_label)\n","  \n","    '''Rolling axes'''\n","    #out_img_send = Pre_processing_3D(out_img)\n","    '''hot encoding'''\n","    #out_label_send = Pre_processing_3D(out_label)\n","    \n","    last=0\n","    if current_batch_no == len(permute_mat):\n","        last=1\n","    \n","    return (out_img,out_label,current_batch_no,permute_mat,last)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZ_Puz4WVroo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":5083},"outputId":"4b750453-f7ce-4ddd-962a-afa155d64716"},"source":["arr = calc_empty()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 1 201883\n","1 1 80605\n","2 1 220841\n","3 1 213947\n","4 1 144298\n","5 1 368180\n","6 1 108953\n","7 1 281845\n","8 1 329332\n","9 1 29834\n","10 1 210056\n","11 1 31440\n","12 1 22635\n","13 1 344240\n","14 1 332372\n","15 1 135316\n","16 1 157689\n","17 1 224447\n","18 1 205435\n","19 1 439940\n","20 1 106375\n","21 1 358423\n","22 1 69629\n","23 1 34927\n","24 1 42511\n","25 1 105152\n","26 1 82010\n","27 1 140177\n","28 1 28219\n","29 1 55612\n","30 1 50490\n","31 1 280861\n","32 1 242448\n","33 1 327712\n","34 1 112555\n","35 1 192381\n","36 1 136903\n","37 1 335603\n","38 1 59727\n","39 1 197493\n","40 1 221109\n","41 1 45124\n","42 1 64856\n","43 1 59038\n","44 1 164258\n","45 1 290860\n","46 1 272541\n","47 1 233066\n","48 1 174627\n","49 1 159887\n","50 1 168608\n","51 1 148798\n","52 1 109137\n","53 1 157645\n","54 1 174678\n","55 1 177175\n","56 1 181147\n","57 1 170528\n","58 1 178628\n","59 1 261560\n","60 1 178106\n","61 1 73728\n","62 1 187931\n","63 1 383511\n","64 1 263171\n","65 1 245587\n","66 1 69776\n","67 1 37879\n","68 1 60795\n","69 1 56144\n","70 1 69766\n","71 1 134992\n","72 1 115026\n","73 1 178292\n","74 1 223735\n","75 1 285720\n","76 1 39359\n","77 1 101646\n","78 1 118682\n","79 1 200699\n","80 1 141148\n","81 1 377350\n","82 1 97629\n","83 1 45422\n","84 1 135598\n","85 1 143744\n","86 1 92208\n","87 1 338392\n","88 1 255570\n","89 1 139035\n","90 1 280334\n","91 1 182877\n","92 1 348711\n","93 1 316010\n","94 1 44935\n","95 1 182893\n","96 1 256455\n","97 1 143312\n","98 1 133167\n","99 1 134389\n","100 1 130337\n","101 1 80696\n","102 1 154265\n","103 1 77569\n","104 1 136292\n","105 1 134618\n","106 1 114135\n","107 1 138013\n","108 1 166983\n","109 1 118334\n","110 1 137206\n","111 1 255641\n","112 1 280113\n","113 1 134364\n","114 1 163669\n","115 1 281983\n","116 1 362314\n","117 1 174483\n","118 1 149921\n","119 1 53237\n","120 1 64616\n","121 1 56760\n","122 1 184318\n","123 1 227410\n","124 1 230635\n","125 1 162019\n","126 1 107980\n","127 1 30810\n","128 1 338387\n","129 1 61881\n","130 1 235278\n","131 1 116279\n","132 1 79583\n","133 1 67687\n","134 1 225329\n","135 1 84289\n","136 1 233719\n","137 1 15790\n","138 1 142812\n","139 1 235498\n","140 1 380811\n","141 1 295723\n","142 1 200462\n","143 1 71653\n","144 1 210683\n","145 1 104981\n","146 1 117277\n","147 1 123585\n","148 1 144872\n","149 1 165757\n","150 1 192288\n","151 1 230591\n","152 1 70359\n","153 1 188208\n","154 1 480877\n","155 1 302299\n","156 1 218100\n","157 1 99079\n","158 1 252410\n","159 1 40643\n","160 1 143823\n","161 1 103241\n","162 1 135515\n","163 1 111980\n","164 1 192916\n","165 1 191364\n","166 1 42799\n","167 1 48092\n","168 1 129780\n","169 1 190560\n","170 1 35197\n","171 1 224488\n","172 1 254296\n","173 1 194554\n","174 1 9150\n","175 1 11428\n","176 1 12941\n","177 1 16895\n","178 1 29170\n","179 1 124199\n","180 1 316969\n","181 1 259212\n","182 1 24629\n","183 1 39737\n","184 1 40627\n","185 1 41976\n","186 1 47353\n","187 1 48240\n","188 1 44556\n","189 1 136646\n","190 1 286247\n","191 1 120149\n","192 1 169638\n","193 1 108863\n","194 1 136569\n","195 1 183573\n","196 1 160538\n","197 1 291108\n","198 1 97802\n","199 1 217611\n","200 1 241554\n","201 1 298675\n","202 1 163501\n","203 1 151852\n","204 1 218059\n","205 1 343512\n","206 1 249905\n","207 1 125756\n","208 1 130845\n","209 1 150876\n","210 1 95757\n","211 1 97203\n","212 1 85384\n","213 1 86890\n","214 1 84874\n","215 1 76802\n","216 1 318358\n","217 1 248197\n","218 1 179747\n","219 1 104355\n","220 1 124119\n","221 1 94008\n","222 1 346219\n","223 1 26038\n","224 1 307577\n","225 1 71366\n","226 1 148759\n","227 1 167154\n","228 1 304502\n","229 1 37379\n","230 1 133397\n","231 1 31569\n","232 1 64980\n","233 1 170961\n","234 1 176245\n","235 1 230315\n","236 1 210395\n","237 1 25177\n","238 1 251019\n","239 1 332916\n","240 1 215764\n","241 1 304345\n","242 1 152278\n","243 1 244133\n","244 1 114011\n","245 1 123385\n","246 1 110516\n","247 1 104512\n","248 1 482086\n","249 1 236151\n","250 1 402778\n","251 1 125457\n","252 1 102357\n","253 1 100095\n","254 1 169250\n","255 1 217248\n","256 1 128179\n","257 1 87937\n","258 1 67267\n","259 1 143384\n","260 1 324023\n","261 1 33944\n","262 1 250005\n","263 1 36123\n","264 1 406262\n","265 1 372451\n","266 1 147704\n","267 1 231849\n","268 1 203198\n","269 1 91691\n","270 1 348302\n","271 1 103576\n","272 1 116914\n","273 1 128307\n","274 1 201883\n","275 1 224450\n","276 1 329402\n","277 1 29834\n","278 1 281846\n","279 1 332372\n","280 1 220842\n","281 1 144302\n","282 1 368204\n","283 1 213961\n","284 1 439950\n","285 1 344243\n","286 1 31444\n","287 1 22635\n","288 1 135336\n","289 1 108953\n","290 1 157690\n","291 1 205439\n","292 1 80605\n","293 1 210060\n","294 1 282906\n","295 1 89266\n","296 1 36536\n","297 1 145394\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jJjTq1EdWGKB","colab_type":"code","colab":{}},"source":["print(np.shape(arr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAbQ-i6ZWjOL","colab_type":"code","colab":{}},"source":["with h5py.File('/content/gdrive/My Drive/Brain_Tumour_segmentation/find_empty.hdf5','w') as f:\n","    dset = f.create_dataset(\"chech_tumour\",data=arr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAMdBqoL5qGI","colab_type":"code","outputId":"0deaf2b2-368f-4f9e-aab1-91459d38e6cf","executionInfo":{"status":"ok","timestamp":1556546586208,"user_tz":-330,"elapsed":4695428,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":8906}},"source":["def cut_brain():\n","    current_batch_no = 0\n","    permute_mat = 0\n","    iteration = 0\n","    #while(1):\n","  \n","    #dice_val,sess_results = sess.run([dice,training_output], feed_dict={X: epoch_x, y: epoch_y})\n","    #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","  \n","    '''variables'''\n","    top = 0\n","    left= 0\n","    right = 0\n","    bot = 0\n","    flag= 0\n","    while(1):\n","        epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","        x_batch = epoch_x[:,:,:,0]\n","        for i in range(np.shape(x_batch)[2]):\n","            for j in range(np.shape(x_batch)[0]):\n","                if np.sum(x_batch[j,:,i])>0:\n","                    break\n","            for k in range(np.shape(x_batch)[1]):\n","                if np.sum(x_batch[:,k,i])>0:\n","                    break\n","            for l in range(np.shape(x_batch)[0]-1,-1,-1):\n","                if np.sum(x_batch[l,:,i])>0:\n","                    break\n","            for m in range(np.shape(x_batch)[1]-1,-1,-1):\n","                if np.sum(x_batch[:,m,i])>0:\n","                    break\n","            if flag == 0:\n","                top = j\n","                left= k\n","                bot = l\n","                right=m\n","                flag = 1\n","            else:\n","                if j<top:\n","                    top = j\n","                if k<left:\n","                    left = k\n","                if l>bot:\n","                    bot = l\n","                if m>right:\n","                    right = m\n","        if last ==1:\n","            break\n","        iteration +=1\n","        print(iteration)\n","    return(top,left,bot,right)\n","        \n","    \n","  \n","\n","a,b,c,d = cut_brain()\n","print(a,b,c,d)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","38 19 199 210\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"source":["'''DEFINING VARIABLES'''\n","\n","batch_size=2              #batch size taken at a time\n","n_class = 4               #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","X = tf.placeholder(shape=[None,160,240,240,4], dtype=tf.float32, name='input_image')\n","y = tf.placeholder(shape=[None,160,240,240,1], dtype=tf.int64, name='input_label')\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","  '''Defining a filter variable to perform convolution'''\n","  return (tf.Variable(tf.random_normal([ker_size,ker_size,ker_size,in_chan,out_chan],stddev=0.05),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","  '''Defining a filter variable to perform transpose convolution'''\n","  return (tf.Variable(tf.random_normal([ker_size,ker_size,ker_size,out_chan,in_chan],stddev=0.05),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","  '''Function to perform Convolution and apply activation filter'''\n","  '''Convolution'''\n","  conv = tf.nn.conv3d(input_im,filter_mask,strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","  #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","  '''Activation'''\n","  if activation == 'relu':\n","      return(tf.nn.relu(conv))\n","  elif activation == 'softmax':\n","      return(tf.nn.softmax(conv))\n","  elif activation == 'elu':\n","      return(tf.nn.elu(conv))\n","  else:\n","      #activation == 'None'\n","      return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","  '''Function to perform Transpose Convolution and apply activation filter'''\n","  '''Transpose Convolution'''\n","  inp_shape = np.shape(input_im) #tf.shape()\n","  out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2),int(inp_shape[3].value*2), int(inp_shape[4].value/2)]\n","    \n","  conv = tf.nn.conv3d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","  #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","  '''Activation'''\n","  if activation == 'relu':\n","      return(tf.nn.relu(conv))\n","  elif activation == 'softmax':\n","      return(tf.nn.softmax(conv))\n","  elif activation == 'elu':\n","      return(tf.nn.elu(conv))\n","  else:\n","      #activation == 'None'\n","      return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","  '''normalization of each input channels'''\n","  global out_img\n","  '''CHANNEL INFO'''\n","  # maximum value found using function called \"Finding_maximum_to_normalise \"\n","  #'''max value for dimension 4 is 5337.0'''\n","  #'''max value for dimension 3 is 11737.0'''\n","  #'''max value for dimension 2 is 9751.0'''\n","  #'''max value for dimension 1 is 6476.0'''\n","  out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","  out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","  out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","  out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","  \n","\n","  \n","def Pre_processing_3D(a):\n","  '''Function to roll axis to convert array into[depth,width,height,channels] and the divide it in batches'''\n","  #print(np.shape(a))\n","  b = np.rollaxis(a,2, 0)\n","  #print(np.shape(b))\n","  #image shape\n","  out_arr = np.empty(shape=[batch_size,160,np.shape(b)[1],np.shape(b)[2],np.shape(b)[3]])\n","  for i in range(batch_size):\n","    start = i*155\n","    end = start+155\n","    out_arr[i,0:155,:,:,:] = b[start:end,:,:,:]\n","  \n","  a = [out_arr[:,154:155,:,:,:]]*5\n","  out_arr[:,155:160,:,:,:] = a[0]\n","  \n","  '''clippig data from front of each batch'''\n","  '''to fit the model we remove first 3 slices of each batch'''\n","  '''shape of a is [batch_size,depth,width,height,channels]'''\n","  out_send = out_arr[:,:,:,:,:]\n","  #print(\"arr\",np.shape(out_arr),\"send\",np.shape(out_send))\n","  return (out_send)\n","\n","#a = np.random.rand(240,240,batch_size*155,4)\n","#b = Pre_processing_3D(a)\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","  '''Function to take batches randomly'''\n","  global out_img\n","  global out_label\n","  \n","  '''training info'''\n","  train_info = 50  #50 patients with 155 images each\n","\n","  if current_batch_no == 0:\n","      no_of_batches = train_info//(batch_size) \n","      permute_mat = np.random.permutation(no_of_batches)\n","  \n","  start = permute_mat[current_batch_no]*batch_size*155\n","  end = start + (batch_size*155)\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","  current_batch_no += 1\n","  #print(len(out_img))\n","  '''Input normalization'''\n","  normalizing_input()\n","  '''normalization oof labels'''\n","  #out_label = out_label\n","  '''converting multi class to dual class'''\n","  #out_label = convert_dual_class(out_label)\n","  \n","  '''Rolling axes'''\n","  out_img_send = Pre_processing_3D(out_img)\n","  '''hot encoding'''\n","  out_label_send = Pre_processing_3D(out_label)\n","    \n","  last=0\n","  if current_batch_no == len(permute_mat):\n","      last=1\n","    \n","  return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","\n","\n","def test_batch():\n","  '''Function to take next test batch'''\n","  \n","  global out_img\n","  global out_label\n","    \n","  '''training and testing info'''\n","  train_info = 50  #100 patients with 155 images each\n","  test_info = 184  #100 patients with 155 images each\n","    \n","  no_of_batches = test_info//batch_size  \n","  permute_mat = np.random.permutation(no_of_batches)\n","  start = (permute_mat[0]*batch_size*155) +(train_info*155)\n","  end = start + (155*batch_size)\n","  train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","  train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","  '''normalization'''\n","  normalizing_input()\n","  '''Converting multiclass to dual class'''\n","  #out_label = convert_dual_class(out_label) \n","  '''rolling axis'''\n","  out_img_send = Pre_processing_3D(out_img)\n","  '''hot encoding'''\n","  out_label_send = Pre_processing_3D(out_label)\n","    \n","  return (out_img_send,out_label_send)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-NTV_zXbaY7","colab_type":"code","colab":{}},"source":["'''loss function definition'''\n","\n","\n","###############################################################################################################################\n","#LOSS FUNCTION IN VERSION 1 OF TRIAL 2\n","###############################################################################################################################\n","def dice_coeff(y_true, y_pred):\n","  '''Finding dice coefficient for one class'''\n","  flat_layer = tf.layers.Flatten()\n","  y_true_f = flat_layer(y_true)\n","  y_pred_f = flat_layer(y_pred)\n","  intersection = tf.math.reduce_sum(y_true_f*y_pred_f)\n","  return (2*intersection )/(tf.math.reduce_sum(y_true_f)+tf.math.reduce_sum(y_pred_f)+1 )\n","\n","  \n","def dice_coef_multilabel(y_true, y_pred, numLabels = n_class):\n","  '''Finding dice loss for each class'''\n","  dice = 0\n","  \n","    \n","  for index in range(numLabels):\n","    #weight of each class(not in previous versions)\n","    frequency = (tf.reduce_sum(y_true[:,:,:,index,0]))\n","    #if frequency == 0 or frequency == 240*240*batch_size:\n","      #weight = 0\n","    #else:\n","      #weight = batch_size/frequency\n","    if index == 0:\n","      weight = 0\n","    else:\n","      weight = 1\n","    '''Here, as of now we are neglecting the background class'''\n","    dice -= (weight*dice_coeff(y_true[:,:,:,index,0],y_pred[:,:,:,index]))\n","    #print(weight)\n","  return (dice)\n","#############################################################################################################################\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","  '''function for hot encoding images'''\n","  a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","  print(np.shape(a))\n","  b = tf.transpose(a,perm=[0,1,2,3,5,4])\n","  return b\n","\n","\n","#############################################################################################################################\n","#NEW DICE LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_dice_coeff(y_true, y_pred):\n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3])\n","    w = 1/((w**2)+0.000001)\n","    print(np.shape(w))\n","    # Compute gen dice coef:\n","    numerator = y_true*y_pred\n","    denominator = y_true+y_pred\n","    a=b=0\n","    for i in range(np.shape(w)[0]):\n","      a += w[i]*numerator[:,:,:,:,i]\n","      b += w[i]*denominator[:,:,:,:,i]\n","      \n","    \n","    num = tf.reduce_sum(a)\n","    den = tf.reduce_sum(b)\n","\n","    gen_dice_coef = num/den\n","\n","    return (-gen_dice_coef)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"source":["INITIALIZING THE MODEL FILTERS"]},{"cell_type":"code","metadata":{"id":"oHMGiKsBL-QK","colab_type":"code","colab":{}},"source":["'''MODEL1 Filter definition'''\n","'''LEFT'''\n","\n","filter1 = left_filter_def(3,4,8,name='filter1')\n","filter2 = left_filter_def(3,8,8,name='filter2')\n","\n","filter3 = left_filter_def(3,8,16,name='filter3')\n","filter4 = left_filter_def(3,16,16,name='filter4')\n","\n","filter5 = left_filter_def(3,16,32,name='filter5')\n","filter6 = left_filter_def(3,32,32,name='filter6')\n","\n","filter7 = left_filter_def(3,32,64,name='filter7')\n","filter8 = left_filter_def(3,64,64,name='filter8')\n","\n","filter9 = left_filter_def(3,64,128,name='filter9')\n","filter10= left_filter_def(3,128,64,name='filter10')\n","\n","'''RIGHT'''\n","\n","filter11 = right_filter_def(3,128,64,name='filter11')\n","filter12 = left_filter_def(3,64,64,name='filter12')\n","filter13 = left_filter_def(3,64,32,name='filter13')\n","\n","filter14 = right_filter_def(3,64,32,name='filter14')\n","filter15 = left_filter_def(3,32,32,name='filter15')\n","filter16 = left_filter_def(3,32,16,name='filter16')\n","\n","filter17 = right_filter_def(3,32,16,name='filter17')\n","filter18 = left_filter_def(3,16,16,name='filter18')\n","filter19 = left_filter_def(3,16,8,name='filter19')\n","\n","filter20 = right_filter_def(3,16,8,name='filter20')\n","filter21 = left_filter_def(3,8,8,name='filter21')\n","filter22 = left_filter_def(3,8,n_class,name='filter22')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"source":["DEFINING THE MODEL STRUCTURE"]},{"cell_type":"code","metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"source":["def predict_model1(X):\n","    '''Function to define the UNET model'''\n","    with tf.name_scope(\"BLOCK1\"):\n","      '''BLOCK1'''\n","      CNN1 = Conv_layer(X,filter1,stride=1,activation='relu',name='CNN1')\n","      print (\"CNN1\",np.shape(CNN1))\n","      CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='CNN2')\n","      print (\"CNN2\",np.shape(CNN2))\n","      pool1 = tf.nn.max_pool3d(CNN2,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","      '''BLOCK2'''\n","      CNN3 = Conv_layer(pool1,filter3,stride=1,activation='relu',name='CNN3')\n","      print (\"CNN3\",np.shape(CNN3))\n","      CNN4 = Conv_layer(CNN3,filter4,stride=1,activation='relu',name='CNN4')\n","      print (\"CNN4\",np.shape(CNN4))\n","      pool2 = tf.nn.max_pool3d(CNN4,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL2')\n","    \n","    with tf.name_scope(\"BLOCK3\"):\n","      '''BLOCK3'''\n","      CNN5 = Conv_layer(pool2,filter5,stride=1,activation='relu',name='CNN5')\n","      print (\"CNN5\",np.shape(CNN5))\n","      CNN6 = Conv_layer(CNN5,filter6,stride=1,activation='relu',name='CNN6')\n","      print (\"CNN6\",np.shape(CNN6))\n","      pool3 = tf.nn.max_pool3d(CNN6,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL3')\n","    \n","    with tf.name_scope(\"BLOCK4\"):\n","      '''BLOCK4'''\n","      CNN7 = Conv_layer(pool3,filter7,stride=1,activation='relu',name='CNN7')\n","      print (\"CNN7\",np.shape(CNN7))\n","      CNN8 = Conv_layer(CNN7,filter8,stride=1,activation='relu',name='CNN8')\n","      print (\"CNN8\",np.shape(CNN8))\n","      pool4 = tf.nn.max_pool3d(CNN8,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL4')\n","    \n","    with tf.name_scope(\"BLOCK5\"):\n","      '''BLOCK5'''\n","      CNN9 = Conv_layer(pool4,filter9,stride=1,activation='relu',name='CNN9')\n","      print (\"CNN9\",np.shape(CNN9))\n","      CNN10 = Conv_layer(CNN9,filter10,stride=1,activation='relu',name='CNN10')\n","      print (\"CNN10\",np.shape(CNN10))\n","    \n","    '''Moving UP'''\n","    \n","    with tf.name_scope(\"BLOCK6\"):\n","      '''BLOCK6'''\n","      concat1 = tf.concat([CNN10,pool4],axis=4,name='CONCAT1')\n","      print (\"concat1\",np.shape(concat1))\n","      DCNN1= Deconv_layer(concat1,filter11,stride=2,activation='relu',name='DE_CONV1')\n","      print (\"DCNN1\",np.shape(DCNN1))\n","      CNN11 = Conv_layer(DCNN1,filter12,stride=1,activation='relu',name='CNN11')\n","      #print (\"CNN11\",np.shape(CNN11))\n","      CNN12 = Conv_layer(CNN11,filter13,stride=1,activation='relu',name='CNN12')\n","      #print (\"CNN12\",np.shape(CNN12))\n","    \n","    with tf.name_scope(\"BLOCK7\"):\n","      '''BLOCK7'''\n","      concat2 = tf.concat([CNN12,pool3],axis=4,name='CONCAT2')\n","      #print (\"concat2\",np.shape(concat2))\n","      DCNN2= Deconv_layer(concat2,filter14,stride=2,activation='relu',name='DE_CONV2')\n","      #print (\"DCNN2\",np.shape(DCNN2))\n","      CNN13 = Conv_layer(DCNN2,filter15,stride=1,activation='relu',name='CNN13')\n","      #print (\"CNN13\",np.shape(CNN13))\n","      CNN14 = Conv_layer(CNN13,filter16,stride=1,activation='relu',name='CNN14')\n","      #print (\"CNN14\",np.shape(CNN14))\n","    \n","    with tf.name_scope(\"BLOCK8\"):\n","      '''BLOCK8'''\n","      concat3 = tf.concat([CNN14,pool2],axis=4,name='CONCAT3')\n","      #print (\"concat3\",np.shape(concat3))\n","      DCNN3= Deconv_layer(concat3,filter17,stride=2,activation='relu',name='DE_CONV3')\n","      #print (\"DCNN3\",np.shape(DCNN3))\n","      CNN15 = Conv_layer(DCNN3,filter18,stride=1,activation='relu',name='CNN14')\n","      #print (\"CNN15\",np.shape(CNN15))\n","      CNN16 = Conv_layer(CNN15,filter19,stride=1,activation='relu',name='CNN15')\n","      #print (\"CNN16\",np.shape(CNN16))\n","      \n","    with tf.name_scope(\"BLOCK9\"):\n","      '''BLOCK9'''\n","      concat4 = tf.concat([CNN16,pool1],axis=4,name='CONCAT4')\n","      #print (\"concat4\",np.shape(concat4))\n","      DCNN4= Deconv_layer(concat4,filter20,stride=2,activation='relu',name='DE_CONV4')\n","      #print (\"DCNN4\",np.shape(DCNN4))\n","      CNN17 = Conv_layer(DCNN4,filter21,stride=1,activation='relu',name='CNN17')\n","      #print (\"CNN17\",np.shape(CNN17))\n","      CNN18 = Conv_layer(CNN17,filter22,stride=1,activation='relu',name='CNN18')\n","      print (\"CNN18\",np.shape(CNN18))\n","      return (CNN18)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_ROD14VTzmB","colab_type":"code","colab":{}},"source":["def train_unet(learning_rate =0.0001,n_epochs = 100):\n","  '''Function to Train U-Net'''\n","  prediction = predict_model1(X)#logits\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"LOSS_FUNCTION\"):\n","    '''using multi dimensional dice'''\n","    hot_y = hot_encode(y)\n","    #dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","    dice = 1 + generalized_dice_coeff(hot_y[:,:,:,:,:,0], prediction)\n","  '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","  with tf.name_scope(\"COST_FUNCTION\"):\n","    '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","    loss = tf.reduce_mean(dice, name=\"loss\")\n","\n","  with tf.name_scope(\"OPTIMIZER\"):\n","    '''Optimizer'''\n","    optimize = tf.train.AdamOptimizer(learning_rate = learning_rate)\n","    training_output = optimize.minimize(loss)\n","\n","\n","  '''initializing'''\n","  init = tf.global_variables_initializer()\n","  saver = tf.train.Saver()\n","\n","  '''Timing'''\n","  start = time.time()\n","  '''Session'''\n","  with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","      current_batch_no = 0\n","      permute_mat = 0\n","      iteration = 0\n","      while(1):\n","        epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","        dice_val,sess_results = sess.run([dice,training_output], feed_dict={X: epoch_x, y: epoch_y})\n","        #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","            \n","        '''DICE Coefficient for iteration'''\n","        if iteration%10==0:\n","          acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","          test_images, test_labels = test_batch()\n","          acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","          print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","        if last ==1:\n","          break\n","        iteration +=1\n","      test_images, test_labels = test_batch()\n","      acc_test = 1-dice.eval(feed_dict={X: test_images, y: test_labels})\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","      print(\"After Epoch\", epoch+1, \"Test accuracy:\", acc_test)\n","      print(\"-------------------------------------------------------------------------------------------------------\")\n","        \n","        \n","      if epoch % 1 == 0:\n","        test_example =   test_images\n","        test_example_gt = test_labels#np.rollaxis(test_labels,2,0)\n","        sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","        sess_results = sess_results[0,100,:,:,1] + (2*sess_results[0,100,:,:,2]) + (3*sess_results[0,100,:,:,3])\n","        test_example = test_example[0,100,:,:,3]\n","        test_example_gt = test_example_gt[0,100,:,:,:]\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Original Image')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/3D segmentation/without_resnet/'+str(epoch)+\"a_Original_Image.png\")\n","              \n","              \n","        plt.figure()\n","        plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Ground Truth Mask')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/3D segmentation/without_resnet/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","        plt.figure()\n","        plt.imshow(np.squeeze(sess_results),cmap='gray')\n","        plt.axis('off')\n","        plt.title('Generated Mask')\n","        plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/3D segmentation/without_resnet/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","        plt.close('all')\n","\n","    '''Saving the graph'''\n","    save_path = saver.save(sess, \"/content/gdrive/My Drive/Brain_Tumour_segmentation/3D segmentation/final_madel_graph\")\n","  end = time.time()\n","  total_time = end-start\n","  return (total_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"cell_type":"code","metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"73401257-313f-4094-f37a-5a9964b50040","executionInfo":{"status":"error","timestamp":1554878659087,"user_tz":-330,"elapsed":526824,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":3179}},"source":["train_unet()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CNN1 (?, 160, 240, 240, 8)\n","CNN2 (?, 160, 240, 240, 8)\n","CNN3 (?, 80, 120, 120, 16)\n","CNN4 (?, 80, 120, 120, 16)\n","CNN5 (?, 40, 60, 60, 32)\n","CNN6 (?, 40, 60, 60, 32)\n","CNN7 (?, 20, 30, 30, 64)\n","CNN8 (?, 20, 30, 30, 64)\n","CNN9 (?, 10, 15, 15, 128)\n","CNN10 (?, 10, 15, 15, 64)\n","concat1 (?, 10, 15, 15, 128)\n","DCNN1 (2, 20, 30, 30, 64)\n","CNN18 (2, 160, 240, 240, 4)\n","(?, 160, 240, 240, 1, 4)\n","(4,)\n","Minibatch at Epoch 1 batch 1 Train accuracy: 0.0005748867988586426 Test accuracy: 0.00010734796524047852\n","Minibatch at Epoch 1 batch 11 Train accuracy: 0.0022426247596740723 Test accuracy: 0.0015712976455688477\n","Minibatch at Epoch 1 batch 21 Train accuracy: 0.0 Test accuracy: 0.0038892030715942383\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 1 Test accuracy: 0.005032718181610107\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 2 batch 1 Train accuracy: 0.014599323272705078 Test accuracy: 0.0016951560974121094\n","Minibatch at Epoch 2 batch 11 Train accuracy: 0.022907018661499023 Test accuracy: 0.0120469331741333\n","Minibatch at Epoch 2 batch 21 Train accuracy: 0.003609299659729004 Test accuracy: 0.005012989044189453\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 2 Test accuracy: 0.04578518867492676\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 3 batch 1 Train accuracy: 0.004379987716674805 Test accuracy: 0.008924007415771484\n","Minibatch at Epoch 3 batch 11 Train accuracy: 0.007027328014373779 Test accuracy: 6.473064422607422e-05\n","Minibatch at Epoch 3 batch 21 Train accuracy: 0.10957562923431396 Test accuracy: 0.005685389041900635\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 3 Test accuracy: 0.018448293209075928\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 4 batch 1 Train accuracy: 0.15663480758666992 Test accuracy: 0.16326630115509033\n","Minibatch at Epoch 4 batch 11 Train accuracy: 0.0 Test accuracy: 0.0026573538780212402\n","Minibatch at Epoch 4 batch 21 Train accuracy: 0.0187341570854187 Test accuracy: 0.003202676773071289\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 4 Test accuracy: 0.0170438289642334\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 5 batch 1 Train accuracy: 0.03894209861755371 Test accuracy: 0.020628809928894043\n","Minibatch at Epoch 5 batch 11 Train accuracy: 0.07416599988937378 Test accuracy: 0.013818740844726562\n","Minibatch at Epoch 5 batch 21 Train accuracy: 0.06511330604553223 Test accuracy: 0.19251906871795654\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 5 Test accuracy: 0.15820717811584473\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 6 batch 1 Train accuracy: 0.15372610092163086 Test accuracy: 0.05549144744873047\n","Minibatch at Epoch 6 batch 11 Train accuracy: 0.11969131231307983 Test accuracy: 0.1348717212677002\n","Minibatch at Epoch 6 batch 21 Train accuracy: 0.2722729444503784 Test accuracy: 0.24263310432434082\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 6 Test accuracy: 0.18586617708206177\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 7 batch 1 Train accuracy: 0.12779486179351807 Test accuracy: 0.26468437910079956\n","Minibatch at Epoch 7 batch 11 Train accuracy: 0.0 Test accuracy: 0.030296266078948975\n","Minibatch at Epoch 7 batch 21 Train accuracy: 0.47023212909698486 Test accuracy: 0.6051846742630005\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 7 Test accuracy: 0.6649422645568848\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 8 batch 1 Train accuracy: 0.3348230719566345 Test accuracy: 0.1446692943572998\n","Minibatch at Epoch 8 batch 11 Train accuracy: 0.8021737933158875 Test accuracy: 0.19422149658203125\n","Minibatch at Epoch 8 batch 21 Train accuracy: 0.5760923027992249 Test accuracy: 0.6162896156311035\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 8 Test accuracy: 0.17968541383743286\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 9 batch 1 Train accuracy: 0.3244030475616455 Test accuracy: 0.043879032135009766\n","Minibatch at Epoch 9 batch 11 Train accuracy: 0.47004252672195435 Test accuracy: 0.4320704936981201\n","Minibatch at Epoch 9 batch 21 Train accuracy: 0.002345263957977295 Test accuracy: 0.0016154050827026367\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 9 Test accuracy: 0.0012035369873046875\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 10 batch 1 Train accuracy: 0.0035477876663208008 Test accuracy: 0.00048488378524780273\n","Minibatch at Epoch 10 batch 11 Train accuracy: 0.0075339674949646 Test accuracy: 0.005609989166259766\n","Minibatch at Epoch 10 batch 21 Train accuracy: 0.008378982543945312 Test accuracy: 0.009780347347259521\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 10 Test accuracy: 0.011167705059051514\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 11 batch 1 Train accuracy: 0.013830602169036865 Test accuracy: 0.0032150745391845703\n","Minibatch at Epoch 11 batch 11 Train accuracy: 0.020813047885894775 Test accuracy: 0.023811042308807373\n","Minibatch at Epoch 11 batch 21 Train accuracy: 0.04506409168243408 Test accuracy: 0.0\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 11 Test accuracy: 0.06833595037460327\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 12 batch 1 Train accuracy: 0.042579591274261475 Test accuracy: 0.05945616960525513\n","Minibatch at Epoch 12 batch 11 Train accuracy: 0.09989500045776367 Test accuracy: 0.01853764057159424\n","Minibatch at Epoch 12 batch 21 Train accuracy: 0.03707718849182129 Test accuracy: 0.13808554410934448\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 12 Test accuracy: 0.0995645523071289\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 13 batch 1 Train accuracy: 0.017085611820220947 Test accuracy: 0.07005572319030762\n","Minibatch at Epoch 13 batch 11 Train accuracy: 0.05810898542404175 Test accuracy: 0.14652717113494873\n","Minibatch at Epoch 13 batch 21 Train accuracy: 0.17834675312042236 Test accuracy: 0.06638014316558838\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 13 Test accuracy: 0.23584765195846558\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 14 batch 1 Train accuracy: 0.18024569749832153 Test accuracy: 0.34126341342926025\n","Minibatch at Epoch 14 batch 11 Train accuracy: 0.3045692443847656 Test accuracy: 0.0004722476005554199\n","Minibatch at Epoch 14 batch 21 Train accuracy: 0.0 Test accuracy: 0.1142662763595581\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 14 Test accuracy: 0.2976566553115845\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 15 batch 1 Train accuracy: 0.46053004264831543 Test accuracy: 0.28301072120666504\n","Minibatch at Epoch 15 batch 11 Train accuracy: 0.0 Test accuracy: 0.6457176804542542\n","Minibatch at Epoch 15 batch 21 Train accuracy: 0.4392654299736023 Test accuracy: 0.3448507785797119\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 15 Test accuracy: 0.17828458547592163\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 16 batch 1 Train accuracy: 0.008054733276367188 Test accuracy: 0.14209574460983276\n","Minibatch at Epoch 16 batch 11 Train accuracy: 0.40698277950286865 Test accuracy: 0.6171920299530029\n","Minibatch at Epoch 16 batch 21 Train accuracy: 0.6279339790344238 Test accuracy: 0.8296966552734375\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 16 Test accuracy: 0.35602760314941406\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 17 batch 1 Train accuracy: 0.8023852705955505 Test accuracy: 0.0009801983833312988\n","Minibatch at Epoch 17 batch 11 Train accuracy: 0.4419722557067871 Test accuracy: 0.7640342116355896\n","Minibatch at Epoch 17 batch 21 Train accuracy: 0.39348089694976807 Test accuracy: 0.29326343536376953\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 17 Test accuracy: 0.6699993014335632\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 18 batch 1 Train accuracy: 0.6843301057815552 Test accuracy: 0.1834477186203003\n","Minibatch at Epoch 18 batch 11 Train accuracy: 0.3711591958999634 Test accuracy: 0.0\n","Minibatch at Epoch 18 batch 21 Train accuracy: 0.19838571548461914 Test accuracy: 0.4754936695098877\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 18 Test accuracy: 0.8187758326530457\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 19 batch 1 Train accuracy: 0.9390514492988586 Test accuracy: 0.6643241047859192\n","Minibatch at Epoch 19 batch 11 Train accuracy: 0.27661943435668945 Test accuracy: 0.39469248056411743\n","Minibatch at Epoch 19 batch 21 Train accuracy: 0.7131462097167969 Test accuracy: 0.011966407299041748\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 19 Test accuracy: 0.298095703125\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 20 batch 1 Train accuracy: 0.2331991195678711 Test accuracy: 0.20614498853683472\n","Minibatch at Epoch 20 batch 11 Train accuracy: 0.25877225399017334 Test accuracy: 0.3079107999801636\n","Minibatch at Epoch 20 batch 21 Train accuracy: 0.5794699192047119 Test accuracy: 0.7533946633338928\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 20 Test accuracy: 0.17121362686157227\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 21 batch 1 Train accuracy: 0.46889567375183105 Test accuracy: 0.20558947324752808\n","Minibatch at Epoch 21 batch 11 Train accuracy: 0.011749088764190674 Test accuracy: 0.0033032894134521484\n","Minibatch at Epoch 21 batch 21 Train accuracy: 0.015235424041748047 Test accuracy: 0.008677065372467041\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 21 Test accuracy: 0.005494475364685059\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 22 batch 1 Train accuracy: 0.0 Test accuracy: 0.0\n","Minibatch at Epoch 22 batch 11 Train accuracy: 0.023447811603546143 Test accuracy: 0.018585681915283203\n","Minibatch at Epoch 22 batch 21 Train accuracy: 0.003932833671569824 Test accuracy: 0.01181185245513916\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 22 Test accuracy: 0.02408391237258911\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 23 batch 1 Train accuracy: 0.01870274543762207 Test accuracy: 0.01945549249649048\n","Minibatch at Epoch 23 batch 11 Train accuracy: 0.037765443325042725 Test accuracy: 0.03181195259094238\n","Minibatch at Epoch 23 batch 21 Train accuracy: 0.0 Test accuracy: 0.03152889013290405\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 23 Test accuracy: 0.013389766216278076\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 24 batch 1 Train accuracy: 0.04394853115081787 Test accuracy: 0.027760982513427734\n","Minibatch at Epoch 24 batch 11 Train accuracy: 0.046581923961639404 Test accuracy: 0.031198501586914062\n","Minibatch at Epoch 24 batch 21 Train accuracy: 0.032096803188323975 Test accuracy: 0.030750572681427002\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 24 Test accuracy: 0.05726414918899536\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 25 batch 1 Train accuracy: 0.06186521053314209 Test accuracy: 0.030947983264923096\n","Minibatch at Epoch 25 batch 11 Train accuracy: 0.056736767292022705 Test accuracy: 0.0631912350654602\n","Minibatch at Epoch 25 batch 21 Train accuracy: 0.09391367435455322 Test accuracy: 0.02797877788543701\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 25 Test accuracy: 0.058214783668518066\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 26 batch 1 Train accuracy: 0.0 Test accuracy: 0.050159752368927\n","Minibatch at Epoch 26 batch 11 Train accuracy: 0.037391602993011475 Test accuracy: 0.08887577056884766\n","Minibatch at Epoch 26 batch 21 Train accuracy: 0.051590144634246826 Test accuracy: 0.06888198852539062\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 26 Test accuracy: 0.06935000419616699\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 27 batch 1 Train accuracy: 0.10458976030349731 Test accuracy: 0.0\n","Minibatch at Epoch 27 batch 11 Train accuracy: 0.027685463428497314 Test accuracy: 0.06195223331451416\n","Minibatch at Epoch 27 batch 21 Train accuracy: 0.06632012128829956 Test accuracy: 0.03796041011810303\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 27 Test accuracy: 0.11818450689315796\n","-------------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Icc3WNqr_ZYR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
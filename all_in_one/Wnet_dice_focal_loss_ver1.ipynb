{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Wnet_dice_focal_loss_ver1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","2. The problem of class imbalance still persists and dice coeff is more than 1. So i this version we have implemented a new dice coefficient function.\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eSGVMeVbibw0","colab_type":"code","outputId":"eaa195bf-932c-4810-c6af-96bd66c88e00","executionInfo":{"status":"ok","timestamp":1563608414219,"user_tz":-330,"elapsed":1612,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","import math\n","import os\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"source":["'''DEFINING VARIABLES'''\n","\n","batch_size=1              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","'''Here we crop the 155x240x240 image to  160x160x192 by repeating the last layer 5 times to convert 155 to 160'''\n","X = tf.placeholder(shape=[None,160,160,192,4], dtype=tf.float32, name='input_image')\n","y = tf.placeholder(shape=[None,160,160,192,1], dtype=tf.int64, name='hot_encode_label')\n","training = tf.placeholder_with_default(False, shape = (), name = 'training')\n","gamma = tf.placeholder(shape=[], dtype=tf.float32, name=\"dice_weight\")\n","##alpha = tf.placeholder(shape=[], dtype=tf.float32, name=\"mse_focal_weight\")\n","learning_rate = tf.placeholder(shape=[], dtype=tf.float32, name=\"learning_rate\")\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size*155,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size*155,1),dtype=np.int64)\n","\n","'''parameter constants'''\n","#LR = np.array([0.0001])\n","#gm = np.array([0.9])\n","#al = np.array([0.5])\n","LR = 0.0001\n","gm = 0.9\n","##al = float(0.6)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","    '''Defining a filter variable to perform convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))           #HE initialization\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,in_chan,out_chan],stddev=stddev),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","    '''Defining a filter variable to perform transpose convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,out_chan,in_chan],stddev=stddev),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","    '''Function to perform Convolution and apply activation filter'''\n","    '''Convolution'''\n","    conv = tf.nn.conv3d(input_im,filter_mask,strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","    '''Function to perform Transpose Convolution and apply activation filter'''\n","    '''Transpose Convolution'''\n","    inp_shape = np.shape(input_im) #tf.shape()\n","    out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2),int(inp_shape[3].value*2), int(inp_shape[4].value/2)]\n","    \n","    conv = tf.nn.conv3d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","    '''normalization of each input channels'''\n","    global out_img\n","    '''CHANNEL INFO'''\n","    # maximum value found using function called \"Finding_maximum_to_normalise \"\n","    #'''max value for dimension 4 is 5337.0'''\n","    #'''max value for dimension 3 is 11737.0'''\n","    #'''max value for dimension 2 is 9751.0'''\n","    #'''max value for dimension 1 is 6476.0'''\n","    out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","    out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","    out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","    out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","    \n","    \n","\n","def crop_image_fit_brain(in_image):\n","    '''cropping size was found using the code finding_brain.ipynb'''\n","    left = 19\n","    right= 210\n","    top  = 38\n","    bot  = 199\n","    out_image = in_image[top:(bot-1),left:(right+1),:,:]\n","    return (out_image)\n","\n","\n","def Pre_processing_3D(a):\n","    '''Function to roll axis to convert array into[depth,width,height,channels] and the divide it in batches'''\n","    #print(np.shape(a))\n","    b = np.rollaxis(a,2, 0)\n","    #print(np.shape(b))\n","    #image shape\n","    out_arr = np.empty(shape=[batch_size,160,np.shape(b)[1],np.shape(b)[2],np.shape(b)[3]])\n","    for i in range(batch_size):\n","        start = i*155\n","        end = start+155\n","        out_arr[i,0:155,:,:,:] = b[start:end,:,:,:]\n","    \n","    a = [out_arr[:,154:155,:,:,:]]*5\n","    out_arr[:,155:160,:,:,:] = a[0]\n","    \n","    '''clippig data from front of each batch'''\n","    '''to fit the model we remove first 3 slices of each batch'''\n","    '''shape of a is [batch_size,depth,width,height,channels]'''\n","    out_send = out_arr[:,:,:,:,:]\n","    #print(\"arr\",np.shape(out_arr),\"send\",np.shape(out_send))\n","    return (out_send)\n","\n","def random_rotate(in_image,in_label):\n","    \n","    check = np.random.random(1)[0]\n","    if check<0.25:\n","        out_image = in_image[:,:,::-1,:,:]\n","        out_lab = in_label[:,:,::-1,:,:]\n","        \n","    elif check<0.50:\n","        \n","        out_image = in_image[:,:,:,::-1,:]\n","        out_lab = in_label[:,:,:,::-1,:]\n","        \n","    elif check<0.75:\n","        \n","        out_image = in_image[:,::-1,:,:,:]\n","        out_lab = in_label[:,::-1,:,:,:]\n","        \n","    else:\n","        out_image =in_image\n","        out_lab = in_label\n","    \n","    return (out_image,out_lab)\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","    '''Function to take batches randomly'''\n","    global out_img\n","    global out_label\n","    \n","    '''training info'''\n","    train_info = 380  #100 patients with 155 images each\n","\n","    if current_batch_no == 0:\n","        no_of_batches = train_info//batch_size  \n","        permute_mat = np.random.permutation(no_of_batches)\n","    \n","    start = permute_mat[current_batch_no]*batch_size*155\n","    end = start + (batch_size*155)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    current_batch_no += 1\n","    #print(len(out_img))\n","    '''Input normalization'''\n","    normalizing_input()\n","    '''normalization oof labels'''\n","    #out_label = out_label\n","    #normalizing_label()\n","    '''converting multi class to dual class'''\n","    #out_label = convert_dual_class(out_label)\n","    '''cropping image and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label)\n","    '''Rolling axes'''\n","    #out_img_send = np.rollaxis(crop_out_image,2, 0)\n","    '''hot encoding'''\n","    #out_label_send = crop_out_label\n","    '''3D conversion'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    '''Data augmentation rotation'''\n","    #out_img_send,out_label_send = random_rotate(out_img_send,out_label_send)\n","     \n","    last=0\n","    if current_batch_no == len(permute_mat):\n","        last=1\n","    \n","    return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","def test_batch():\n","    '''Function to take next test batch''' \n","    global out_img\n","    global out_label\n","    \n","    '''training and testing info'''\n","    train_info = 380  #100 patients with 155 images each\n","    test_info = 484-train_info  #100 patients with 155 images each\n","    \n","    no_of_batches = test_info//batch_size  \n","    permute_mat = np.random.permutation(no_of_batches)\n","    start = (permute_mat[0]*batch_size*155) +(train_info*155)\n","    end = start + (155*batch_size)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","    '''normalization'''\n","    normalizing_input()\n","    #normalizing_label()\n","    '''croping images and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label) \n","    '''3D processing'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    \n","    return (out_img_send,out_label_send)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-NTV_zXbaY7","colab_type":"code","colab":{}},"source":["\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","    '''function for hot encoding images'''\n","    a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","    b = tf.transpose(a,perm=[0,1,2,3,5,4])\n","    return b\n","\n","#############################################################################################################################\n","#GENERALIZED DICE LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_dice_coeff(y_true, y_pred):\n","    Ncl = y_pred.shape[-1]\n","    print(Ncl)\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    print(np.shape(w))\n","    w = 1/((w**2))\n","    print(np.shape(w))\n","    # Compute gen dice coef:\n","    numerator = tf.reduce_sum(y_true*y_pred, axis=[0,1,2,3])\n","    denominator = tf.reduce_sum(y_true+y_pred, axis=[0,1,2,3])\n","    num=den=0\n","    for i in range(np.shape(w)[0]):\n","        num += w[i]*numerator[i]\n","        den += w[i]*denominator[i]\n","            \n","    #num = tf.reduce_sum(a)\n","    #den = tf.reduce_sum(b)\n","    num = num + 0.000000001\n","    den = den + 0.000000001\n","\n","    gen_dice_coef = tf.identity(tf.divide((2*num),den),name='dice_coeff')\n","\n","    return (gen_dice_coef)\n","#############################################################################################################################\n","#GENERALIZED FOCAL LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n","    \n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    w = 1/((w))\n","    y_pred = y_pred + 0.000000001                                               #to ensure that logarithm in next step doenst give math error\n","    \n","    ce = tf.multiply(y_true, -tf.log(y_pred))                                   #cross entropy (multiclass)\n","    fl_var = tf.multiply(y_true, tf.pow(tf.subtract(1., y_pred), gamma))        #focal loss variables (gamma*(1-pt)*(graund_truth))\n","    #fl = tf.multiply(alpha, tf.multiply(fl_var, ce))\n","    fl = tf.multiply(fl_var, ce)\n","    #fl=ce\n","    final = tf.reduce_sum(fl, axis=[0,1,2,3])\n","    normalized_focal = 0\n","    \n","    for i in range(np.shape(w)[0]):\n","        #print(i)\n","        #a = w[i]#/tf.reduce_sum(w)\n","        #b = fl[:,:,:,:,i]\n","        #c = (b)/(tf.reduce_max(b)+1)\n","        #normalized_focal += tf.reduce_sum(a*b)\n","        #b += w[i]*denominator[:,:,:,i]\n","        normalized_focal += w[i]*final[i]\n","    weighted_focal = tf.divide(normalized_focal,4.0,name='focal_loss')\n","    return (weighted_focal)\n","#############################################################################################################################\n","#LOSS FUNCTION\n","#############################################################################################################################\n","def hybrid_loss(y_true, y_pred, gamma=0.5):\n","    dice = generalized_dice_coeff(y_true, y_pred)\n","    focal = generalized_focal_loss(y_true, y_pred)\n","    #update = (gamma*dice) + ((1-gamma)*focal)\n","    update = (gamma*(1-dice))\n","    final_loss = tf.add(update,((1-gamma)*focal),name='final_loss')\n","    return final_loss,dice,focal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"source":["INITIALIZING THE MODEL FILTERS"]},{"cell_type":"markdown","metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"source":["DEFINING THE MODEL STRUCTURE"]},{"cell_type":"code","metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"source":["def predict_model1(X):\n","    '''Function to define the UNET model'''\n","    '''MODEL1 Filter definition'''\n","    '''LEFT'''\n","    \n","    filter1 = left_filter_def(3,4,8,name='filter1')\n","    filter2 = left_filter_def(3,8,16,name='filter2')\n","    pre_resnet_filter1 = left_filter_def(1,16,8,name='pre_resnet_filter1')\n","    resnet_filter1 = left_filter_def(3,8,8,name='pre_resnet_filter1')\n","    \n","    filter3 = left_filter_def(3,8,16,name='filter3')\n","    filter4 = left_filter_def(3,16,32,name='filter4')\n","    pre_resnet_filter2 = left_filter_def(1,32,16,name='pre_resnet_filter2')\n","    resnet_filter2 = left_filter_def(3,16,16,name='pre_resnet_filter2')\n","    \n","    \n","    filter5 = left_filter_def(3,16,32,name='filter5')\n","    filter6 = left_filter_def(3,32,64,name='filter6')\n","    pre_resnet_filter3 = left_filter_def(1,64,32,name='pre_resnet_filter3')\n","    resnet_filter3 = left_filter_def(3,32,32,name='pre_resnet_filter3')\n","    \n","    \n","    filter7 = left_filter_def(3,32,64,name='filter7')\n","    filter8 = left_filter_def(3,64,128,name='filter8')\n","    pre_resnet_filter4 = left_filter_def(1,128,64,name='pre_resnet_filter4')\n","    resnet_filter4 = left_filter_def(3,64,64,name='pre_resnet_filter5')\n","    \n","    \n","    filter9 = left_filter_def(3,64,128,name='filter9')\n","    filter10= left_filter_def(3,128,(128*2),name='filter10')\n","    pre_resnet_filter5 = left_filter_def(1,(2*128),128,name='pre_resnet_filter5')\n","    resnet_filter5 = left_filter_def(3,128,128,name='pre_resnet_filter5')\n","    \n","    \n","    \n","    '''last_up'''\n","    lastup_deconv_filter1 = right_filter_def(3,128,64,name='lastup_deconv_filter1')\n","    lastup_deconv_filter2 = right_filter_def(3,64,32,name='lastup_deconv_filter2')\n","    lastup_conv_filter1 = left_filter_def(3,32,64,name='lastup_conv_filter1')\n","    lastup_conv_filter2 = left_filter_def(3,64,32,name='lastup_conv_filter2')\n","    lastup_deconv_filter3 = right_filter_def(3,32,16,name='lastup_deconv_filter1')\n","    lastup_deconv_filter4 = right_filter_def(3,16,8,name='lastup_deconv_filter2')\n","    '''mid_up'''\n","    midup_deconv_filter1 = right_filter_def(3,32,16,name='midup_deconv_filter1')\n","    midup_conv_filter1 = left_filter_def(3,16,16,name='midup_conv_filter1')\n","    midup_conv_filter2 = left_filter_def(3,16,16,name='midup_conv_filter2')\n","    midup_deconv_filter2 = right_filter_def(3,16,8,name='midup_deconv_filter2')\n","    '''final_meet'''\n","    filter11 = left_filter_def(1,(8*3),8,name='filter11')\n","    pre_resnet_filter6 = left_filter_def(3,8,8,name='pre_resnet_filter6')\n","    resnet_filter6 = left_filter_def(3,8,8,name='pre_resnet_filter6')\n","    filter12 = left_filter_def(1,8,4,name='filter12')\n","    \n","    \n","    \n","    with tf.name_scope(\"BLOCK1\"):\n","        '''BLOCK1'''\n","        CNN1 = Conv_layer(X,filter1,stride=1,activation='relu',name='CNN1')\n","        #print (\"CNN1\",np.shape(CNN1))\n","        CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='CNN2')\n","        #print (\"CNN2\",np.shape(CNN2))\n","        pre_CNN1 = Conv_layer(CNN2,pre_resnet_filter1,stride=1,activation='relu',name='pre_resnet_filter1CNN2')\n","        resnet_CNN1 = tf.add(Conv_layer(pre_CNN1,resnet_filter1,stride=1,activation='relu'),CNN1,name='resnet_filter1CNN2')\n","        batch_norm1 = tf.layers.batch_normalization(resnet_CNN1, training=training, momentum=0.9, name=\"batch_norm\")\n","        pool1 = tf.nn.max_pool3d(resnet_CNN1,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","        '''BLOCK2'''\n","        CNN3 = Conv_layer(pool1,filter3,stride=1,activation='relu',name='CNN3')\n","        #print (\"CNN3\",np.shape(CNN3))\n","        CNN4 = Conv_layer(CNN3,filter4,stride=1,activation='relu',name='CNN4')\n","        #print (\"CNN4\",np.shape(CNN4))\n","        pre_CNN2 = Conv_layer(CNN4,pre_resnet_filter2,stride=1,activation='relu',name='pre_resnet_filter2CNN2')\n","        resnet_CNN2 = tf.add(Conv_layer(pre_CNN2,resnet_filter2,stride=1,activation='relu'),CNN3,name='resnet_filter2CNN2') \n","\n","        pool2 = tf.nn.max_pool3d(resnet_CNN2,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL2')\n","    \n","    with tf.name_scope(\"BLOCK3\"):\n","        '''BLOCK3'''\n","        CNN5 = Conv_layer(pool2,filter5,stride=1,activation='relu',name='CNN5')\n","        #print (\"CNN5\",np.shape(CNN5))\n","        CNN6 = Conv_layer(CNN5,filter6,stride=1,activation='relu',name='CNN6')\n","        #print (\"CNN6\",np.shape(CNN6))\n","        pre_CNN3 = Conv_layer(CNN6,pre_resnet_filter3,stride=1,activation='relu',name='pre_resnet_filter3CNN2')\n","        resnet_CNN3 = tf.add(Conv_layer(pre_CNN3,resnet_filter3,stride=1,activation='relu'),CNN5,name='resnet_filter3CNN2')\n","        pool3 = tf.nn.max_pool3d(resnet_CNN3,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL3')\n","    \n","    with tf.name_scope(\"BLOCK4\"):\n","        '''BLOCK4'''\n","        CNN7 = Conv_layer(pool3,filter7,stride=1,activation='relu',name='CNN7')\n","        #print (\"CNN7\",np.shape(CNN7))\n","        CNN8 = Conv_layer(CNN7,filter8,stride=1,activation='relu',name='CNN8')\n","        #print (\"CNN8\",np.shape(CNN8))\n","        pre_CNN4 = Conv_layer(CNN8,pre_resnet_filter4,stride=1,activation='relu',name='pre_resnet_filter4CNN2')\n","        resnet_CNN4 = tf.add(Conv_layer(pre_CNN4,resnet_filter4,stride=1,activation='relu'),CNN7,name='resnet_filter4CNN2')\n","\n","        pool4 = tf.nn.max_pool3d(resnet_CNN4,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL4')\n","    \n","    with tf.name_scope(\"BLOCK5\"):\n","        '''BLOCK5'''\n","        CNN9 = Conv_layer(pool4,filter9,stride=1,activation='relu',name='CNN9')\n","        #print (\"CNN9\",np.shape(CNN9))\n","        CNN10 = Conv_layer(CNN9,filter10,stride=1,activation='relu',name='CNN10')\n","        pre_CNN5 = Conv_layer(CNN10,pre_resnet_filter5,stride=1,activation='relu',name='pre_resnet_filter5CNN2')\n","        resnet_CNN5 = tf.add(Conv_layer(pre_CNN5,resnet_filter5,stride=1,activation='relu'),CNN9,name='resnet_filter5CNN2')\n","        \n","        #print (\"CNN10\",np.shape(CNN10))\n","        \n","    with tf.name_scope(\"mid_up\"):\n","        \n","        '''DECONVOLUTION'''\n","        midup_deconv1 = Deconv_layer(resnet_CNN3,midup_deconv_filter1,stride=2,activation='relu',name='midup_DE_CONV1')\n","        '''MID_RESNET'''\n","        Mid_resnet1 = Conv_layer(midup_deconv1,midup_conv_filter1,stride=1,activation='relu',name='mid_resnet1')\n","        Mid_resnet2 = tf.add(Conv_layer(Mid_resnet1,midup_conv_filter2,stride=1,activation='relu',name='mid_resnet2'),midup_deconv1)\n","        '''Deconvolution'''\n","        midup_deconv2 = Deconv_layer(Mid_resnet2,midup_deconv_filter2,stride=2,activation='relu',name='midup_DE_CONV2')\n","        \n","        \n","    with tf.name_scope(\"last_up\"):\n","        \n","        '''DECONVOLUTION'''\n","        lastup_deconv1 = Deconv_layer(resnet_CNN5,lastup_deconv_filter1,stride=2,activation='relu',name='lastup_DE_CONV1')\n","        lastup_deconv2 = Deconv_layer(lastup_deconv1,lastup_deconv_filter2,stride=2,activation='relu',name='lastup_DE_CONV2')\n","        '''MID_RESNET'''\n","        Mid_resnet3 = Conv_layer(lastup_deconv2,lastup_conv_filter1,stride=1,activation='relu',name='mid_resnet1')\n","        Mid_resnet4 = tf.add(Conv_layer(Mid_resnet3,lastup_conv_filter2,stride=1,activation='relu',name='mid_resnet2'),lastup_deconv2)\n","        \n","        lastup_deconv3 = Deconv_layer(Mid_resnet4,lastup_deconv_filter3,stride=2,activation='relu',name='lastup_DE_CONV3')\n","        lastup_deconv4 = Deconv_layer(lastup_deconv3,lastup_deconv_filter4,stride=2,activation='relu',name='lastup_DE_CONV4')\n","        \n","        \n","    with tf.name_scope(\"final_layer\"):\n","        concat1 = tf.concat([lastup_deconv4,midup_deconv2,resnet_CNN1],axis=4,name='CONCAT4')\n","        CNN_depth = Conv_layer(concat1,filter11,stride=1,activation='relu',name='CNN_depth')\n","        batch_norm = tf.layers.batch_normalization(CNN_depth, training=training, momentum=0.9, name=\"batch_norm\")\n","        pre_CNN6 = Conv_layer(batch_norm,pre_resnet_filter6,stride=1,activation='relu',name='pre_resnet_filter6')\n","        resnet_CNN6 = tf.add(Conv_layer(pre_CNN6,resnet_filter6,stride=1,activation='relu'),batch_norm,name='resnet_filter6')\n","        final_CNN = Conv_layer(resnet_CNN6,filter12,stride=1,activation='softmax',name='CNN_depth2')\n","        \n","    return(final_CNN)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VWIc00uslsC","colab_type":"code","colab":{}},"source":["def chose_train_restore(n_epochs = 100):\n","    output_dir = \"/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/Wnet_dice_focal/saving_model\"\n","    model_checkpoint_file_base = os.path.join(output_dir, \"model.ckpt\")\n","\n","    \n","    if not os.path.exists(model_checkpoint_file_base + \".meta\"):\n","        '''FIRST TIME TRAINING'''\n","        print(\"Making new\")\n","        brand_new = True\n","        \n","        prediction = predict_model1(X)#logits\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"LOSS_FUNCTION\"):\n","            '''using multi dimensional dice'''\n","            hot_y = hot_encode(y)\n","            #dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","            #dice = generalized_dice_coeff(hot_y[:,:,:,:,0], prediction)\n","            #focal = generalized_focal_loss(hot_y[:,:,:,:,0], prediction)\n","            #hybrid,dice,focal,sep = find_hybrid_loss(hot_y[:,:,:,:,:,0], prediction, gamma=0.9,alpha=0.8)\n","            #mean_error = ddice_coeffiff_error(hot_y[:,:,:,:,0], prediction)\n","            main_loss,dice,focal= hybrid_loss(hot_y[:,:,:,:,:,0], prediction,gamma=gamma)\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"COST_FUNCTION\"):\n","            '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","            loss = tf.reduce_mean(main_loss, name=\"loss\")\n","\n","        saver = tf.train.Saver()\n","        \n","    else:\n","        '''RESTORED MODEL'''\n","        print(\"Reloading existing\")\n","        brand_new = False\n","        saver = tf.train.import_meta_graph(model_checkpoint_file_base + \".meta\")\n","        g = tf.get_default_graph()\n","        \n","        #mean_error = g.get_tensor_by_name(\"LOSS_FUNCTION/mse_loss:0\")\n","        dice = g.get_tensor_by_name(\"LOSS_FUNCTION/dice_coeff:0\")\n","        focal = g.get_tensor_by_name(\"LOSS_FUNCTION/focal_loss:0\")\n","        #hybrid = g.get_tensor_by_name(\"LOSS_FUNCTION/hybrid_loss:0\")\n","        main_loss = g.get_tensor_by_name(\"LOSS_FUNCTION/final_loss:0\")\n","        prediction = g.get_tensor_by_name(\"BLOCK9/Softmax:0\") \n","        loss = g.get_tensor_by_name(\"COST_FUNCTION/loss:0\")\n","        \n","        #X = g.get_tensor_by_name(\"input_image:0\")\n","        #y = g.get_tensor_by_name(\"hot_encode_label:0\")\n","        #training = g.get_tensor_by_name(\"training:0\")\n","        #gamma = g.get_tensor_by_name(\"dice_weight:0\")\n","        #alpha = g.get_tensor_by_name(\"mse_focal_weight:0\")\n","        #learning_rate = g.get_tensor_by_name(\"learning_rate:0\")\n","        #gamma = tf.placeholder(shape=[], dtype=tf.float32, name=\"dice_weight\")\n","        ##alpha = tf.placeholder(shape=[], dtype=tf.float32, name=\"mse_focal_weight\")\n","        #learning_rate = tf.placeholder(shape=[], dtype=tf.float32, name=\"learning_rate\")\n","        \n","\n","    \n","    \n","    \n","    '''TRAINING'''\n","    '''starting session'''\n","    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n","    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_option)) as sess:\n","        '''Initializing optimizer'''\n","        if brand_new:\n","            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","            init = tf.global_variables_initializer()\n","            sess.run(init)\n","            tf.add_to_collection(\"optimizer\", optimizer)\n","        else:\n","            saver = tf.train.Saver()\n","            saver.restore(sess, model_checkpoint_file_base)\n","            optimizer = tf.get_collection(\"optimizer\")[0]\n","\n","        for epoch in range(19,n_epochs):\n","            current_batch_no = 0\n","            permute_mat = 0\n","            iteration = 0\n","            while(1):\n","                #with tf.device('/cpu:0'):\n","                epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat,)\n","                sess_results = sess.run(optimizer, feed_dict={X: epoch_x, y: epoch_y, gamma: gm, learning_rate: LR, training: True})\n","                #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","                \n","                '''DICE Coefficient for iteration'''\n","                #with tf.device('/cpu:0'):\n","                if iteration%10==0:\n","                    #acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","                    train_loss,train_dice,train_focal = sess.run([main_loss,dice,focal], feed_dict={X: epoch_x, y: epoch_y, gamma: gm, training: True})\n","                    test_images, test_labels = test_batch()\n","                    #acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","                    test_loss,test_dice,test_focal = sess.run([main_loss,dice,focal], feed_dict={X: test_images, y: test_labels, gamma: gm, training: True})\n","                    print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train Loss:\", train_loss, \"Train Dice Coeff\",train_dice,\"Train Focal Loss\",train_focal,\"Test Loss:\", test_loss, \"Test Dice Coeff\",test_dice,\"Test Focal Loss\",test_focal)\n","                    #print(\"After Epoch\", epoch+1, \"Hybrid Train accuracy:\", 1-hybrid_train, \"Dice Train accuracy:\", 1-dice_train, \"Focal Train accuracy:\", 1-focal_train, \"MSE Train accuracy:\", 1-diff_train)\n","                if last ==1:\n","                    break\n","                iteration +=1\n","            test_images, test_labels = test_batch()\n","            #hybrid_test,dice_test,focal_test,diff_test = sess.run([hybrid,dice,focal,sep], feed_dict={X: test_images, y: test_labels})\n","            #diff_test = sess.run(dice, feed_dict={X: test_images, y: test_labels})\n","            test_loss,test_dice,test_focal = sess.run([main_loss,dice,focal], feed_dict={X: test_images, y: test_labels, gamma: gm, training: False})\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            #print(\"After Epoch\", epoch+1, \"Hybrid Test accuracy:\", 1-hybrid_test, \"Dice Test accuracy:\", 1-dice_test, \"Focal Test accuracy:\", 1-focal_test, \"MSE Test accuracy:\", 1-diff_test)\n","            print(\"After Epoch\", epoch+1,\"Test Loss:\", test_loss, \"Test Dice Coeff\",test_dice,\"Test Focal Loss\",test_focal)\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            '''saving model after each epoch'''\n","            save_path = tf.train.Saver(max_to_keep=1).save(sess, model_checkpoint_file_base)\n","            \n","            if epoch % 1 == 0:\n","                test_example =   test_images\n","                test_example_gt = test_labels#np.rollaxis(test_labels,2,0)\n","                sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","                sess_results = sess_results[0,100,:,:,1] + (2*sess_results[0,100,:,:,2]) + (3*sess_results[0,100,:,:,3])\n","                test_example = test_example[0,100,:,:,3]\n","                test_example_gt = test_example_gt[0,100,:,:,:]\n","                \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Original Image')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/Wnet_dice_focal/result/'+str(epoch)+\"a_Original_Image.png\")\n","                 \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Ground Truth Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/Wnet_dice_focal/result/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","                plt.figure()\n","                plt.imshow(np.squeeze(sess_results),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Generated Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/Wnet_dice_focal/result/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","                plt.close('all')\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"cell_type":"code","metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"1f04649b-ce68-4a51-a7db-8d4c98aec352","executionInfo":{"status":"error","timestamp":1558293168946,"user_tz":-330,"elapsed":32322,"user":{"displayName":"dwijay shanbhag","photoUrl":"","userId":"10252205309947413859"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["chose_train_restore()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Making new\n","4\n","(4,)\n","(4,)\n","Minibatch at Epoch 20 batch 1 Train Loss: 0.965555 Train Dice Coeff 0.013632802 Train Focal Loss 0.778245 Test Loss: 0.9751781 Test Dice Coeff 0.0030004366 Test Focal Loss 0.7787851\n","Minibatch at Epoch 20 batch 11 Train Loss: 0.9693272 Train Dice Coeff 0.009349411 Train Focal Loss 0.7774169 Test Loss: 0.97646284 Test Dice Coeff 0.00158735 Test Focal Loss 0.7789143\n","Minibatch at Epoch 20 batch 21 Train Loss: 0.96259534 Train Dice Coeff 0.016786925 Train Focal Loss 0.7770361 Test Loss: 0.9687085 Test Dice Coeff 0.009851904 Test Focal Loss 0.77575237\n","Minibatch at Epoch 20 batch 31 Train Loss: 0.9700868 Train Dice Coeff 0.008099121 Train Focal Loss 0.77376044 Test Loss: 0.9737768 Test Dice Coeff 0.0042248084 Test Focal Loss 0.77579206\n","Minibatch at Epoch 20 batch 41 Train Loss: 0.9757374 Train Dice Coeff 9.895162e-06 Train Focal Loss 0.7574627 Test Loss: 0.9737044 Test Dice Coeff 0.003022599 Test Focal Loss 0.76424766\n","Minibatch at Epoch 20 batch 51 Train Loss: 0.96474737 Train Dice Coeff 0.009284812 Train Focal Loss 0.73103714 Test Loss: 0.9718527 Test Dice Coeff 0.0020982204 Test Focal Loss 0.7374111\n","Minibatch at Epoch 20 batch 61 Train Loss: 0.9640133 Train Dice Coeff 0.006662338 Train Focal Loss 0.7000933 Test Loss: 0.9724649 Test Dice Coeff 0.0003182162 Test Focal Loss 0.72751355\n","Minibatch at Epoch 20 batch 71 Train Loss: 0.95756507 Train Dice Coeff 0.011973736 Train Focal Loss 0.6834139 Test Loss: 0.96622187 Test Dice Coeff 0.0035353377 Test Focal Loss 0.694037\n","Minibatch at Epoch 20 batch 81 Train Loss: 0.9603657 Train Dice Coeff 0.007873264 Train Focal Loss 0.6745163 Test Loss: 0.9650494 Test Dice Coeff 0.00860868 Test Focal Loss 0.7279725\n","Minibatch at Epoch 20 batch 91 Train Loss: 0.9662024 Train Dice Coeff 0.0017994194 Train Focal Loss 0.6782185 Test Loss: 0.9687339 Test Dice Coeff 0.0017840418 Test Focal Loss 0.70339555\n","Minibatch at Epoch 20 batch 101 Train Loss: 0.96770275 Train Dice Coeff 0.00048782903 Train Focal Loss 0.68141794 Test Loss: 0.9643559 Test Dice Coeff 0.0045327023 Test Focal Loss 0.68435305\n","Minibatch at Epoch 20 batch 111 Train Loss: 0.96062684 Train Dice Coeff 0.0044952333 Train Focal Loss 0.6467261 Test Loss: 0.96355075 Test Dice Coeff 0.004135405 Test Focal Loss 0.6727264\n","Minibatch at Epoch 20 batch 121 Train Loss: 0.96795064 Train Dice Coeff 0.0002136291 Train Focal Loss 0.6814294 Test Loss: 0.96417814 Test Dice Coeff 0.0034066208 Test Focal Loss 0.6724416\n","Minibatch at Epoch 20 batch 131 Train Loss: 0.9612287 Train Dice Coeff 0.004123634 Train Focal Loss 0.6494005 Test Loss: 0.9640864 Test Dice Coeff 0.0037349134 Test Focal Loss 0.67447835\n","Minibatch at Epoch 20 batch 141 Train Loss: 0.96127474 Train Dice Coeff 0.0064230673 Train Focal Loss 0.6705551 Test Loss: 0.96354336 Test Dice Coeff 0.0034319996 Test Focal Loss 0.66632116\n","Minibatch at Epoch 20 batch 151 Train Loss: 0.96288145 Train Dice Coeff 0.0040563615 Train Focal Loss 0.66532147 Test Loss: 0.96083957 Test Dice Coeff 0.006667008 Test Focal Loss 0.66839904\n","Minibatch at Epoch 20 batch 161 Train Loss: 0.9592948 Train Dice Coeff 0.006151408 Train Focal Loss 0.6483112 Test Loss: 0.9626395 Test Dice Coeff 0.0036141018 Test Focal Loss 0.6589219\n","Minibatch at Epoch 20 batch 171 Train Loss: 0.9644647 Train Dice Coeff 0.0006573899 Train Focal Loss 0.6505637 Test Loss: 0.96513414 Test Dice Coeff 0.00013259683 Test Focal Loss 0.65253496\n","Minibatch at Epoch 20 batch 181 Train Loss: 0.95361924 Train Dice Coeff 0.012952283 Train Focal Loss 0.65276337 Test Loss: 0.9652423 Test Dice Coeff 0.0009508374 Test Focal Loss 0.6609808\n","Minibatch at Epoch 20 batch 191 Train Loss: 0.96594626 Train Dice Coeff 0.0014493049 Train Focal Loss 0.672506 Test Loss: 0.9641686 Test Dice Coeff 0.002356418 Test Focal Loss 0.66289395\n","Minibatch at Epoch 20 batch 201 Train Loss: 0.95984095 Train Dice Coeff 0.008719504 Train Focal Loss 0.6768849 Test Loss: 0.9575372 Test Dice Coeff 0.008866118 Test Focal Loss 0.6551671\n","Minibatch at Epoch 20 batch 211 Train Loss: 0.95658594 Train Dice Coeff 0.01023509 Train Focal Loss 0.65797496 Test Loss: 0.94982517 Test Dice Coeff 0.016145343 Test Focal Loss 0.64356035\n","Minibatch at Epoch 20 batch 221 Train Loss: 0.965749 Train Dice Coeff 5.9454593e-05 Train Focal Loss 0.6580249 Test Loss: 0.9637903 Test Dice Coeff 0.0007141852 Test Focal Loss 0.64433104\n","Minibatch at Epoch 20 batch 231 Train Loss: 0.969405 Train Dice Coeff 0.00040013267 Train Focal Loss 0.6976513 Test Loss: 0.9653584 Test Dice Coeff 0.0058748275 Test Focal Loss 0.7064571\n","Minibatch at Epoch 20 batch 241 Train Loss: 0.96173865 Train Dice Coeff 0.0028490096 Train Focal Loss 0.6430272 Test Loss: 0.95838016 Test Dice Coeff 0.0077697216 Test Focal Loss 0.65372926\n","Minibatch at Epoch 20 batch 251 Train Loss: 0.95533323 Train Dice Coeff 0.0076883207 Train Focal Loss 0.6225274 Test Loss: 0.9642349 Test Dice Coeff 0.0002161503 Test Focal Loss 0.64429367\n","Minibatch at Epoch 20 batch 261 Train Loss: 0.9290196 Train Dice Coeff 0.027886491 Train Focal Loss 0.5411746 Test Loss: 0.94717824 Test Dice Coeff 0.006752709 Test Focal Loss 0.53255755\n","Minibatch at Epoch 20 batch 271 Train Loss: 0.94157964 Train Dice Coeff 0.0043022977 Train Focal Loss 0.45451728 Test Loss: 0.9783821 Test Dice Coeff 0.027378943 Test Focal Loss 1.0302317\n","Minibatch at Epoch 20 batch 281 Train Loss: 0.81386304 Train Dice Coeff 0.1486634 Train Focal Loss 0.4766009 Test Loss: 0.81735665 Test Dice Coeff 0.1544529 Test Focal Loss 0.56364304\n","Minibatch at Epoch 20 batch 291 Train Loss: 0.9466289 Train Dice Coeff 2.402092e-10 Train Focal Loss 0.46628952 Test Loss: 0.90323746 Test Dice Coeff 0.052403443 Test Focal Loss 0.50400585\n","Minibatch at Epoch 20 batch 301 Train Loss: 0.92564267 Train Dice Coeff 0.04220627 Train Focal Loss 0.63628376 Test Loss: 0.77695745 Test Dice Coeff 0.18738297 Test Focal Loss 0.45602152\n","Minibatch at Epoch 20 batch 311 Train Loss: 0.6944162 Train Dice Coeff 0.2820768 Train Focal Loss 0.4828537 Test Loss: 0.827651 Test Dice Coeff 0.1397836 Test Focal Loss 0.53456277\n","Minibatch at Epoch 20 batch 321 Train Loss: 0.807555 Train Dice Coeff 0.18264112 Train Focal Loss 0.71932036 Test Loss: 0.74875325 Test Dice Coeff 0.26123878 Test Focal Loss 0.83868164\n","Minibatch at Epoch 20 batch 331 Train Loss: 0.7437399 Train Dice Coeff 0.24712923 Train Focal Loss 0.6615617 Test Loss: 0.8773778 Test Dice Coeff 0.0906621 Test Focal Loss 0.58973736\n","Minibatch at Epoch 20 batch 341 Train Loss: 0.7916432 Train Dice Coeff 0.18603076 Train Focal Loss 0.5907089 Test Loss: 0.8025226 Test Dice Coeff 0.2372703 Test Focal Loss 1.1606584\n","Minibatch at Epoch 20 batch 351 Train Loss: 0.93576753 Train Dice Coeff 1.0650805e-09 Train Focal Loss 0.35767543 Test Loss: 0.83728725 Test Dice Coeff 0.124741174 Test Focal Loss 0.49554336\n","Minibatch at Epoch 20 batch 361 Train Loss: 0.7861706 Train Dice Coeff 0.1802748 Train Focal Loss 0.48417914 Test Loss: 0.8505316 Test Dice Coeff 0.105588295 Test Focal Loss 0.45561087\n","Minibatch at Epoch 20 batch 371 Train Loss: 0.8253004 Train Dice Coeff 0.15590523 Train Focal Loss 0.6561512 Test Loss: 0.86185926 Test Dice Coeff 0.10424336 Test Focal Loss 0.55678296\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 20 Test Loss: 0.9626527 Test Dice Coeff 0.008513972 Test Focal Loss 0.7031528\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 21 batch 1 Train Loss: 0.87831587 Train Dice Coeff 0.0930525 Train Focal Loss 0.6206312 Test Loss: 0.7550859 Test Dice Coeff 0.2486088 Test Focal Loss 0.7883384\n","Minibatch at Epoch 21 batch 11 Train Loss: 0.68736804 Train Dice Coeff 0.28960037 Train Focal Loss 0.48008353 Test Loss: 0.779427 Test Dice Coeff 0.19470346 Test Focal Loss 0.54660136\n","Minibatch at Epoch 21 batch 21 Train Loss: 0.7835541 Train Dice Coeff 0.18859959 Train Focal Loss 0.53293693 Test Loss: 0.95408607 Test Dice Coeff 0.0027270354 Test Focal Loss 0.5654043\n","Minibatch at Epoch 21 batch 31 Train Loss: 0.90745497 Train Dice Coeff 0.03699767 Train Focal Loss 0.40752888 Test Loss: 0.9071832 Test Dice Coeff 0.038260993 Test Focal Loss 0.416181\n","Minibatch at Epoch 21 batch 41 Train Loss: 0.94673604 Train Dice Coeff 0.13394603 Train Focal Loss 1.6728749 Test Loss: 0.8684117 Test Dice Coeff 0.07990876 Test Focal Loss 0.40329546\n","Minibatch at Epoch 21 batch 51 Train Loss: 0.85017425 Train Dice Coeff 0.10250477 Train Focal Loss 0.42428523 Test Loss: 0.85540986 Test Dice Coeff 0.096544005 Test Focal Loss 0.42299476\n","Minibatch at Epoch 21 batch 61 Train Loss: 0.795926 Train Dice Coeff 0.15977255 Train Focal Loss 0.39721233 Test Loss: 0.8930105 Test Dice Coeff 0.061849393 Test Focal Loss 0.48675\n","Minibatch at Epoch 21 batch 71 Train Loss: 0.7973475 Train Dice Coeff 0.15674771 Train Focal Loss 0.38420403 Test Loss: 0.77500266 Test Dice Coeff 0.1823257 Test Focal Loss 0.3909583\n","Minibatch at Epoch 21 batch 81 Train Loss: 0.7888131 Train Dice Coeff 0.17882496 Train Focal Loss 0.49755546 Test Loss: 0.75654393 Test Dice Coeff 0.21884589 Test Focal Loss 0.53505236\n","Minibatch at Epoch 21 batch 91 Train Loss: 0.80692405 Train Dice Coeff 0.18401779 Train Focal Loss 0.7254001 Test Loss: 0.738605 Test Dice Coeff 0.25097847 Test Focal Loss 0.64485675\n","Minibatch at Epoch 21 batch 101 Train Loss: 0.7307611 Train Dice Coeff 0.2878038 Train Focal Loss 0.8978448 Test Loss: 0.88401103 Test Dice Coeff 0.067101665 Test Focal Loss 0.444025\n","Minibatch at Epoch 21 batch 111 Train Loss: 0.7853231 Train Dice Coeff 0.19770803 Train Focal Loss 0.6326028 Test Loss: 0.7719095 Test Dice Coeff 0.19368637 Test Focal Loss 0.4622719\n","Minibatch at Epoch 21 batch 121 Train Loss: 0.8851571 Train Dice Coeff 0.13214041 Train Focal Loss 1.0408344 Test Loss: 0.7634028 Test Dice Coeff 0.2707693 Test Focal Loss 1.0709515\n","Minibatch at Epoch 21 batch 131 Train Loss: 0.88410413 Train Dice Coeff 0.060952324 Train Focal Loss 0.38961205 Test Loss: 0.88727957 Test Dice Coeff 0.048965015 Test Focal Loss 0.31348103\n","Minibatch at Epoch 21 batch 141 Train Loss: 0.8496761 Train Dice Coeff 0.1630461 Train Focal Loss 0.9641757 Test Loss: 0.9577149 Test Dice Coeff 0.0024130319 Test Focal Loss 0.5988666\n","Minibatch at Epoch 21 batch 151 Train Loss: 1.1174779 Train Dice Coeff 0.1289678 Train Focal Loss 3.3354883 Test Loss: 0.7036864 Test Dice Coeff 0.2772215 Test Focal Loss 0.5318574\n","Minibatch at Epoch 21 batch 161 Train Loss: 0.7108647 Train Dice Coeff 0.24720784 Train Focal Loss 0.33351785 Test Loss: 0.9125595 Test Dice Coeff 0.02354246 Test Focal Loss 0.33747715\n","Minibatch at Epoch 21 batch 171 Train Loss: 0.780585 Train Dice Coeff 0.29724237 Train Focal Loss 1.4810313 Test Loss: 0.62666875 Test Dice Coeff 0.36398607 Test Focal Loss 0.5425623\n","Minibatch at Epoch 21 batch 181 Train Loss: 0.9533121 Train Dice Coeff 0.008569347 Train Focal Loss 0.61024547 Test Loss: 0.8748177 Test Dice Coeff 0.07725798 Test Focal Loss 0.44349977\n","Minibatch at Epoch 21 batch 191 Train Loss: 0.7000478 Train Dice Coeff 0.32798 Train Focal Loss 0.9522983 Test Loss: 0.8534225 Test Dice Coeff 0.09487746 Test Focal Loss 0.38812286\n","Minibatch at Epoch 21 batch 201 Train Loss: 0.8992278 Train Dice Coeff 0.06489215 Train Focal Loss 0.57630765 Test Loss: 0.71955985 Test Dice Coeff 0.23189862 Test Focal Loss 0.28268656\n","Minibatch at Epoch 21 batch 211 Train Loss: 0.6723627 Train Dice Coeff 0.2970819 Train Focal Loss 0.3973639 Test Loss: 0.6510071 Test Dice Coeff 0.3181149 Test Focal Loss 0.37310517\n","Minibatch at Epoch 21 batch 221 Train Loss: 0.82357323 Train Dice Coeff 0.13175693 Train Focal Loss 0.4215446 Test Loss: 0.72112095 Test Dice Coeff 0.2286624 Test Focal Loss 0.26917073\n","Minibatch at Epoch 21 batch 231 Train Loss: 0.82640254 Train Dice Coeff 0.15552106 Train Focal Loss 0.66371465 Test Loss: 0.78331745 Test Dice Coeff 0.17265908 Test Focal Loss 0.3871067\n","Minibatch at Epoch 21 batch 241 Train Loss: 0.903621 Train Dice Coeff 0.04406822 Train Focal Loss 0.43282413 Test Loss: 0.87677056 Test Dice Coeff 0.09288658 Test Focal Loss 0.6036849\n","Minibatch at Epoch 21 batch 251 Train Loss: 0.7877284 Train Dice Coeff 0.198934 Train Focal Loss 0.66769075 Test Loss: 0.9577327 Test Dice Coeff 0.006906437 Test Focal Loss 0.63948464\n","Minibatch at Epoch 21 batch 261 Train Loss: 0.796686 Train Dice Coeff 0.15964875 Train Focal Loss 0.4036994 Test Loss: 0.93381655 Test Dice Coeff 0.036931477 Test Focal Loss 0.6705488\n","Minibatch at Epoch 21 batch 271 Train Loss: 0.6008167 Train Dice Coeff 0.37963963 Train Focal Loss 0.42492348 Test Loss: 0.620802 Test Dice Coeff 0.38034654 Test Focal Loss 0.6311387\n","Minibatch at Epoch 21 batch 281 Train Loss: 0.60411507 Train Dice Coeff 0.38535863 Train Focal Loss 0.509378 Test Loss: 0.8016451 Test Dice Coeff 0.20788987 Test Focal Loss 0.88745975\n","Minibatch at Epoch 21 batch 291 Train Loss: 0.90316945 Train Dice Coeff 0.03154392 Train Focal Loss 0.31558993 Test Loss: 0.7664582 Test Dice Coeff 0.17975293 Test Focal Loss 0.2823593\n","Minibatch at Epoch 21 batch 301 Train Loss: 0.8107428 Train Dice Coeff 0.13414608 Train Focal Loss 0.31474298 Test Loss: 0.9338995 Test Dice Coeff 0.03668634 Test Focal Loss 0.6691725\n","Minibatch at Epoch 21 batch 311 Train Loss: 0.64925563 Train Dice Coeff 0.318985 Train Focal Loss 0.3634212 Test Loss: 0.6578579 Test Dice Coeff 0.31947798 Test Focal Loss 0.45388103\n","Minibatch at Epoch 21 batch 321 Train Loss: 0.8271051 Train Dice Coeff 0.13184334 Train Focal Loss 0.45764112 Test Loss: 0.68255305 Test Dice Coeff 0.29961413 Test Focal Loss 0.52205795\n","Minibatch at Epoch 21 batch 331 Train Loss: 0.72769684 Train Dice Coeff 0.23533769 Train Focal Loss 0.39500743 Test Loss: 0.8125075 Test Dice Coeff 0.13923237 Test Focal Loss 0.37816674\n","Minibatch at Epoch 21 batch 341 Train Loss: 0.6907247 Train Dice Coeff 0.30949935 Train Focal Loss 0.692741 Test Loss: 0.7657183 Test Dice Coeff 0.20933932 Test Focal Loss 0.5412369\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet_dice_focal_loss.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RIqV48zuBpwM","colab_type":"text"},"source":["\"\"\"\n","COMPLETELY NEW VERSION OF UNET DESIGNED FOR BRAIN TUMOUR SEGMENTATION\n","\n","SPECS:\n","\n","1. Input size  is 240x240x4 for each image\n","\n","2. BRATS dataset \n","\n","3. DICE LOSS IS TAKEN AS A LOSS FUNCTION\n","\n","4. MULTICLASS SEGMENTATION HAS BEEN IMPLEMENTED\n","\n","\n","DESC:\n","\n","HERE OUR PREDICTION WILL HAVE 4 DIMENSIONS(because we have 4 classes) FOR EACH IMAGE. THESE 4 PREDICTIONS are compared with hot encoded label(Ground truth)\n","THIS IN A WAY TRAINS THE SYSTEM TO HOT ENCODE THE PREDICTIONS TOO.\n","WE ARE TRYING TO IMPLEMENT THE ABOVE STATED MODEL TO IMPLEMENT MULTICLASS SEGEMENTATION. BUT HAD TO CHANGE SOME THINGS WHICH ARE STATED BELOW\n","\n","PROBLEMS AND CHANGES:\n","\n","1. Faced the problem of class imbalance. So in this version we multiply dice coefficient for each class with certain weight. this weight is reciprocal of the frequency of that class\n","\n","2. The problem of class imbalance still persists and dice coeff is more than 1. So i this version we have implemented a new dice coefficient function.\n","\n","\n","FUTURE:\n","\n","1. IMPROVING DICE COEFFICIENT\n","\n","2. 3D IMPLEMENTATION\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eSGVMeVbibw0","colab_type":"code","outputId":"2fe5f52e-6ad1-485a-af97-4620b0d4022a","executionInfo":{"status":"ok","timestamp":1558533997110,"user_tz":-330,"elapsed":1363,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["'''Mounting Google Drive on the Colab notebook'''\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hxotR3CiirGu","colab_type":"code","colab":{}},"source":["#file_image = '/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5'\n","import h5py\n","#dataset has data of 484 patients. (155 images of each patient)\n","#data is extracted using 4 different techniques\n","#size of data of 1 patient is [240,240,155,4]\n","#for 2D segmentation we stack in 3rd dimension (axis=2)\n","#train_image\n","image_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_image.hdf5\", \"r\")\n","#train_labels\n","label_store = h5py.File(\"/content/gdrive/My Drive/Brain_Tumour_segmentation/Train_label.hdf5\", \"r\")\n","train_images = image_store[\"image\"]\n","train_labels = label_store[\"label\"]\n","#print('hi')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIkjuDek_v5Q","colab_type":"code","colab":{}},"source":["'''IMPORTING LIBRARIES'''\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","import math\n","import os\n","'''Clearing tesorflow computation graph'''\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAFj-oXR_0Rs","colab_type":"code","colab":{}},"source":["'''DEFINING VARIABLES'''\n","\n","batch_size=1              #batch size taken at a time\n","n_class = 4                #number of classes in the label\n","\n","'''PLACEHOLDER for input and output of UNET'''\n","'''Here we crop the 155x240x240 image to  160x160x192 by repeating the last layer 5 times to convert 155 to 160'''\n","#X = tf.placeholder(shape=[None,160,160,192,4], dtype=tf.float32, name='input_image')\n","#y = tf.placeholder(shape=[None,160,160,192,1], dtype=tf.int64, name='hot_encode_label')\n","#training = tf.placeholder_with_default(False, shape = (), name = 'training')\n","\n","\n","'''Batch variable exraction from h5py file (used by functions 'rnadom_h5py_batch' and 'test_batch')'''\n","out_img = np.empty((240,240,batch_size*155,4),dtype=np.float32)\n","out_label = np.empty((240,240,batch_size*155,1),dtype=np.int64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"re3MWWJHLxuD","colab_type":"text"},"source":["DEFINING ALL THE REQUIRED FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"No2sukbcBnCm","colab_type":"code","colab":{}},"source":["'''COMPUTATION GRAPH Function Definitions'''\n","\n","\n","def left_filter_def(ker_size,in_chan,out_chan,name='left_filter'):\n","    '''Defining a filter variable to perform convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))           #HE initialization\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,in_chan,out_chan],stddev=stddev),name=name))\n","\n","\n","def right_filter_def(ker_size,in_chan,out_chan,name='right_filter'):\n","    '''Defining a filter variable to perform transpose convolution'''\n","    stddev = np.sqrt(4/(ker_size*ker_size*ker_size*ker_size*in_chan*out_chan))\n","    if stddev < 0.0008:\n","        stddev = 0.0008\n","    return (tf.Variable(tf.truncated_normal([ker_size,ker_size,ker_size,out_chan,in_chan],stddev=stddev),name=name))\n","\n","\n","def Conv_layer(input_im,filter_mask,stride,activation='None',name='conv'):\n","    '''Function to perform Convolution and apply activation filter'''\n","    '''Convolution'''\n","    conv = tf.nn.conv3d(input_im,filter_mask,strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","    \n","\n","def Deconv_layer(input_im,filter_mask,stride,activation='None',name='De_conv'):\n","    '''Function to perform Transpose Convolution and apply activation filter'''\n","    '''Transpose Convolution'''\n","    inp_shape = np.shape(input_im) #tf.shape()\n","    out_shape = [batch_size]+[int(inp_shape[1].value*2), int(inp_shape[2].value*2),int(inp_shape[3].value*2), int(inp_shape[4].value/2)]\n","    \n","    conv = tf.nn.conv3d_transpose(input_im, filter_mask, out_shape, strides = [1,stride,stride,stride,1], padding = \"SAME\",name=name)\n","    #norm_conv = tf.layers.batch_normalization(conv, training=training, momentum=0.9)\n","    '''Activation'''\n","    if activation == 'relu':\n","        return(tf.nn.relu(conv))\n","    elif activation == 'softmax':\n","        return(tf.nn.softmax(conv,axis=-1))\n","    elif activation == 'elu':\n","        return(tf.nn.elu(conv))\n","    else:\n","        #activation == 'None'\n","        return(conv)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2tyXeDFbpMy","colab_type":"code","colab":{}},"source":["'''Functions for batch Extraction and pre processing'''\n","\n","def normalizing_input():\n","    '''normalization of each input channels'''\n","    global out_img\n","    '''CHANNEL INFO'''\n","    # maximum value found using function called \"Finding_maximum_to_normalise \"\n","    #'''max value for dimension 4 is 5337.0'''\n","    #'''max value for dimension 3 is 11737.0'''\n","    #'''max value for dimension 2 is 9751.0'''\n","    #'''max value for dimension 1 is 6476.0'''\n","    out_img[:,:,:,0] = out_img[:,:,:,0]/6476.0\n","    out_img[:,:,:,1] = out_img[:,:,:,1]/9751.0\n","    out_img[:,:,:,2] = out_img[:,:,:,2]/11737.0\n","    out_img[:,:,:,3] = out_img[:,:,:,3]/5337.0\n","    \n","    \n","\n","def crop_image_fit_brain(in_image):\n","    '''cropping size was found using the code finding_brain.ipynb'''\n","    left = 19\n","    right= 210\n","    top  = 38\n","    bot  = 199\n","    out_image = in_image[top:(bot-1),left:(right+1),:,:]\n","    return (out_image)\n","\n","\n","def Pre_processing_3D(a):\n","    '''Function to roll axis to convert array into[depth,width,height,channels] and the divide it in batches'''\n","    #print(np.shape(a))\n","    b = np.rollaxis(a,2, 0)\n","    #print(np.shape(b))\n","    #image shape\n","    out_arr = np.empty(shape=[batch_size,160,np.shape(b)[1],np.shape(b)[2],np.shape(b)[3]])\n","    for i in range(batch_size):\n","        start = i*155\n","        end = start+155\n","        out_arr[i,0:155,:,:,:] = b[start:end,:,:,:]\n","    \n","    a = [out_arr[:,154:155,:,:,:]]*5\n","    out_arr[:,155:160,:,:,:] = a[0]\n","    \n","    '''clippig data from front of each batch'''\n","    '''to fit the model we remove first 3 slices of each batch'''\n","    '''shape of a is [batch_size,depth,width,height,channels]'''\n","    out_send = out_arr[:,:,:,:,:]\n","    #print(\"arr\",np.shape(out_arr),\"send\",np.shape(out_send))\n","    return (out_send)\n","\n","def random_rotate(in_image,in_label):\n","    \n","    check = np.random.random(1)[0]\n","    if check<0.25:\n","        out_image = in_image[:,:,::-1,:,:]\n","        out_lab = in_label[:,:,::-1,:,:]\n","        \n","    elif check<0.50:\n","        \n","        out_image = in_image[:,:,:,::-1,:]\n","        out_lab = in_label[:,:,:,::-1,:]\n","        \n","    elif check<0.75:\n","        \n","        out_image = in_image[:,::-1,:,:,:]\n","        out_lab = in_label[:,::-1,:,:,:]\n","        \n","    else:\n","        out_image =in_image\n","        out_lab = in_label\n","    \n","    return (out_image,out_lab)\n","\n","\n","\n","def random_h5py_batch(current_batch_no,permute_mat):\n","    '''Function to take batches randomly'''\n","    global out_img\n","    global out_label\n","    \n","    '''training info'''\n","    train_info = 380  #100 patients with 155 images each\n","\n","    if current_batch_no == 0:\n","        no_of_batches = train_info//batch_size  \n","        permute_mat = np.random.permutation(no_of_batches)\n","    \n","    start = permute_mat[current_batch_no]*batch_size*155\n","    end = start + (batch_size*155)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    current_batch_no += 1\n","    #print(len(out_img))\n","    '''Input normalization'''\n","    normalizing_input()\n","    '''normalization oof labels'''\n","    #out_label = out_label\n","    #normalizing_label()\n","    '''converting multi class to dual class'''\n","    #out_label = convert_dual_class(out_label)\n","    '''cropping image and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label)\n","    '''Rolling axes'''\n","    #out_img_send = np.rollaxis(crop_out_image,2, 0)\n","    '''hot encoding'''\n","    #out_label_send = crop_out_label\n","    '''3D conversion'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    '''Data augmentation rotation'''\n","    out_img_send,out_label_send = random_rotate(out_img_send,out_label_send)\n","     \n","    last=0\n","    if current_batch_no == len(permute_mat):\n","        last=1\n","    \n","    return (out_img_send,out_label_send,current_batch_no,permute_mat,last)\n","\n","def test_batch():\n","    '''Function to take next test batch''' \n","    global out_img\n","    global out_label\n","    \n","    '''training and testing info'''\n","    train_info = 380  #100 patients with 155 images each\n","    test_info = 484-train_info  #100 patients with 155 images each\n","    \n","    no_of_batches = test_info//batch_size  \n","    permute_mat = np.random.permutation(no_of_batches)\n","    start = (permute_mat[0]*batch_size*155) +(train_info*155)\n","    end = start + (155*batch_size)\n","    train_images.read_direct(out_img,np.s_[:,:,start:end,:])\n","    train_labels.read_direct(out_label,np.s_[:,:,start:end,:])\n","    \n","    '''normalization'''\n","    normalizing_input()\n","    #normalizing_label()\n","    '''croping images and labels'''\n","    crop_out_image = crop_image_fit_brain(out_img)\n","    crop_out_label = crop_image_fit_brain(out_label) \n","    '''3D processing'''\n","    out_img_send = Pre_processing_3D(crop_out_image)\n","    out_label_send = Pre_processing_3D(crop_out_label)\n","    \n","    return (out_img_send,out_label_send)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-NTV_zXbaY7","colab_type":"code","colab":{}},"source":["\n","  \n","def hot_encode(check_image,depth=n_class,name='hot_encode'):\n","    '''function for hot encoding images'''\n","    a = tf.one_hot(indices = check_image, depth=depth,name=name)\n","    b = tf.transpose(a,perm=[0,1,2,3,5,4])\n","    return b\n","\n","#############################################################################################################################\n","#GENERALIZED DICE LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_dice_coeff(y_true, y_pred):\n","    Ncl = y_pred.shape[-1]\n","    print(Ncl)\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    print(np.shape(w))\n","    w = 1/((w**2))\n","    print(np.shape(w))\n","    # Compute gen dice coef:\n","    numerator = tf.reduce_sum(y_true*y_pred, axis=[0,1,2,3])\n","    denominator = tf.reduce_sum(y_true+y_pred, axis=[0,1,2,3])\n","    num=den=0\n","    for i in range(np.shape(w)[0]):\n","        num += w[i]*numerator[i]\n","        den += w[i]*denominator[i]\n","            \n","    #num = tf.reduce_sum(a)\n","    #den = tf.reduce_sum(b)\n","    num = num + 0.000000001\n","    den = den + 0.000000001\n","\n","    gen_dice_coef = tf.identity(tf.divide((2*num),den),name='dice_coeff')\n","\n","    return (gen_dice_coef)\n","#############################################################################################################################\n","#GENERALIZED FOCAL LOSS FUNCTION\n","#############################################################################################################################\n","def generalized_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n","    \n","    Ncl = y_pred.shape[-1]\n","    w = np.zeros(shape=(Ncl,))\n","    w = tf.reduce_sum(y_true, axis=[0,1,2,3]) + 1\n","    w = 1/((w))\n","    y_pred = y_pred + 0.000000001                                               #to ensure that logarithm in next step doenst give math error\n","    \n","    ce = tf.multiply(y_true, -tf.log(y_pred))                                   #cross entropy (multiclass)\n","    fl_var = tf.multiply(y_true, tf.pow(tf.subtract(1., y_pred), gamma))        #focal loss variables (gamma*(1-pt)*(graund_truth))\n","    #fl = tf.multiply(alpha, tf.multiply(fl_var, ce))\n","    fl = tf.multiply(fl_var, ce)\n","    #fl=ce\n","    final = tf.reduce_sum(fl, axis=[0,1,2,3])\n","    normalized_focal = 0\n","    \n","    for i in range(np.shape(w)[0]):\n","        #print(i)\n","        #a = w[i]#/tf.reduce_sum(w)\n","        #b = fl[:,:,:,:,i]\n","        #c = (b)/(tf.reduce_max(b)+1)\n","        #normalized_focal += tf.reduce_sum(a*b)\n","        #b += w[i]*denominator[:,:,:,i]\n","        normalized_focal += w[i]*final[i]\n","    weighted_focal = tf.divide(normalized_focal,4.0,name='focal_loss')\n","    return (weighted_focal)\n","#############################################################################################################################\n","#LOSS FUNCTION\n","#############################################################################################################################\n","def hybrid_loss(y_true, y_pred, gamma=0.5):\n","    dice = generalized_dice_coeff(y_true, y_pred)\n","    focal = generalized_focal_loss(y_true, y_pred)\n","    #update = (gamma*dice) + ((1-gamma)*focal)\n","    update = (gamma*(1-dice))\n","    final_loss = tf.add(update,((1-gamma)*focal),name='final_loss')\n","    return final_loss,dice,focal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CQCK3RO4L3hl","colab_type":"text"},"source":["INITIALIZING THE MODEL FILTERS"]},{"cell_type":"markdown","metadata":{"id":"mf9Y3JjeMQnI","colab_type":"text"},"source":["DEFINING THE MODEL STRUCTURE"]},{"cell_type":"code","metadata":{"id":"3uIZEngOMVfS","colab_type":"code","colab":{}},"source":["def predict_model1(X):\n","    '''Function to define the UNET model'''\n","    '''MODEL1 Filter definition'''\n","    '''LEFT'''\n","    \n","    filter1 = left_filter_def(3,4,8,name='filter1')\n","    filter2 = left_filter_def(3,8,16,name='filter2')\n","    pre_resnet_filter1 = left_filter_def(1,16,8,name='pre_resnet_filter1')\n","    resnet_filter1 = left_filter_def(3,8,8,name='pre_resnet_filter1')\n","    \n","    filter3 = left_filter_def(3,8,16,name='filter3')\n","    filter4 = left_filter_def(3,16,32,name='filter4')\n","    pre_resnet_filter2 = left_filter_def(1,32,16,name='pre_resnet_filter2')\n","    resnet_filter2 = left_filter_def(3,16,16,name='pre_resnet_filter2')\n","    \n","    \n","    filter5 = left_filter_def(3,16,32,name='filter5')\n","    filter6 = left_filter_def(3,32,64,name='filter6')\n","    pre_resnet_filter3 = left_filter_def(1,64,32,name='pre_resnet_filter3')\n","    resnet_filter3 = left_filter_def(3,32,32,name='pre_resnet_filter3')\n","    \n","    \n","    filter7 = left_filter_def(3,32,64,name='filter7')\n","    filter8 = left_filter_def(3,64,128,name='filter8')\n","    pre_resnet_filter4 = left_filter_def(1,128,64,name='pre_resnet_filter4')\n","    resnet_filter4 = left_filter_def(3,64,64,name='pre_resnet_filter5')\n","    \n","    \n","    filter9 = left_filter_def(3,64,128,name='filter9')\n","    pre_resnet_filter5 = left_filter_def(3,128,128,name='pre_resnet_filter5')\n","    resnet_filter5 = left_filter_def(3,128,128,name='pre_resnet_filter5')\n","    filter10= left_filter_def(3,128,64,name='filter10')\n","    \n","    '''RIGHT'''\n","    \n","    filter11 = right_filter_def(3,128,64,name='filter11')\n","    filter12 = left_filter_def(3,64,64,name='filter12')\n","    resnet_filter6 = left_filter_def(3,64,64,name='pre_resnet_filter6')\n","    filter13 = left_filter_def(3,64,32,name='filter13')\n","    \n","    filter14 = right_filter_def(3,64,32,name='filter14')\n","    filter15 = left_filter_def(3,32,32,name='filter15')\n","    resnet_filter7 = left_filter_def(3,32,32,name='pre_resnet_filter7')\n","    filter16 = left_filter_def(3,32,16,name='filter16')\n","    \n","    filter17 = right_filter_def(3,32,16,name='filter17')\n","    filter18 = left_filter_def(3,16,16,name='filter18')\n","    resnet_filter8 = left_filter_def(3,16,16,name='pre_resnet_filter8')\n","    filter19 = left_filter_def(3,16,8,name='filter19')\n","    \n","    filter20 = right_filter_def(3,16,8,name='filter20')\n","    filter21 = left_filter_def(3,8,8,name='filter21')\n","    resnet_filter9 = left_filter_def(3,8,8,name='pre_resnet_filter8')\n","    filter22 = left_filter_def(3,8,n_class,name='filter22')\n","    \n","    \n","    with tf.name_scope(\"BLOCK1\"):\n","        '''BLOCK1'''\n","        CNN1 = Conv_layer(X,filter1,stride=1,activation='relu',name='CNN1')\n","        #print (\"CNN1\",np.shape(CNN1))\n","        CNN2 = Conv_layer(CNN1,filter2,stride=1,activation='relu',name='CNN2')\n","        #print (\"CNN2\",np.shape(CNN2))\n","        pre_CNN1 = Conv_layer(CNN2,pre_resnet_filter1,stride=1,activation='relu',name='pre_resnet_filter1CNN2')\n","        resnet_CNN1 = tf.add(Conv_layer(pre_CNN1,resnet_filter1,stride=1,activation='relu'),CNN1,name='resnet_filter1CNN2') \n","        pool1 = tf.nn.max_pool3d(resnet_CNN1,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL1')\n","    \n","    with tf.name_scope(\"BLOCK2\"):\n","        '''BLOCK2'''\n","        CNN3 = Conv_layer(pool1,filter3,stride=1,activation='relu',name='CNN3')\n","        #print (\"CNN3\",np.shape(CNN3))\n","        CNN4 = Conv_layer(CNN3,filter4,stride=1,activation='relu',name='CNN4')\n","        #print (\"CNN4\",np.shape(CNN4))\n","        pre_CNN2 = Conv_layer(CNN4,pre_resnet_filter2,stride=1,activation='relu',name='pre_resnet_filter2CNN2')\n","        resnet_CNN2 = tf.add(Conv_layer(pre_CNN2,resnet_filter2,stride=1,activation='relu'),CNN3,name='resnet_filter2CNN2') \n","\n","        pool2 = tf.nn.max_pool3d(resnet_CNN2,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL2')\n","    \n","    with tf.name_scope(\"BLOCK3\"):\n","        '''BLOCK3'''\n","        CNN5 = Conv_layer(pool2,filter5,stride=1,activation='relu',name='CNN5')\n","        #print (\"CNN5\",np.shape(CNN5))\n","        CNN6 = Conv_layer(CNN5,filter6,stride=1,activation='relu',name='CNN6')\n","        #print (\"CNN6\",np.shape(CNN6))\n","        pre_CNN3 = Conv_layer(CNN6,pre_resnet_filter3,stride=1,activation='relu',name='pre_resnet_filter3CNN2')\n","        resnet_CNN3 = tf.add(Conv_layer(pre_CNN3,resnet_filter3,stride=1,activation='relu'),CNN5,name='resnet_filter3CNN2')\n","        pool3 = tf.nn.max_pool3d(resnet_CNN3,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL3')\n","    \n","    with tf.name_scope(\"BLOCK4\"):\n","        '''BLOCK4'''\n","        CNN7 = Conv_layer(pool3,filter7,stride=1,activation='relu',name='CNN7')\n","        #print (\"CNN7\",np.shape(CNN7))\n","        CNN8 = Conv_layer(CNN7,filter8,stride=1,activation='relu',name='CNN8')\n","        #print (\"CNN8\",np.shape(CNN8))\n","        pre_CNN4 = Conv_layer(CNN8,pre_resnet_filter4,stride=1,activation='relu',name='pre_resnet_filter4CNN2')\n","        resnet_CNN4 = tf.add(Conv_layer(pre_CNN4,resnet_filter4,stride=1,activation='relu'),CNN7,name='resnet_filter4CNN2')\n","\n","        pool4 = tf.nn.max_pool3d(resnet_CNN4,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding='VALID',name='POOL4')\n","    \n","    with tf.name_scope(\"BLOCK5\"):\n","        '''BLOCK5'''\n","        CNN9 = Conv_layer(pool4,filter9,stride=1,activation='relu',name='CNN9')\n","        #print (\"CNN9\",np.shape(CNN9))\n","        pre_CNN5 = Conv_layer(CNN9,pre_resnet_filter5,stride=1,activation='relu',name='pre_resnet_filter5CNN2')\n","        resnet_CNN5 = tf.add(Conv_layer(pre_CNN5,resnet_filter5,stride=1,activation='relu'),CNN9,name='resnet_filter5CNN2')\n","        CNN10 = Conv_layer(resnet_CNN5,filter10,stride=1,activation='relu',name='CNN10')\n","        #print (\"CNN10\",np.shape(CNN10))\n","    \n","    '''Moving UP'''\n","    \n","    with tf.name_scope(\"BLOCK6\"):\n","        '''BLOCK6'''\n","        concat1 = tf.concat([CNN10,pool4],axis=4,name='CONCAT1')\n","        #print (\"concat1\",np.shape(concat1))\n","        DCNN1= Deconv_layer(concat1,filter11,stride=2,activation='relu',name='DE_CONV1')\n","        #print (\"DCNN1\",np.shape(DCNN1))\n","        CNN11 = Conv_layer(DCNN1,filter12,stride=1,activation='relu',name='CNN11')\n","        #print (\"CNN11\",np.shape(CNN11))\n","        resnet_CNN6 = tf.add(Conv_layer(CNN11,resnet_filter6,stride=1,activation='relu'),DCNN1,name='resnet_filter6CNN2')\n","        CNN12 = Conv_layer(resnet_CNN6,filter13,stride=1,activation='relu',name='CNN12')\n","        #print (\"CNN12\",np.shape(CNN12))\n","    \n","    with tf.name_scope(\"BLOCK7\"):\n","        '''BLOCK7'''\n","        concat2 = tf.concat([CNN12,pool3],axis=4,name='CONCAT2')\n","        #print (\"concat2\",np.shape(concat2))\n","        DCNN2= Deconv_layer(concat2,filter14,stride=2,activation='relu',name='DE_CONV2')\n","        #print (\"DCNN2\",np.shape(DCNN2))\n","        CNN13 = Conv_layer(DCNN2,filter15,stride=1,activation='relu',name='CNN13')\n","        #print (\"CNN13\",np.shape(CNN13))\n","        resnet_CNN7 = tf.add(Conv_layer(CNN13,resnet_filter7,stride=1,activation='relu'),DCNN2,name='resnet_filter7CNN2')\n","        CNN14 = Conv_layer(resnet_CNN7,filter16,stride=1,activation='relu',name='CNN14')\n","        #print (\"CNN14\",np.shape(CNN14))\n","    \n","    with tf.name_scope(\"BLOCK8\"):\n","        '''BLOCK8'''\n","        concat3 = tf.concat([CNN14,pool2],axis=4,name='CONCAT3')\n","        #print (\"concat3\",np.shape(concat3))\n","        DCNN3= Deconv_layer(concat3,filter17,stride=2,activation='relu',name='DE_CONV3')\n","        #print (\"DCNN3\",np.shape(DCNN3))\n","        CNN15 = Conv_layer(DCNN3,filter18,stride=1,activation='relu',name='CNN14')\n","        #print (\"CNN15\",np.shape(CNN15))\n","        resnet_CNN8 = tf.add(Conv_layer(CNN15,resnet_filter8,stride=1,activation='relu'),DCNN3,name='resnet_filter8CNN2')\n","        CNN16 = Conv_layer(resnet_CNN8,filter19,stride=1,activation='relu',name='CNN15')\n","        #print (\"CNN16\",np.shape(CNN16))\n","        \n","    with tf.name_scope(\"BLOCK9\"):\n","        '''BLOCK9'''\n","        concat4 = tf.concat([CNN16,pool1],axis=4,name='CONCAT4')\n","        #print (\"concat4\",np.shape(concat4))\n","        DCNN4= Deconv_layer(concat4,filter20,stride=2,activation='relu',name='DE_CONV4')\n","        #print (\"DCNN4\",np.shape(DCNN4))\n","        CNN17 = Conv_layer(DCNN4,filter21,stride=1,activation='relu',name='CNN17')\n","        #print (\"CNN17\",np.shape(CNN17))\n","        resnet_CNN9 = tf.add(Conv_layer(CNN17,resnet_filter9,stride=1,activation='relu'),DCNN4,name='resnet_filter9CNN2')\n","        CNN18 = Conv_layer(resnet_CNN9,filter22,stride=1,activation='softmax',name='CNN18')\n","        #print (\"CNN18\",np.shape(CNN18))\n","    return (CNN18)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VWIc00uslsC","colab_type":"code","colab":{}},"source":["def chose_train_restore(learning_rate =0.0001,n_epochs = 100):\n","    output_dir = \"/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/resnet_dice_focal_loss/saving_model\"\n","    model_checkpoint_file_base = os.path.join(output_dir, \"model.ckpt\")\n","\n","    \n","    if not os.path.exists(model_checkpoint_file_base + \".meta\"):\n","        '''FIRST TIME TRAINING'''\n","        print(\"Making new\")\n","        brand_new = True\n","        \n","        prediction = predict_model1(X)#logits\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"LOSS_FUNCTION\"):\n","            '''using multi dimensional dice'''\n","            hot_y = hot_encode(y)\n","            #dice = 1+ dice_coef_multilabel(hot_y,prediction)     #dice loss for verison 1\n","            #dice = generalized_dice_coeff(hot_y[:,:,:,:,0], prediction)\n","            #focal = generalized_focal_loss(hot_y[:,:,:,:,0], prediction)\n","            #hybrid,dice,focal,sep = find_hybrid_loss(hot_y[:,:,:,:,:,0], prediction, gamma=0.9,alpha=0.8)\n","            #mean_error = ddice_coeffiff_error(hot_y[:,:,:,:,0], prediction)\n","            main_loss,dice,focal = hybrid_loss(hot_y[:,:,:,:,:,0], prediction,gamma=0.9)\n","        '''----------------------------------------------------------------------------------------------------------------------------------------------------------------'''\n","        with tf.name_scope(\"COST_FUNCTION\"):\n","            '''Cost function''''''Remember to change max to min min to mx depending on loss function'''\n","            loss = tf.reduce_mean(main_loss, name=\"loss\")\n","\n","        saver = tf.train.Saver()\n","        \n","    else:\n","        '''RESTORED MODEL'''\n","        print(\"Reloading existing\")\n","        brand_new = False\n","        saver = tf.train.import_meta_graph(model_checkpoint_file_base + \".meta\")\n","        g = tf.get_default_graph()\n","        \n","        #sep = g.get_tensor_by_name(\"LOSS_FUNCTION/mse_loss:0\")\n","        dice = g.get_tensor_by_name(\"LOSS_FUNCTION/dice_coeff:0\")\n","        focal = g.get_tensor_by_name(\"LOSS_FUNCTION/focal_loss:0\")\n","        #hybrid = g.get_tensor_by_name(\"LOSS_FUNCTION/hybrid_loss:0\")\n","        main_loss = g.get_tensor_by_name(\"LOSS_FUNCTION/final_loss:0\")\n","        prediction = g.get_tensor_by_name(\"BLOCK9/Softmax:0\") \n","        loss = g.get_tensor_by_name(\"COST_FUNCTION/loss:0\")\n","        \n","        X = g.get_tensor_by_name(\"input_image:0\")\n","        y = g.get_tensor_by_name(\"hot_encode_label:0\")\n","\n","\n","    \n","    \n","    \n","    '''TRAINING'''\n","    '''starting session'''\n","    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n","    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_option)) as sess:\n","        '''Initializing optimizer'''\n","        if brand_new:\n","            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","            init = tf.global_variables_initializer()\n","            sess.run(init)\n","            tf.add_to_collection(\"optimizer\", optimizer)\n","        else:\n","            saver = tf.train.Saver()\n","            saver.restore(sess, model_checkpoint_file_base)\n","            optimizer = tf.get_collection(\"optimizer\")[0]\n","\n","        for epoch in range(72,n_epochs):\n","            current_batch_no = 0\n","            permute_mat = 0\n","            iteration = 0\n","            while(1):\n","                #with tf.device('/cpu:0'):\n","                epoch_x,epoch_y,current_batch_no,permute_mat,last = random_h5py_batch(current_batch_no,permute_mat)\n","                sess_results = sess.run(optimizer, feed_dict={X: epoch_x, y: epoch_y})\n","                #print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\n","                \n","                '''DICE Coefficient for iteration'''\n","                #with tf.device('/cpu:0'):\n","                if iteration%10==0:\n","                    #acc_train = 1-(dice.eval(feed_dict={X: epoch_x, y: epoch_y}))\n","                    train_loss,train_dice,train_focal = sess.run([main_loss,dice,focal], feed_dict={X: epoch_x, y: epoch_y})\n","                    test_images, test_labels = test_batch()\n","                    #acc_test = 1-(dice.eval(feed_dict={X: test_images, y: test_labels}))\n","                    test_loss,test_dice,test_focal = sess.run([main_loss,dice,focal], feed_dict={X: test_images, y: test_labels})\n","                    print(\"Minibatch at\",\"Epoch\", epoch+1,\"batch\",iteration+1, \"Train Loss:\", train_loss, \"Train Dice Coeff\",train_dice,\"Train Focal Loss\",train_focal,\"Test Loss:\", test_loss, \"Test Dice Coeff\",test_dice,\"Test Focal Loss\",test_focal)\n","                    #print(\"After Epoch\", epoch+1, \"Hybrid Train accuracy:\", 1-hybrid_train, \"Dice Train accuracy:\", 1-dice_train, \"Focal Train accuracy:\", 1-focal_train, \"MSE Train accuracy:\", 1-diff_train)\n","                if last ==1:\n","                    break\n","                iteration +=1\n","            test_images, test_labels = test_batch()\n","            #hybrid_test,dice_test,focal_test,diff_test = sess.run([hybrid,dice,focal,sep], feed_dict={X: test_images, y: test_labels})\n","            #diff_test = sess.run(dice, feed_dict={X: test_images, y: test_labels})\n","            test_loss,test_dice,test_focal = sess.run([main_loss,dice,focal], feed_dict={X: test_images, y: test_labels})\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            #print(\"After Epoch\", epoch+1, \"Hybrid Test accuracy:\", 1-hybrid_test, \"Dice Test accuracy:\", 1-dice_test, \"Focal Test accuracy:\", 1-focal_test, \"MSE Test accuracy:\", 1-diff_test)\n","            print(\"After Epoch\", epoch+1,\"Test Loss:\", test_loss, \"Test Dice Coeff\",test_dice,\"Test Focal Loss\",test_focal)\n","            print(\"-------------------------------------------------------------------------------------------------------\")\n","            '''saving model after each epoch'''\n","            save_path = tf.train.Saver(max_to_keep=1).save(sess, model_checkpoint_file_base)\n","            \n","            if epoch % 1 == 0:\n","                test_example =   test_images\n","                test_example_gt = test_labels#np.rollaxis(test_labels,2,0)\n","                sess_results = sess.run(prediction,feed_dict={X:test_example})\n","\n","                sess_results = sess_results[0,100,:,:,1] + (2*sess_results[0,100,:,:,2]) + (3*sess_results[0,100,:,:,3])\n","                test_example = test_example[0,100,:,:,3]\n","                test_example_gt = test_example_gt[0,100,:,:,:]\n","                \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Original Image')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/resnet_dice_focal_loss/result/'+str(epoch)+\"a_Original_Image.png\")\n","                 \n","                plt.figure()\n","                plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Ground Truth Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/resnet_dice_focal_loss/result/'+str(epoch)+\"b_Original_Mask.png\")\n","\n","                plt.figure()\n","                plt.imshow(np.squeeze(sess_results),cmap='gray')\n","                plt.axis('off')\n","                plt.title('Generated Mask')\n","                plt.savefig('/content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/resnet_dice_focal_loss/result/'+str(epoch)+\"c_Generated_Mask.png\")\n","\n","                plt.close('all')\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIDXJRhzqbNn","colab_type":"text"},"source":["COMMENTS ON DICE LOSS AND ACCURACY\n","\n","1. Under the name_scope \"LOSS FUNCTION\", the variable named dice corresponds to dice loss and since the function dice_multipleclass() returns a value between -1 and 0(both included) , we add 1. Also another reason for this is there is no maximize function in adam optimizer(or any other optimizing function).\n","\n","2. While printing  the accuracy (everywhere)  we have to print dice coefficient and not dice loss therefore we add 1 to the dice_loss calculation"]},{"cell_type":"code","metadata":{"id":"vLp7SRmzT6Jp","colab_type":"code","outputId":"d425ff25-a5b5-483f-cdf3-596618a491c2","executionInfo":{"status":"error","timestamp":1558544135409,"user_tz":-330,"elapsed":1465020,"user":{"displayName":"DWIJAY SHANBHAG","photoUrl":"","userId":"01321982494498435915"}},"colab":{"base_uri":"https://localhost:8080/","height":2402}},"source":["chose_train_restore()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Reloading existing\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Brain_Tumour_segmentation/all_in_one/resnet_dice_focal_loss/saving_model/model.ckpt\n","Minibatch at Epoch 73 batch 1 Train Loss: 0.74262625 Train Dice Coeff 0.22415051 Train Focal Loss 0.44361752 Test Loss: 0.66974235 Test Dice Coeff 0.31554064 Test Focal Loss 0.5372896\n","Minibatch at Epoch 73 batch 11 Train Loss: 0.78642267 Train Dice Coeff 0.19232987 Train Focal Loss 0.5951959 Test Loss: 0.7661655 Test Dice Coeff 0.1826221 Test Focal Loss 0.30525398\n","Minibatch at Epoch 73 batch 21 Train Loss: 0.9233629 Train Dice Coeff 2.1832724e-07 Train Focal Loss 0.23363148 Test Loss: 0.58332396 Test Dice Coeff 0.38665685 Test Focal Loss 0.3131516\n","Minibatch at Epoch 73 batch 31 Train Loss: 0.28408977 Train Dice Coeff 0.7322507 Train Focal Loss 0.431154 Test Loss: 0.9148621 Test Dice Coeff 0.009044876 Test Focal Loss 0.23002516\n","Minibatch at Epoch 73 batch 41 Train Loss: 0.91832423 Train Dice Coeff 3.2894545e-07 Train Focal Loss 0.18324569 Test Loss: 0.77279675 Test Dice Coeff 0.16768849 Test Focal Loss 0.2371642\n","Minibatch at Epoch 73 batch 51 Train Loss: 0.31855723 Train Dice Coeff 0.6935881 Train Focal Loss 0.427865 Test Loss: 0.40901485 Test Dice Coeff 0.57315814 Test Focal Loss 0.24857187\n","Minibatch at Epoch 73 batch 61 Train Loss: 0.30270457 Train Dice Coeff 0.7102735 Train Focal Loss 0.41950747 Test Loss: 0.53932846 Test Dice Coeff 0.4334342 Test Focal Loss 0.2941922\n","Minibatch at Epoch 73 batch 71 Train Loss: 0.5206987 Train Dice Coeff 0.5386102 Train Focal Loss 1.0544794 Test Loss: 0.8842394 Test Dice Coeff 0.03740886 Test Focal Loss 0.17907302\n","Minibatch at Epoch 73 batch 81 Train Loss: 0.7785725 Train Dice Coeff 0.16772844 Train Focal Loss 0.29528105 Test Loss: 0.83564144 Test Dice Coeff 0.1373896 Test Focal Loss 0.59292084\n","Minibatch at Epoch 73 batch 91 Train Loss: 0.31470498 Train Dice Coeff 0.6717115 Train Focal Loss 0.19245365 Test Loss: 0.9534005 Test Dice Coeff 0.097890995 Test Focal Loss 1.4150238\n","Minibatch at Epoch 73 batch 101 Train Loss: 0.50684947 Train Dice Coeff 0.5075362 Train Focal Loss 0.6363205 Test Loss: 0.34041443 Test Dice Coeff 0.6393968 Test Focal Loss 0.15871555\n","Minibatch at Epoch 73 batch 111 Train Loss: 0.4533372 Train Dice Coeff 0.54636085 Train Focal Loss 0.45061964 Test Loss: 0.54745775 Test Dice Coeff 0.4254546 Test Focal Loss 0.30366915\n","Minibatch at Epoch 73 batch 121 Train Loss: 0.36899632 Train Dice Coeff 0.6261711 Train Focal Loss 0.3255034 Test Loss: 0.4771441 Test Dice Coeff 0.5689953 Test Focal Loss 0.8923987\n","Minibatch at Epoch 73 batch 131 Train Loss: 0.81603575 Train Dice Coeff 0.1583229 Train Focal Loss 0.58526397 Test Loss: 0.4153959 Test Dice Coeff 0.5690173 Test Focal Loss 0.27511454\n","Minibatch at Epoch 73 batch 141 Train Loss: 0.84256786 Train Dice Coeff 0.20244156 Train Focal Loss 1.2476529 Test Loss: 0.667673 Test Dice Coeff 0.28168273 Test Focal Loss 0.21187499\n","Minibatch at Epoch 73 batch 151 Train Loss: 0.4557407 Train Dice Coeff 0.5851512 Train Focal Loss 0.82376766 Test Loss: 0.84259427 Test Dice Coeff 0.08547816 Test Focal Loss 0.1952464\n","Minibatch at Epoch 73 batch 161 Train Loss: 0.41222224 Train Dice Coeff 0.5704436 Train Focal Loss 0.25621468 Test Loss: 0.7737173 Test Dice Coeff 0.15276952 Test Focal Loss 0.11209854\n","Minibatch at Epoch 73 batch 171 Train Loss: 0.34249252 Train Dice Coeff 0.6432883 Train Focal Loss 0.21451996 Test Loss: 0.7301342 Test Dice Coeff 0.24516344 Test Focal Loss 0.50781286\n","Minibatch at Epoch 73 batch 181 Train Loss: 0.37825337 Train Dice Coeff 0.66870433 Train Focal Loss 0.8008728 Test Loss: 0.36725423 Test Dice Coeff 0.61077833 Test Focal Loss 0.16954741\n","Minibatch at Epoch 73 batch 191 Train Loss: 0.33764952 Train Dice Coeff 0.71249956 Train Focal Loss 0.78899133 Test Loss: 1.0082585 Test Dice Coeff 0.0008792291 Test Focal Loss 1.0904981\n","Minibatch at Epoch 73 batch 201 Train Loss: 0.45410237 Train Dice Coeff 0.55306625 Train Focal Loss 0.5186199 Test Loss: 0.94443655 Test Dice Coeff 0.1948436 Test Focal Loss 2.1979582\n","Minibatch at Epoch 73 batch 211 Train Loss: 0.74629354 Train Dice Coeff 0.2228045 Train Focal Loss 0.46817625 Test Loss: 0.8238751 Test Dice Coeff 0.10008445 Test Focal Loss 0.1395114\n","Minibatch at Epoch 73 batch 221 Train Loss: 0.28893572 Train Dice Coeff 0.7171958 Train Focal Loss 0.3441196 Test Loss: 0.6337584 Test Dice Coeff 0.33224878 Test Focal Loss 0.32782364\n","Minibatch at Epoch 73 batch 231 Train Loss: 0.49250105 Train Dice Coeff 0.47783086 Train Focal Loss 0.22548859 Test Loss: 1.1845083 Test Dice Coeff 0.17724662 Test Focal Loss 4.4403024\n","Minibatch at Epoch 73 batch 241 Train Loss: 0.37012824 Train Dice Coeff 0.6082726 Train Focal Loss 0.17573605 Test Loss: 0.88366604 Test Dice Coeff 0.09318685 Test Focal Loss 0.6753423\n","Minibatch at Epoch 73 batch 251 Train Loss: 0.47680697 Train Dice Coeff 0.601248 Train Focal Loss 1.1793021 Test Loss: 0.46434876 Test Dice Coeff 0.59578425 Test Focal Loss 1.0055457\n","Minibatch at Epoch 73 batch 261 Train Loss: 0.719577 Train Dice Coeff 0.43749636 Train Focal Loss 2.1332378 Test Loss: 0.5640695 Test Dice Coeff 0.48702958 Test Focal Loss 1.0239612\n","Minibatch at Epoch 73 batch 271 Train Loss: 0.5738833 Train Dice Coeff 0.43007588 Train Focal Loss 0.609516 Test Loss: 1.5237639 Test Dice Coeff 0.009758982 Test Focal Loss 6.3254704\n","Minibatch at Epoch 73 batch 281 Train Loss: 0.2719905 Train Dice Coeff 0.7216667 Train Focal Loss 0.21490547 Test Loss: 0.55527353 Test Dice Coeff 0.4158242 Test Focal Loss 0.29515296\n","Minibatch at Epoch 73 batch 291 Train Loss: 0.3549466 Train Dice Coeff 0.70864487 Train Focal Loss 0.92727005 Test Loss: 0.42471853 Test Dice Coeff 0.5896502 Test Focal Loss 0.5540374\n","Minibatch at Epoch 73 batch 301 Train Loss: 0.35287476 Train Dice Coeff 0.6661974 Train Focal Loss 0.52452433 Test Loss: 0.74091893 Test Dice Coeff 0.1886475 Test Focal Loss 0.10701689\n","Minibatch at Epoch 73 batch 311 Train Loss: 0.39445 Train Dice Coeff 0.62507296 Train Focal Loss 0.57015693 Test Loss: 0.7874481 Test Dice Coeff 0.15852006 Test Focal Loss 0.30116153\n","Minibatch at Epoch 73 batch 321 Train Loss: 0.42036235 Train Dice Coeff 0.71010023 Train Focal Loss 1.5945256 Test Loss: 0.38926917 Test Dice Coeff 0.61681056 Test Focal Loss 0.44398698\n","Minibatch at Epoch 73 batch 331 Train Loss: 1.10789 Train Dice Coeff 0.011292735 Train Focal Loss 2.1805348 Test Loss: 0.89116144 Test Dice Coeff 0.049058978 Test Focal Loss 0.35314542\n","Minibatch at Epoch 73 batch 341 Train Loss: 0.23230962 Train Dice Coeff 0.80094784 Train Focal Loss 0.53162694 Test Loss: 0.6456733 Test Dice Coeff 0.3138097 Test Focal Loss 0.28102028\n","Minibatch at Epoch 73 batch 351 Train Loss: 0.78974503 Train Dice Coeff 0.19457994 Train Focal Loss 0.64867043 Test Loss: 0.7955605 Test Dice Coeff 0.22856727 Test Focal Loss 1.0127103\n","Minibatch at Epoch 73 batch 361 Train Loss: 0.28918856 Train Dice Coeff 0.71967775 Train Focal Loss 0.3689856 Test Loss: 0.7813959 Test Dice Coeff 0.19365047 Test Focal Loss 0.5568137\n","Minibatch at Epoch 73 batch 371 Train Loss: 0.26861373 Train Dice Coeff 0.75535274 Train Focal Loss 0.4843118 Test Loss: 0.76251346 Test Dice Coeff 0.2110297 Test Focal Loss 0.52440226\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 73 Test Loss: 0.53610384 Test Dice Coeff 0.45696345 Test Focal Loss 0.47370914\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 74 batch 1 Train Loss: 0.34415263 Train Dice Coeff 0.67249906 Train Focal Loss 0.49401787 Test Loss: 0.77381444 Test Dice Coeff 0.1618654 Test Focal Loss 0.19493338\n","Minibatch at Epoch 74 batch 11 Train Loss: 0.32429135 Train Dice Coeff 0.6979853 Train Focal Loss 0.5247809 Test Loss: 0.82569766 Test Dice Coeff 0.24266437 Test Focal Loss 1.4409558\n","Minibatch at Epoch 74 batch 21 Train Loss: 0.49579743 Train Dice Coeff 0.5507788 Train Focal Loss 0.9149838 Test Loss: 0.31277433 Test Dice Coeff 0.6611462 Test Focal Loss 0.07805932\n","Minibatch at Epoch 74 batch 31 Train Loss: 0.5039342 Train Dice Coeff 0.50049084 Train Focal Loss 0.5437598 Test Loss: 0.55130416 Test Dice Coeff 0.39861804 Test Focal Loss 0.100604475\n","Minibatch at Epoch 74 batch 41 Train Loss: 0.40354759 Train Dice Coeff 0.62274486 Train Focal Loss 0.6401798 Test Loss: 0.598168 Test Dice Coeff 0.362674 Test Focal Loss 0.2457463\n","Minibatch at Epoch 74 batch 51 Train Loss: 0.4956646 Train Dice Coeff 0.49270254 Train Focal Loss 0.390969 Test Loss: 0.50281495 Test Dice Coeff 0.5689951 Test Focal Loss 1.1491057\n","Minibatch at Epoch 74 batch 61 Train Loss: 0.18927336 Train Dice Coeff 0.8286909 Train Focal Loss 0.35095158 Test Loss: 0.5631801 Test Dice Coeff 0.49249303 Test Focal Loss 1.0642381\n","Minibatch at Epoch 74 batch 71 Train Loss: 0.4655813 Train Dice Coeff 0.5547039 Train Focal Loss 0.6481479 Test Loss: 0.7234216 Test Dice Coeff 0.21923791 Test Focal Loss 0.20735767\n","Minibatch at Epoch 74 batch 81 Train Loss: 0.38012868 Train Dice Coeff 0.6449948 Train Focal Loss 0.60624015 Test Loss: 0.6219263 Test Dice Coeff 0.49250114 Test Focal Loss 1.6517735\n","Minibatch at Epoch 74 batch 91 Train Loss: 0.4573444 Train Dice Coeff 0.56803715 Train Focal Loss 0.6857785 Test Loss: 0.5902091 Test Dice Coeff 0.3796217 Test Focal Loss 0.31868675\n","Minibatch at Epoch 74 batch 101 Train Loss: 0.39170337 Train Dice Coeff 0.6506549 Train Focal Loss 0.7729281 Test Loss: 0.6821041 Test Dice Coeff 0.2787616 Test Focal Loss 0.32989585\n","Minibatch at Epoch 74 batch 111 Train Loss: 0.38605642 Train Dice Coeff 0.6767851 Train Focal Loss 0.95163 Test Loss: 0.64724356 Test Dice Coeff 0.31040698 Test Focal Loss 0.26609844\n","Minibatch at Epoch 74 batch 121 Train Loss: 0.24995038 Train Dice Coeff 0.7542094 Train Focal Loss 0.28738844 Test Loss: 0.3994781 Test Dice Coeff 0.62681866 Test Focal Loss 0.63614905\n","Minibatch at Epoch 74 batch 131 Train Loss: 0.5070791 Train Dice Coeff 0.48815852 Train Focal Loss 0.46421823 Test Loss: 0.9678486 Test Dice Coeff 0.013670131 Test Focal Loss 0.8015176\n","Minibatch at Epoch 74 batch 141 Train Loss: 0.8303761 Train Dice Coeff 0.17626053 Train Focal Loss 0.89010596 Test Loss: 0.8866156 Test Dice Coeff 0.110735014 Test Focal Loss 0.8627709\n","Minibatch at Epoch 74 batch 151 Train Loss: 0.91072416 Train Dice Coeff 1.224886e-06 Train Focal Loss 0.10725303 Test Loss: 0.73346776 Test Dice Coeff 0.3115884 Test Focal Loss 1.1389732\n","Minibatch at Epoch 74 batch 161 Train Loss: 0.63053334 Train Dice Coeff 0.3228694 Train Focal Loss 0.2111581 Test Loss: 0.7471772 Test Dice Coeff 0.18805449 Test Focal Loss 0.1642626\n","Minibatch at Epoch 74 batch 171 Train Loss: 0.29018977 Train Dice Coeff 0.73050606 Train Focal Loss 0.47645253 Test Loss: 0.6805452 Test Dice Coeff 0.28895295 Test Focal Loss 0.40602854\n","Minibatch at Epoch 74 batch 181 Train Loss: 0.29292837 Train Dice Coeff 0.71544826 Train Focal Loss 0.36831787 Test Loss: 0.69505674 Test Dice Coeff 0.30369925 Test Focal Loss 0.683861\n","Minibatch at Epoch 74 batch 191 Train Loss: 0.38257208 Train Dice Coeff 0.66625035 Train Focal Loss 0.821974 Test Loss: 0.819165 Test Dice Coeff 0.13316734 Test Focal Loss 0.39015612\n","Minibatch at Epoch 74 batch 201 Train Loss: 0.5065663 Train Dice Coeff 0.52743405 Train Focal Loss 0.81256986 Test Loss: 0.734248 Test Dice Coeff 0.21900712 Test Focal Loss 0.3135442\n","Minibatch at Epoch 74 batch 211 Train Loss: 0.3700691 Train Dice Coeff 0.6215001 Train Focal Loss 0.29419154 Test Loss: 0.7435008 Test Dice Coeff 0.72424436 Test Focal Loss 4.9532075\n","Minibatch at Epoch 74 batch 221 Train Loss: 1.0834969 Train Dice Coeff 0.30981416 Train Focal Loss 4.6232967 Test Loss: 0.23171324 Test Dice Coeff 0.79084903 Test Focal Loss 0.43477368\n","Minibatch at Epoch 74 batch 231 Train Loss: 0.33021292 Train Dice Coeff 0.6994381 Train Focal Loss 0.5970723 Test Loss: 0.49144712 Test Dice Coeff 0.5101357 Test Focal Loss 0.5056926\n","Minibatch at Epoch 74 batch 241 Train Loss: 0.5037594 Train Dice Coeff 0.5386768 Train Focal Loss 0.8856848 Test Loss: 0.5164367 Test Dice Coeff 0.48499703 Test Focal Loss 0.5293407\n","Minibatch at Epoch 74 batch 251 Train Loss: 0.2543296 Train Dice Coeff 0.75693774 Train Focal Loss 0.35573563 Test Loss: 0.29734352 Test Dice Coeff 0.7092232 Test Focal Loss 0.356444\n","Minibatch at Epoch 74 batch 261 Train Loss: 0.42124477 Train Dice Coeff 0.6114714 Train Focal Loss 0.7156905 Test Loss: 0.7558114 Test Dice Coeff 0.23350579 Test Focal Loss 0.6596661\n","Minibatch at Epoch 74 batch 271 Train Loss: 0.5204287 Train Dice Coeff 0.5213799 Train Focal Loss 0.89670646 Test Loss: 0.363589 Test Dice Coeff 0.71534944 Test Focal Loss 1.0740347\n","Minibatch at Epoch 74 batch 281 Train Loss: 0.44267076 Train Dice Coeff 0.68008035 Train Focal Loss 1.5474306 Test Loss: 0.60558873 Test Dice Coeff 0.36782095 Test Focal Loss 0.3662767\n","Minibatch at Epoch 74 batch 291 Train Loss: 0.37213683 Train Dice Coeff 0.64468503 Train Focal Loss 0.5235334 Test Loss: 0.9125432 Test Dice Coeff 0.23173057 Test Focal Loss 2.2110076\n","Minibatch at Epoch 74 batch 301 Train Loss: 0.5330677 Train Dice Coeff 0.46025845 Train Focal Loss 0.47300336 Test Loss: 0.7297759 Test Dice Coeff 0.22029077 Test Focal Loss 0.2803759\n","Minibatch at Epoch 74 batch 311 Train Loss: 0.3595529 Train Dice Coeff 0.6617897 Train Focal Loss 0.5516364 Test Loss: 0.7476172 Test Dice Coeff 0.24752867 Test Focal Loss 0.7039304\n","Minibatch at Epoch 74 batch 321 Train Loss: 0.21306592 Train Dice Coeff 0.79930663 Train Focal Loss 0.32441896 Test Loss: 0.48820442 Test Dice Coeff 0.49298808 Test Focal Loss 0.31893736\n","Minibatch at Epoch 74 batch 331 Train Loss: 0.41100442 Train Dice Coeff 0.5762371 Train Focal Loss 0.2961783 Test Loss: 0.4622523 Test Dice Coeff 0.50108856 Test Focal Loss 0.13231991\n","Minibatch at Epoch 74 batch 341 Train Loss: 1.0641181 Train Dice Coeff 0.038297925 Train Focal Loss 1.9858634 Test Loss: 0.54263777 Test Dice Coeff 0.41790327 Test Focal Loss 0.18750744\n","Minibatch at Epoch 74 batch 351 Train Loss: 0.3922782 Train Dice Coeff 0.6223739 Train Focal Loss 0.52414685 Test Loss: 2.4510732 Test Dice Coeff 0.0034540172 Test Focal Loss 15.54182\n","Minibatch at Epoch 74 batch 361 Train Loss: 0.75514185 Train Dice Coeff 0.2570988 Train Focal Loss 0.86530834 Test Loss: 0.65038013 Test Dice Coeff 0.33244944 Test Focal Loss 0.49584615\n","Minibatch at Epoch 74 batch 371 Train Loss: 0.78762096 Train Dice Coeff 0.39322647 Train Focal Loss 2.4152482 Test Loss: 0.30743223 Test Dice Coeff 0.7100679 Test Focal Loss 0.46493384\n","-------------------------------------------------------------------------------------------------------\n","After Epoch 74 Test Loss: 0.74988663 Test Dice Coeff 0.20747685 Test Focal Loss 0.36615843\n","-------------------------------------------------------------------------------------------------------\n","Minibatch at Epoch 75 batch 1 Train Loss: 0.45455074 Train Dice Coeff 0.53854185 Train Focal Loss 0.3923842 Test Loss: 1.0981623 Test Dice Coeff 0.041579608 Test Focal Loss 2.35584\n","Minibatch at Epoch 75 batch 11 Train Loss: 0.29384992 Train Dice Coeff 0.73601127 Train Focal Loss 0.56260073 Test Loss: 0.9011966 Test Dice Coeff 0.037264418 Test Focal Loss 0.34734607\n","Minibatch at Epoch 75 batch 21 Train Loss: 0.28818434 Train Dice Coeff 0.7137283 Train Focal Loss 0.30539852 Test Loss: 0.7151355 Test Dice Coeff 0.26303166 Test Focal Loss 0.5186402\n","Minibatch at Epoch 75 batch 31 Train Loss: 0.28182286 Train Dice Coeff 0.73153543 Train Focal Loss 0.40204751 Test Loss: 0.6106215 Test Dice Coeff 0.34677005 Test Focal Loss 0.22714555\n","Minibatch at Epoch 75 batch 41 Train Loss: 0.32361573 Train Dice Coeff 0.67109746 Train Focal Loss 0.27603436 Test Loss: 0.61548513 Test Dice Coeff 0.37286496 Test Focal Loss 0.5106362\n","Minibatch at Epoch 75 batch 51 Train Loss: 0.37319526 Train Dice Coeff 0.65374535 Train Focal Loss 0.6156609 Test Loss: 0.5833407 Test Dice Coeff 0.37837568 Test Focal Loss 0.23878808\n","Minibatch at Epoch 75 batch 61 Train Loss: 0.36927667 Train Dice Coeff 0.61523205 Train Focal Loss 0.22985521 Test Loss: 0.58450943 Test Dice Coeff 0.43647575 Test Focal Loss 0.77337646\n","Minibatch at Epoch 75 batch 71 Train Loss: 0.33868256 Train Dice Coeff 0.67657125 Train Focal Loss 0.47596693 Test Loss: 0.22269954 Test Dice Coeff 0.76226586 Test Focal Loss 0.08738822\n","Minibatch at Epoch 75 batch 81 Train Loss: 0.36457872 Train Dice Coeff 0.6368766 Train Focal Loss 0.37767643 Test Loss: 0.3945336 Test Dice Coeff 0.60249627 Test Focal Loss 0.36780274\n","Minibatch at Epoch 75 batch 91 Train Loss: 0.38702816 Train Dice Coeff 0.65866524 Train Focal Loss 0.7982689 Test Loss: 0.49541596 Test Dice Coeff 0.5128757 Test Focal Loss 0.57004076\n","Minibatch at Epoch 75 batch 101 Train Loss: 0.27671885 Train Dice Coeff 0.71738654 Train Focal Loss 0.22366747 Test Loss: 0.67823887 Test Dice Coeff 0.3158364 Test Focal Loss 0.62491655\n","Minibatch at Epoch 75 batch 111 Train Loss: 0.5316094 Train Dice Coeff 0.47336197 Train Focal Loss 0.57635194 Test Loss: 0.6598226 Test Dice Coeff 0.33852637 Test Focal Loss 0.6449633\n","Minibatch at Epoch 75 batch 121 Train Loss: 0.8450886 Train Dice Coeff 0.09513697 Train Focal Loss 0.30711955 Test Loss: 0.7043472 Test Dice Coeff 0.27851328 Test Focal Loss 0.5500918\n","Minibatch at Epoch 75 batch 131 Train Loss: 0.36146566 Train Dice Coeff 0.714777 Train Focal Loss 1.0476497 Test Loss: 0.29262573 Test Dice Coeff 0.7077244 Test Focal Loss 0.29577678\n","Minibatch at Epoch 75 batch 141 Train Loss: 0.48920748 Train Dice Coeff 0.547943 Train Focal Loss 0.8235618 Test Loss: 2.3308814 Test Dice Coeff 0.0062752436 Test Focal Loss 14.36529\n","Minibatch at Epoch 75 batch 151 Train Loss: 0.28636163 Train Dice Coeff 0.72256446 Train Focal Loss 0.36669672 Test Loss: 0.5870926 Test Dice Coeff 0.39576927 Test Focal Loss 0.43284896\n","Minibatch at Epoch 75 batch 161 Train Loss: 0.6108899 Train Dice Coeff 0.51331645 Train Focal Loss 1.728747 Test Loss: 0.3530806 Test Dice Coeff 0.6277365 Test Focal Loss 0.18043455\n","Minibatch at Epoch 75 batch 171 Train Loss: 0.45429805 Train Dice Coeff 0.58202195 Train Focal Loss 0.7811781 Test Loss: 1.0895271 Test Dice Coeff 0.11742598 Test Focal Loss 2.9521055\n","Minibatch at Epoch 75 batch 181 Train Loss: 0.39934266 Train Dice Coeff 0.6334834 Train Focal Loss 0.69477737 Test Loss: 0.59615606 Test Dice Coeff 0.37763214 Test Focal Loss 0.36024964\n","Minibatch at Epoch 75 batch 191 Train Loss: 0.38104495 Train Dice Coeff 0.59989303 Train Focal Loss 0.20948699 Test Loss: 0.2887637 Test Dice Coeff 0.7582499 Test Focal Loss 0.7118859\n","Minibatch at Epoch 75 batch 201 Train Loss: 0.32815522 Train Dice Coeff 0.69744307 Train Focal Loss 0.55853987 Test Loss: 1.0296817 Test Dice Coeff 0.06410302 Test Focal Loss 1.8737444\n","Minibatch at Epoch 75 batch 211 Train Loss: 0.26816067 Train Dice Coeff 0.758784 Train Focal Loss 0.5106627 Test Loss: 1.3751892 Test Dice Coeff 0.00691426 Test Focal Loss 4.814121\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1aedf54d4167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchose_train_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-72d76ce9f22b>\u001b[0m in \u001b[0;36mchose_train_restore\u001b[0;34m(learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m#with tf.device('/cpu:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mepoch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_batch_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermute_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_h5py_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermute_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0msess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;31m#print (\"epoch\",epoch+1,\"batch\",iteration+1)#,\"Cost\",sess_results[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}